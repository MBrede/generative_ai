<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Embedding-based agent-systems – Generative AI</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../content/function_calling.html" rel="next">
<link href="../content/agent_basics.html" rel="prev">
<link href="../cover.jpg" rel="icon" type="image/jpeg">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar docked slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../content/getting_started_with_llms.html">Language Models</a></li><li class="breadcrumb-item"><a href="../content/embeddings.html"><span class="chapter-title">Embedding-based agent-systems</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../index.html" class="sidebar-logo-link">
      <img src="../cover.jpg" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Generative AI</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/MBrede/generative_ai" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/orga.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Organizational Details</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/project_details.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Project Details</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Language Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/getting_started_with_llms.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Getting started with (L)LMs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/prompting.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Prompting</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/agent_basics.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Agent basics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/embeddings.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">Embedding-based agent-systems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/function_calling.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Function Calling</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/agent_interaction.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Agent interaction</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Image Generation</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/generator_basics.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">AI image generation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/augmentation.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Augmentation of image datasets</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Finetuning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/finetuning_approaches.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Finetuning Approaches</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/rank_adaptation.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Rank adaptation</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="2">
    <h2 id="toc-title"><img src="../imgs/rag.webp" class="img-fluid" width="240"><br><br>
</h2><h3 class="anchored"><br>
Embedding-based agent-systems<br>
</h3>
   
  <ul>
  <li><a href="#semantic-embeddings-and-vector-stores" id="toc-semantic-embeddings-and-vector-stores" class="nav-link active" data-scroll-target="#semantic-embeddings-and-vector-stores">Semantic embeddings and vector stores</a></li>
  <li><a href="#retrieval-augmented-generation" id="toc-retrieval-augmented-generation" class="nav-link" data-scroll-target="#retrieval-augmented-generation">Retrieval augmented generation</a>
  <ul class="collapse">
  <li><a href="#vector-databases" id="toc-vector-databases" class="nav-link" data-scroll-target="#vector-databases">Vector databases</a></li>
  <li><a href="#rag" id="toc-rag" class="nav-link" data-scroll-target="#rag">RAG</a></li>
  <li><a href="#document-chunking" id="toc-document-chunking" class="nav-link" data-scroll-target="#document-chunking">Document chunking</a></li>
  <li><a href="#query-expansiontransformation" id="toc-query-expansiontransformation" class="nav-link" data-scroll-target="#query-expansiontransformation">Query Expansion/Transformation</a></li>
  </ul></li>
  <li><a href="#further-readings" id="toc-further-readings" class="nav-link" data-scroll-target="#further-readings">Further Readings</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/MBrede/generative_ai/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../content/getting_started_with_llms.html">Language Models</a></li><li class="breadcrumb-item"><a href="../content/embeddings.html"><span class="chapter-title">Embedding-based agent-systems</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-title">Embedding-based agent-systems</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>All agents we discussed until here are using tools that allow them to use their generated inputs in some way. In most of the task we want to utilize agents, we do not only want to generate text but to also inform the generation based on some kind of existing knowledge base. Examples for these kinds of usecases include:</p>
<ul>
<li>Answering questions about a specific topic (e.g., a company or product)</li>
<li>Summarizing a document</li>
<li>Generating a report based on data</li>
</ul>
<p>Though most modern LLMs are increasingly capable in answering basic knowledge-questions, the more comples a topic or the more relevant the factual basis of an answer is, the more it is important to base generated answers on actual data.</p>
<section id="semantic-embeddings-and-vector-stores" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="semantic-embeddings-and-vector-stores">Semantic embeddings and vector stores</h2>
<p>To empower an agent too look up information during its thought-process, one has to build a tool that allows an agent to use natural language to retrieve information necessary for a task. The fundamental principle to do this are so-called <em>semantic embeddings</em>. These are pretty close to the concept we introduced when talking about the foundations of LLMs (see <a href="getting_started_with_llms">here</a>) and can be understood as a way to map textual data into a vector space. The main idea is that semantically similar texts should have similar embeddings, i.e., they are close in the vector space. Close in this context is meant as having a reasonibly small distance between them. The go-to standard to measure this distance is the <strong>cosine similarity</strong>, which has proven usefull enough to be the standard for a range of semantic retrieval implementations (i.e., they are used in <a href="https://cookbook.openai.com/examples/recommendation_using_embeddings">OpenAI tutorials</a> and in <a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/understand-embeddings">Azure embedding-applications</a>). The cosine similarity is defined as:</p>
<p><span class="math display">\[
\text{cosine\_similarity}(u, v) = \frac{u \cdot v}{\|u\| \|v\|} = \frac{\sum_{i=1}^{n} u_i v_i}{\sqrt{\sum_{i=1}^{n} u_i^2} \sqrt{\sum_{i=1}^{n} v_i^2}}
\]</span> The rationale here is that sequences with semantically similar contents should point to similar directions in the high dimensional vector space. See <a href="#fig-cosineSimilarity" class="quarto-xref">Figure&nbsp;<span>6.1</span></a> for an illustration of this and other common similarity concepts seen in semantic retrieval.</p>
<div class="cell enlarge-onhover" data-layout-align="center">
<div id="fig-cosineSimilarity" class="cell enlarge-onhover quarto-float quarto-figure quarto-figure-center anchored" data-layout-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cosineSimilarity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div id="fig-cosineSimilarity-1" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center" alt="Illustration of &quot;semantic embeddings&quot; of different word. The words &quot;good&quot;, &quot;nice&quot; and &quot;orange&quot; are all mapped to a vector space.">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-cosineSimilarity-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="embeddings_files/figure-html/fig-cosineSimilarity-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" data-ref-parent="fig-cosineSimilarity" alt="Illustration of &quot;semantic embeddings&quot; of different word. The words &quot;good&quot;, &quot;nice&quot; and &quot;orange&quot; are all mapped to a vector space." width="768">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-cosineSimilarity-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Illustration of “semantic embeddings” of different word.
</figcaption>
</figure>
</div>
</div>
<div class="cell-output-display">
<div id="fig-cosineSimilarity-2" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center" alt="Illustration of similarities between these words. 4 common similarity concepts seen in semantic retrieval: cosine, euclidean, dot product and manhattan.">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-cosineSimilarity-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="embeddings_files/figure-html/fig-cosineSimilarity-2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" data-ref-parent="fig-cosineSimilarity" alt="Illustration of similarities between these words. 4 common similarity concepts seen in semantic retrieval: cosine, euclidean, dot product and manhattan." width="768">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-cosineSimilarity-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Illustration of 4 common similarity concepts seen in semantic retrieval: cosine, euclidean, dot product and manhattan. dot product and cosine are taking the direction of the vector into account, while the cosine ignores the length of the vectors and the dot product does not. Manhattan and euclidean are both measuring the distance between two points in a vector space, but they do it differently. Euclidean is the straight line between two points, while manhattan is the sum of the absolute differences between the coordinates of the two points.
</figcaption>
</figure>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cosineSimilarity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Fig&nbsp;6.1: Illustration of common similarity metrics in semantic search.
</figcaption>
</figure>
</div>
</div>
<p>As always, there is not the one solution to all problems though and the applicability of cosine similarity might not be optimal for your usecase <span class="citation" data-cites="steckCosineSimilarityEmbeddingsReally2024 goyalComparativeAnalysisDifferent2022">(<a href="#ref-goyalComparativeAnalysisDifferent2022" role="doc-biblioref">Goyal &amp; Sharma, 2022</a>; <a href="#ref-steckCosineSimilarityEmbeddingsReally2024" role="doc-biblioref">Steck et al., 2024</a>)</span>.</p>
<p>Though one could use any kind of (L)LM to calculate embeddings for this case<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>, it is advisable to use models specifically trained for this purpose. <span class="citation" data-cites="reimersSentenceBERTSentenceEmbeddings2019a">Reimers &amp; Gurevych (<a href="#ref-reimersSentenceBERTSentenceEmbeddings2019a" role="doc-biblioref">2019</a>)</span> proposed <em>Sentence-BERT</em> which is a simple but effective approach to calculate semantic embeddings. SBERT and similar approaches are based on a (L)LM that was trained to predict missing words as we discussed before, resulting in a general representation of natural language. In the case of the original paper, they used (among others) the BERT model <span class="citation" data-cites="devlinBERTPretrainingDeep2019a">Devlin et al. (<a href="#ref-devlinBERTPretrainingDeep2019a" role="doc-biblioref">2019</a>)</span> mentioned before.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;And there are approaches to use LLMs to solve this taks i.e., <span class="citation" data-cites="jiangScalingSentenceEmbeddings2023a">Jiang et al. (<a href="#ref-jiangScalingSentenceEmbeddings2023a" role="doc-biblioref">2023</a>)</span></p></div><div id="fn2"><p><sup>2</sup>&nbsp;The original BERT-paper did this by adding a pooling layer before the task-header that extracted and weighed the context-dependend embedding of the first token. The SBERT paper tried different pooling-strategies and used a mean over each embedding dimension of the sequence.</p></div></div><p>The authors then use this to embed a pair of sentences into one embedding-vector each<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>, for which some measure of semantic similarity is known. An example for a dataset containing such sentences is the <a href="https://aclanthology.org/D15-1075/">Stanford Natural Language Inferenc(SNLI) corpus <span class="citation" data-cites="bowmanLargeAnnotatedCorpus2015">Bowman et al. (<span>2015</span>)</span></a> which labels 550k pairs of sentences as either <em>entailment</em>, <em>contradiction</em> or <em>neutral</em>. <span class="citation" data-cites="reimersSentenceBERTSentenceEmbeddings2019a">Reimers &amp; Gurevych (<a href="#ref-reimersSentenceBERTSentenceEmbeddings2019a" role="doc-biblioref">2019</a>)</span> then concated the both senteces embeddings and their element-wise difference into a single vector which is fed to a multiclass classifier, indicating in which category the sentences relationship falls. At inference, this classification head was removed and replaced as the cosine similarity as discussed above. The resulting network is highly effective in calculating semantic similarities between sentences.</p>
<p>A look at the <a href="https://www.sbert.net/docs/sentence_transformer/loss_overview.html">sbert-website</a> shows that the module has somewhat grown and now does supply a series of learning paradigms that can be used to efficiently tune a model for your specific usecase<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>. As the library has grown, so has the sheer amount of pretrained embedding-models in some way based on this architecture that are hosted on huggingface. The <a href="https://huggingface.co/spaces/mteb/leaderboard">MTEB-Leaderboard</a> is a good strat to search for a model for your application. One utilization of this model-family, which has already been implicitly used in this script, is their very efficient ability to semantically search for documents. If a model is very good at finding similar sentences, it can also be very good to find documents that are very similar to a question.</p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;And this does not have to be expensive. <span class="citation" data-cites="tunstallEfficientFewShotLearning2022">Tunstall et al. (<a href="#ref-tunstallEfficientFewShotLearning2022" role="doc-biblioref">2022</a>)</span> have shown a highly efficient contrastive learning paradigm that limts the amount of necessary labels for a ridiuculously small amount of labels.</p></div></div><p>Look at the example illustrated in <a href="#fig-docRetrieval" class="quarto-xref">Figure&nbsp;<span>6.2</span></a>. The question “why is the sky blue” embedded with the same model as our 5 documents stating some facts.</p>
<div id="fig-docRetrieval" class="enlarge-onhover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-docRetrieval-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../imgs/retrieval_illustration.png" class="enlarge-onhover img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-docRetrieval-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Fig&nbsp;6.2: Illustration of the usage of embedding-based distances in retrieval.
</figcaption>
</figure>
</div>
<p>We can then calculate the cosine-similarity between these embeddings and return the document, that has the highest similarity to our question.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
📝 Task
</div>
</div>
<div class="callout-body-container callout-body">
<p>Install the sentence-transformer package and download <a href="https://huggingface.co/datasets/tdiggelm/climate_fever">the climate_fever-dataset</a>.</p>
<p>Choose one model from the <a href="https://huggingface.co/spaces/mteb/leaderboard">MTEB-Leaderboard</a> that you deem adequatly sized and appropriate for the task</p>
<p>Test the different metrics for the first twenty claims of the dataset and a question you formulate.</p>
<p>Use the similarity-implementations from <a href="https://scikit-learn.org/dev/api/sklearn.metrics.html#module-sklearn.metrics.pairwise">sklearn.metrics.pairwise</a>.</p>
</div>
</div>
<p>This approach of using a model to embed documents and questions into a vector space is the basis for the so-called <em>Retrieval augmented generation</em>.</p>
</section>
<section id="retrieval-augmented-generation" class="level2">
<h2 class="anchored" data-anchor-id="retrieval-augmented-generation">Retrieval augmented generation</h2>
<p>Retrieval augmented generation (RAG) is a framework that does pretty much do what it says on the tin. You use a retrieval model to find documents that are similar to your question and then either return these documents our feed them into a generative model, which then generates an answer based on these documents. This process can additionally be wrapped as a tool to be used by an agent, so that your existing agent can now also use external knowledge sources to answer questions.</p>
<p>Retrieval does not have to be semantics-based in this context - all kinds of data sources and databases can be made accessible for a LLM - we will focus on a purely embbedding based approach here though.</p>
<p>Although the small example in the last task was working, it is not really scalable. It was fine for a limited set of examples, if you want to realistically make a whole knowledge base searchable, you need to use an appropriate database system.</p>
<section id="vector-databases" class="level3">
<h3 class="anchored" data-anchor-id="vector-databases">Vector databases</h3>
<p>A vector database is a database that stores vectors and allows for efficient similarity searches. As can be seen in the <a href="https://db-engines.com/en/ranking">db-engines ranking</a> there has been a surge of interest in this area recently, with many new players entering the market. From the plethora of vector databases, these three are examples that virtue a honorary mention:</p>
<ol type="1">
<li><p><a href="https://www.trychroma.com/">Chroma</a> - a in-memory database for small applications that is especially easy to get to run.</p></li>
<li><p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/bring-your-own-vectors.html">Elasticsearch</a> - a well established database that is the go to system for open source search engines and has recently (and kind of naturally) also branched out into vector databases.</p></li>
<li><p><a href="https://qdrant.tech/">Qdrant</a> - the product of a Berlin-based startup that focusses on stability and scalability. It can also run in memory, but does natively support hard drive storage.</p></li>
</ol>
<p>The best way to use qdrant is to use <a href="https://qdrant.tech/documentation/quickstart/">docker</a> to run it and the python sdk to interact with it. Since version 1.1.1, the sdk also allows to just run the client in memory.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
📝 Task
</div>
</div>
<div class="callout-body-container callout-body">
<p>Install the qdrant-client python-sdk and fastembed.</p>
<p>Create a collection for the claims and one for the evidence in <a href="https://huggingface.co/datasets/tdiggelm/climate_fever">the climate_fever-dataset</a>. Add the first 200 entries to each of these collections. Use qdrants <a href="https://qdrant.tech/articles/fastembed/">fastembemd</a>-integration to do this.</p>
<p>Test the similarity search on a question you formulate.</p>
</div>
</div>
</section>
<section id="rag" class="level3">
<h3 class="anchored" data-anchor-id="rag">RAG</h3>
<p>The last step to make this into a RAG pipeline is to use a generative model to answer the question based on the retrieved documents.</p>
<p>This means, that we do collect the relevant documents like we did before, still based on a natural language question, but instead of returning the hits we got from the index, we feed them into a LLM and ask it to generate an answer based on these documents. This is where the name retrieval augmented generation comes from - we use the retrieval step to augment the generative model with additional information. The diagram in <a href="#fig-rag" class="quarto-xref">Figure&nbsp;<span>6.3</span></a> illustrates this process.</p>
<div id="fig-rag" class="enlarge-onhover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rag-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../imgs/rag.png" class="enlarge-onhover img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rag-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Fig&nbsp;6.3: Illustration of a RAG-system.
</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
📝 Task
</div>
</div>
<div class="callout-body-container callout-body">
<p>Implement a RAG pipeline for the climate_fever dataset using qdrant as vector database and a LLM of your choice for the summarization.</p>
<p>Try to find a prompt that results in the LLM</p>
<ol type="a">
<li>using the information given</li>
<li>not inventing new information</li>
<li>referencing the source of the information it uses</li>
</ol>
<p>Upload your results until here (embedder, database and summarization) to moodle.</p>
</div>
</div>
<p>Most agent frameworks provide integrations for a variety of vector databases.</p>
<p>In terms of llamaindex, there are not just one but two tutorials on how to get qdrant to integrate into your agent, one from <a href="https://qdrant.tech/documentation/frameworks/llama-index/">qdrant</a> for general integration and one from <a href="https://docs.llamaindex.ai/en/stable/examples/vector_stores/QdrantIndexDemo/">llamaindex</a>.</p>
<p>The pipeline is pretty close to what we discussed until here, it just uses the llamaindex-typical wrapper classes. See <a href="#tip-llamaindexRag" class="quarto-xref">Tip&nbsp;<span>6.1</span></a> for an example RAG-system implemented in Llamaindex.</p>
<div id="tip-llamaindexRag" class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip&nbsp;6.1: Llamaindex RAG
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The first thing in both the Llamaindex and the manual way of creating a retrieval pipeline is the setup of a vector database:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> qdrant_client <span class="im">import</span> QdrantClient</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> qdrant_client.models <span class="im">import</span> Distance, VectorParams, Batch</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>DIMENSIONS <span class="op">=</span> <span class="dv">384</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>client <span class="op">=</span> QdrantClient(location<span class="op">=</span><span class="st">":memory:"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>To store data and query the database, we have to load a embedding-model. As in the manual way of creating a retrieval pipeline discussed before, we can use a huggingface-SentenceTranformer model. But instead of using the SentenceTransformer class from the sentence_transformers library, we have to use the HuggingFaceEmbedding class from Llamaindex. This model is entered into the Llamaindex-Settings.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> llama_index.core <span class="im">import</span> Settings</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> llama_index.embeddings.huggingface <span class="im">import</span> HuggingFaceEmbedding</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>embed_model <span class="op">=</span> HuggingFaceEmbedding(model_name<span class="op">=</span><span class="st">"sentence-transformers/all-MiniLM-L12-v2"</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>Settings.embed_model <span class="op">=</span> embed_model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The next step is to wrap the vector-store into a Llamaindex-VectorStoreIndex. This index can be used to add our documents to the database.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> llama_index.vector_stores.qdrant <span class="im">import</span> QdrantVectorStore</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>vector_store <span class="op">=</span> QdrantVectorStore(client<span class="op">=</span>client, collection_name<span class="op">=</span><span class="st">"paper"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>As an example, we will add the “Attention is all you need” paper. This is how the head of our txt-file looks like:</p>
<pre><code>         Attention Is All You Need
arXiv:1706.03762v7 [cs.CL] 2 Aug 2023




                                             Ashish Vaswani∗                Noam Shazeer∗               Niki Parmar∗             Jakob Uszkoreit∗
                                              Google Brain                   Google Brain             Google Research            Google Research
                                          avaswani@google.com             noam@google.com            nikip@google.com            usz@google.com</code></pre>
<p>Since we can not just dump the document at once, we will chunk it in sentences (more about that later). This can be done like this (ignore the parameters by now, we will look at them later):</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> llama_index.core.node_parser <span class="im">import</span> SentenceSplitter</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> llama_index.core <span class="im">import</span> Document</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>node_parser <span class="op">=</span> SentenceSplitter(chunk_size<span class="op">=</span><span class="dv">20</span>, chunk_overlap<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>nodes <span class="op">=</span> node_parser.get_nodes_from_documents(</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    [Document(text<span class="op">=</span>text)], show_progress<span class="op">=</span><span class="va">False</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>Metadata length (0) is close to chunk size (20). Resulting chunks are less than 50 tokens. Consider increasing the chunk size or decreasing the size of your metadata to avoid this.</code></pre>
<p>These documents are then added to our database and transformed in an <em>index</em> llamaindex can use:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> llama_index.core <span class="im">import</span> VectorStoreIndex</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>index <span class="op">=</span> VectorStoreIndex(</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    nodes<span class="op">=</span>nodes,</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    vector_store<span class="op">=</span>vector_store,</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This index can already be used to retrieve documents from the database (by converting it to a <em>retriever</em>).</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>retriever <span class="op">=</span> index.as_retriever(similarity_top_k<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>retriever.retrieve(<span class="st">'What do the terms Key, Value and Query stand for in self-attention?'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>[NodeWithScore(node=TextNode(id_='f179163e-b751-462b-bc33-878bd1d4e8e8', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={&lt;NodeRelationship.SOURCE: '1'&gt;: RelatedNodeInfo(node_id='7fda0fb0-6e63-4560-b1f6-e5bc9722ab1a', node_type=&lt;ObjectType.DOCUMENT: '4'&gt;, metadata={}, hash='69ab46ef913b8936b3669fb47d6b591b43a5dd0ca37a3e2dbf0ce04d8c703765'), &lt;NodeRelationship.PREVIOUS: '2'&gt;: RelatedNodeInfo(node_id='a91dc640-1e5f-4ae0-9b12-2d299c219be9', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='2d5b4fe94c35c42fb9f0851b43257320ec9ad5c63b127e463fa2215993f117fc'), &lt;NodeRelationship.NEXT: '3'&gt;: RelatedNodeInfo(node_id='01a0f046-3e0f-457c-a37e-7ba8ea82cca5', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='b8e28f60c384f88a38758a9b8df2ec53ef4316e372031c3bbd60f545ae0ae44f')}, text='attention function can be described as mapping a query and a set of key-value pairs to an output,', mimetype='text/plain', start_char_idx=11228, end_char_idx=11325, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.6689261429690226),
 NodeWithScore(node=TextNode(id_='2b89faef-3d46-401f-b3b0-26006401feef', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={&lt;NodeRelationship.SOURCE: '1'&gt;: RelatedNodeInfo(node_id='7fda0fb0-6e63-4560-b1f6-e5bc9722ab1a', node_type=&lt;ObjectType.DOCUMENT: '4'&gt;, metadata={}, hash='69ab46ef913b8936b3669fb47d6b591b43a5dd0ca37a3e2dbf0ce04d8c703765'), &lt;NodeRelationship.PREVIOUS: '2'&gt;: RelatedNodeInfo(node_id='e1d5b4df-5d1c-439d-9775-5088c63eb9f3', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='b39aebf8977b80cbe0d8dd5fa04a19223f1e818c235371d6af215d0556100bc0'), &lt;NodeRelationship.NEXT: '3'&gt;: RelatedNodeInfo(node_id='0aa4a8f3-4af2-4fe0-9358-1ae72fe8786f', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='d6c4dee83e22dc8559d7a39700c28e907b50c8e479fd5af1941db4408208cf69')}, text='In a self-attention layer all of the keys,', mimetype='text/plain', start_char_idx=16067, end_char_idx=16109, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.6140070285724392),
 NodeWithScore(node=TextNode(id_='44972890-4447-4cb5-852f-e1486eed0bdd', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={&lt;NodeRelationship.SOURCE: '1'&gt;: RelatedNodeInfo(node_id='7fda0fb0-6e63-4560-b1f6-e5bc9722ab1a', node_type=&lt;ObjectType.DOCUMENT: '4'&gt;, metadata={}, hash='69ab46ef913b8936b3669fb47d6b591b43a5dd0ca37a3e2dbf0ce04d8c703765'), &lt;NodeRelationship.PREVIOUS: '2'&gt;: RelatedNodeInfo(node_id='055b5c0c-7837-405b-b65a-5dd5057880e3', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='aa669bf3b7256481cc8755c1c78352a123179dc767de25754ca48fbf23b689bc'), &lt;NodeRelationship.NEXT: '3'&gt;: RelatedNodeInfo(node_id='747a4331-c2ea-4a00-b419-0b08d418ddcd', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='a642151581faab17008e1e3c622d5eec5ed6e9cbea16178e5ac24dafefa4acd8')}, text='of performing a single attention function with dmodel -dimensional keys, values and queries,', mimetype='text/plain', start_char_idx=13777, end_char_idx=13869, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.6003160602014549),
 NodeWithScore(node=TextNode(id_='18e49238-7647-4909-bfe4-23a35d4c5ca3', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={&lt;NodeRelationship.SOURCE: '1'&gt;: RelatedNodeInfo(node_id='7fda0fb0-6e63-4560-b1f6-e5bc9722ab1a', node_type=&lt;ObjectType.DOCUMENT: '4'&gt;, metadata={}, hash='69ab46ef913b8936b3669fb47d6b591b43a5dd0ca37a3e2dbf0ce04d8c703765'), &lt;NodeRelationship.PREVIOUS: '2'&gt;: RelatedNodeInfo(node_id='95f73bbf-b989-433a-b1a6-ebd858241187', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='f6cbc386d726e01d6d2a7c175368263f644c5ce71b85c105929e485dcf0b9203'), &lt;NodeRelationship.NEXT: '3'&gt;: RelatedNodeInfo(node_id='efc76fc4-7ee5-494a-935d-06a9efd571d9', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='5439b8ee1db45977a39995f337377707d150ef741a4180c98a55ce744ac13692')}, text='we vary the number of attention heads and the attention key and value dimensions,', mimetype='text/plain', start_char_idx=32587, end_char_idx=32668, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.5878171880241171),
 NodeWithScore(node=TextNode(id_='a0d046f8-7251-4b55-8457-78e688ef82d1', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={&lt;NodeRelationship.SOURCE: '1'&gt;: RelatedNodeInfo(node_id='7fda0fb0-6e63-4560-b1f6-e5bc9722ab1a', node_type=&lt;ObjectType.DOCUMENT: '4'&gt;, metadata={}, hash='69ab46ef913b8936b3669fb47d6b591b43a5dd0ca37a3e2dbf0ce04d8c703765'), &lt;NodeRelationship.PREVIOUS: '2'&gt;: RelatedNodeInfo(node_id='b4b6a870-d5e7-43b8-a98b-e5660829fa7e', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='7f9189e6569cc76c3e4c706c4a298ce83039920dda5ff4b64ec3d2a2ecd17d16'), &lt;NodeRelationship.NEXT: '3'&gt;: RelatedNodeInfo(node_id='dad4c0dd-260f-43cb-8997-8668ac1ca95e', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='5cf958d495be43203293644ac0c78bb1ae632d95f5eac85a487a134eeb8f83e1')}, text='keys and values we then perform the attention function in parallel,', mimetype='text/plain', start_char_idx=14085, end_char_idx=14152, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.5599990554034037),
 NodeWithScore(node=TextNode(id_='5aeffddb-5e61-4bd2-b7c7-0908b617345c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={&lt;NodeRelationship.SOURCE: '1'&gt;: RelatedNodeInfo(node_id='7fda0fb0-6e63-4560-b1f6-e5bc9722ab1a', node_type=&lt;ObjectType.DOCUMENT: '4'&gt;, metadata={}, hash='69ab46ef913b8936b3669fb47d6b591b43a5dd0ca37a3e2dbf0ce04d8c703765'), &lt;NodeRelationship.PREVIOUS: '2'&gt;: RelatedNodeInfo(node_id='20d4b9c4-38cb-4c61-9ae9-2d79fa770f8b', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='8be71b432f8f0abcddbd647735091a419597e3c6b174d427210c39d82d3357db'), &lt;NodeRelationship.NEXT: '3'&gt;: RelatedNodeInfo(node_id='85add884-1b17-487d-8136-3faed207f79a', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='8e3333b3547e738a6d3fffa8d21dcbd4a8d6b56b6f8cd8f1ccf263541439d80e')}, text='2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different', mimetype='text/plain', start_char_idx=8000, end_char_idx=8096, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.5510363892301526),
 NodeWithScore(node=TextNode(id_='e14b73d1-ee56-46c4-974b-6d6f2401304b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={&lt;NodeRelationship.SOURCE: '1'&gt;: RelatedNodeInfo(node_id='7fda0fb0-6e63-4560-b1f6-e5bc9722ab1a', node_type=&lt;ObjectType.DOCUMENT: '4'&gt;, metadata={}, hash='69ab46ef913b8936b3669fb47d6b591b43a5dd0ca37a3e2dbf0ce04d8c703765'), &lt;NodeRelationship.PREVIOUS: '2'&gt;: RelatedNodeInfo(node_id='2ceaf2a5-d16b-46a8-bd1a-6ab3eb650c52', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='d99e4e0b99cb6641047a17c6e9da3a59b0d4ea0df2f0b213ca47d30d333ebac1'), &lt;NodeRelationship.NEXT: '3'&gt;: RelatedNodeInfo(node_id='c23c3273-7cf2-47c8-9fd5-b47f2cf6e10f', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='4aa20349e1dd87216ce6467799ecd9f96716d0de4be21cdd0d33d83cd860c3f1')}, text='of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension,', mimetype='text/plain', start_char_idx=8165, end_char_idx=8278, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.5396821423481503),
 NodeWithScore(node=TextNode(id_='7ca1930d-752c-4f19-aa10-0f1b26a06f3b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={&lt;NodeRelationship.SOURCE: '1'&gt;: RelatedNodeInfo(node_id='7fda0fb0-6e63-4560-b1f6-e5bc9722ab1a', node_type=&lt;ObjectType.DOCUMENT: '4'&gt;, metadata={}, hash='69ab46ef913b8936b3669fb47d6b591b43a5dd0ca37a3e2dbf0ce04d8c703765'), &lt;NodeRelationship.PREVIOUS: '2'&gt;: RelatedNodeInfo(node_id='10642476-b727-4743-b01b-52096a29fce9', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='a153765fee9f0efcfb6804fd5bc8312f9f13f9c282fd03dd49247f24dbc0c25b'), &lt;NodeRelationship.NEXT: '3'&gt;: RelatedNodeInfo(node_id='fd1e0285-6636-428c-be1d-d29707f22d2f', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='7b536fd47292343852aa48f5586a9d0c1290c7c340aeb49708c8f7ebc2b2da5a')}, text='In practice, we compute the attention function on a set of queries simultaneously,', mimetype='text/plain', start_char_idx=12231, end_char_idx=12313, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.5200972369180269),
 NodeWithScore(node=TextNode(id_='747a4331-c2ea-4a00-b419-0b08d418ddcd', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={&lt;NodeRelationship.SOURCE: '1'&gt;: RelatedNodeInfo(node_id='7fda0fb0-6e63-4560-b1f6-e5bc9722ab1a', node_type=&lt;ObjectType.DOCUMENT: '4'&gt;, metadata={}, hash='69ab46ef913b8936b3669fb47d6b591b43a5dd0ca37a3e2dbf0ce04d8c703765'), &lt;NodeRelationship.PREVIOUS: '2'&gt;: RelatedNodeInfo(node_id='44972890-4447-4cb5-852f-e1486eed0bdd', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='5bfecc13dc676c3effef63180d8e345a59aee3b00971ee632c28a5beb3e936d4'), &lt;NodeRelationship.NEXT: '3'&gt;: RelatedNodeInfo(node_id='35eabe69-ac0e-4d8f-af59-d8c3a75e0394', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='5190a75cc39636a77d1a3db884b7cb86f0afe3c8c5fba1701abbb266740d5bdd')}, text='values and queries,\nwe found it beneficial to linearly project the queries,', mimetype='text/plain', start_char_idx=13850, end_char_idx=13925, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.5128181572746763),
 NodeWithScore(node=TextNode(id_='c310de50-a2a5-4642-be1c-cdea5d9afcf1', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={&lt;NodeRelationship.SOURCE: '1'&gt;: RelatedNodeInfo(node_id='7fda0fb0-6e63-4560-b1f6-e5bc9722ab1a', node_type=&lt;ObjectType.DOCUMENT: '4'&gt;, metadata={}, hash='69ab46ef913b8936b3669fb47d6b591b43a5dd0ca37a3e2dbf0ce04d8c703765'), &lt;NodeRelationship.PREVIOUS: '2'&gt;: RelatedNodeInfo(node_id='ff0f74c7-ddfe-461b-aa5f-ed4e49cb9e8f', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='8d64e319bfdf6f04a0917a395eada4c0a280701c07b475000ff2de68606bca76'), &lt;NodeRelationship.NEXT: '3'&gt;: RelatedNodeInfo(node_id='72169dc7-1426-4e59-a91e-b6ddd4c850d6', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='78736791801c79f892ed19fd6161ad7c8524bc7a9c57b4c0c01121be99dddb18')}, text='Operations\n      Self-Attention', mimetype='text/plain', start_char_idx=18618, end_char_idx=18649, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.5069105137924549)]</code></pre>
<p>The retriever can then directly be use as a tool to answer questions about our documents:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> llama_index.core.tools <span class="im">import</span> BaseTool, FunctionTool</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> find_references(question: <span class="bu">str</span>) <span class="op">-&gt;</span> <span class="bu">str</span>:</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Query a database containing the paper "Attention is all you Need" in parts.</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co">    This paper introduced the mechanism of self-attention to the NLP-literature.</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns a collection of scored text-snippets that are relevant to your question."""</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="st">'</span><span class="ch">\n</span><span class="st">'</span>.join([<span class="ss">f'</span><span class="sc">{</span><span class="bu">round</span>(n.score,<span class="dv">2</span>)<span class="sc">}</span><span class="ss"> - </span><span class="sc">{</span>n<span class="sc">.</span>node<span class="sc">.</span>text<span class="sc">}</span><span class="ss">'</span> <span class="cf">for</span> n <span class="kw">in</span> retriever.retrieve(q)])</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>find_references_tool <span class="op">=</span> FunctionTool.from_defaults(fn<span class="op">=</span>find_references)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This tool can then be added to an agent as we discussed before:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> llama_index.core.agent <span class="im">import</span> ReActAgent</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> llama_index.llms.lmstudio <span class="im">import</span> LMStudio</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>llm <span class="op">=</span> LMStudio(model_name<span class="op">=</span><span class="st">"llama-3.2-1b-instruct"</span>,</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>        base_url<span class="op">=</span><span class="st">"http://localhost:1234/v1"</span>,</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    temperature<span class="op">=</span><span class="fl">0.5</span>,</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>    request_timeout<span class="op">=</span><span class="dv">600</span>)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>agent <span class="op">=</span> ReActAgent.from_tools(tools<span class="op">=</span>[find_references_tool],llm<span class="op">=</span>llm, verbose<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>/home/brede/MEGA/Honorar/Generative AI/script/.venv/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field "model_name" in LMStudio has conflict with protected namespace "model_".

You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.
  warnings.warn(</code></pre>
<p>Which can then be used to answer chat-requests:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> agent.chat(<span class="st">"What is the meaning of Query, Key and Value in the context of self-attention?"</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">str</span>(response))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>&gt; Running step e29ee90e-3a6a-4863-8356-f24382cd4f7b. Step input: What is the meaning of Query, Key and Value in the context of self-attention?
Observation: Error: Could not parse output. Please follow the thought-action-input format. Try again.
&gt; Running step a49b74d8-09cf-4a51-87c0-e193bb82c5d9. Step input: None
Observation: Error: Could not parse output. Please follow the thought-action-input format. Try again.
&gt; Running step e9da34c8-e36f-4ba0-93aa-2871e5f5cb81. Step input: None
Observation: Error: Could not parse output. Please follow the thought-action-input format. Try again.
&gt; Running step 6f6775ff-5c83-46e0-b130-53726789cb61. Step input: None
Observation: Error: Could not parse output. Please follow the thought-action-input format. Try again.
&gt; Running step 20563141-4fb7-42dc-8ad7-6d7113468fa9. Step input: None
Observation: Error: Could not parse output. Please follow the thought-action-input format. Try again.
&gt; Running step 280d7afb-c916-4b54-91aa-0764560b0f14. Step input: None
Observation: Error: Could not parse output. Please follow the thought-action-input format. Try again.
&gt; Running step e61a4039-d100-4ce6-9043-ef10c85bf8f0. Step input: None
Observation: Error: Could not parse output. Please follow the thought-action-input format. Try again.
&gt; Running step 6cb20084-238d-4822-90d9-3d5429482ef2. Step input: None
Observation: Error: Could not parse output. Please follow the thought-action-input format. Try again.
&gt; Running step 6eab1685-2ed3-4144-9d3d-6c7b871e60c9. Step input: None
Thought: In the context of self-attention, Query, Key and Value refer to three important components used in the process of computing attention scores for a given input sequence.
Action: I'll
Action Input: {'input': 'hello world', 'num_beams': 5}
Observation: Error: No such tool named `I'll`.

ValueError: Reached max iterations.
[0;31m---------------------------------------------------------------------------[0m
[0;31mValueError[0m                                Traceback (most recent call last)
Cell [0;32mIn[10], line 1[0m
[0;32m----&gt; 1[0m response [38;5;241m=[39m [43magent[49m[38;5;241;43m.[39;49m[43mchat[49m[43m([49m[38;5;124;43m"[39;49m[38;5;124;43mWhat is the meaning of Query, Key and Value in the context of self-attention?[39;49m[38;5;124;43m"[39;49m[43m)[49m
[1;32m      2[0m [38;5;28mprint[39m([38;5;28mstr[39m(response))

File [0;32m~/MEGA/Honorar/Generative AI/script/.venv/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:311[0m, in [0;36mDispatcher.span.&lt;locals&gt;.wrapper[0;34m(func, instance, args, kwargs)[0m
[1;32m    308[0m             _logger[38;5;241m.[39mdebug([38;5;124mf[39m[38;5;124m"[39m[38;5;124mFailed to reset active_span_id: [39m[38;5;132;01m{[39;00me[38;5;132;01m}[39;00m[38;5;124m"[39m)
[1;32m    310[0m [38;5;28;01mtry[39;00m:
[0;32m--&gt; 311[0m     result [38;5;241m=[39m [43mfunc[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
[1;32m    312[0m     [38;5;28;01mif[39;00m [38;5;28misinstance[39m(result, asyncio[38;5;241m.[39mFuture):
[1;32m    313[0m         [38;5;66;03m# If the result is a Future, wrap it[39;00m
[1;32m    314[0m         new_future [38;5;241m=[39m asyncio[38;5;241m.[39mensure_future(result)

File [0;32m~/MEGA/Honorar/Generative AI/script/.venv/lib/python3.10/site-packages/llama_index/core/callbacks/utils.py:41[0m, in [0;36mtrace_method.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper[0;34m(self, *args, **kwargs)[0m
[1;32m     39[0m callback_manager [38;5;241m=[39m cast(CallbackManager, callback_manager)
[1;32m     40[0m [38;5;28;01mwith[39;00m callback_manager[38;5;241m.[39mas_trace(trace_id):
[0;32m---&gt; 41[0m     [38;5;28;01mreturn[39;00m [43mfunc[49m[43m([49m[38;5;28;43mself[39;49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/MEGA/Honorar/Generative AI/script/.venv/lib/python3.10/site-packages/llama_index/core/agent/runner/base.py:647[0m, in [0;36mAgentRunner.chat[0;34m(self, message, chat_history, tool_choice)[0m
[1;32m    642[0m     tool_choice [38;5;241m=[39m [38;5;28mself[39m[38;5;241m.[39mdefault_tool_choice
[1;32m    643[0m [38;5;28;01mwith[39;00m [38;5;28mself[39m[38;5;241m.[39mcallback_manager[38;5;241m.[39mevent(
[1;32m    644[0m     CBEventType[38;5;241m.[39mAGENT_STEP,
[1;32m    645[0m     payload[38;5;241m=[39m{EventPayload[38;5;241m.[39mMESSAGES: [message]},
[1;32m    646[0m ) [38;5;28;01mas[39;00m e:
[0;32m--&gt; 647[0m     chat_response [38;5;241m=[39m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_chat[49m[43m([49m
[1;32m    648[0m [43m        [49m[43mmessage[49m[38;5;241;43m=[39;49m[43mmessage[49m[43m,[49m
[1;32m    649[0m [43m        [49m[43mchat_history[49m[38;5;241;43m=[39;49m[43mchat_history[49m[43m,[49m
[1;32m    650[0m [43m        [49m[43mtool_choice[49m[38;5;241;43m=[39;49m[43mtool_choice[49m[43m,[49m
[1;32m    651[0m [43m        [49m[43mmode[49m[38;5;241;43m=[39;49m[43mChatResponseMode[49m[38;5;241;43m.[39;49m[43mWAIT[49m[43m,[49m
[1;32m    652[0m [43m    [49m[43m)[49m
[1;32m    653[0m     [38;5;28;01massert[39;00m [38;5;28misinstance[39m(chat_response, AgentChatResponse)
[1;32m    654[0m     e[38;5;241m.[39mon_end(payload[38;5;241m=[39m{EventPayload[38;5;241m.[39mRESPONSE: chat_response})

File [0;32m~/MEGA/Honorar/Generative AI/script/.venv/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:311[0m, in [0;36mDispatcher.span.&lt;locals&gt;.wrapper[0;34m(func, instance, args, kwargs)[0m
[1;32m    308[0m             _logger[38;5;241m.[39mdebug([38;5;124mf[39m[38;5;124m"[39m[38;5;124mFailed to reset active_span_id: [39m[38;5;132;01m{[39;00me[38;5;132;01m}[39;00m[38;5;124m"[39m)
[1;32m    310[0m [38;5;28;01mtry[39;00m:
[0;32m--&gt; 311[0m     result [38;5;241m=[39m [43mfunc[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
[1;32m    312[0m     [38;5;28;01mif[39;00m [38;5;28misinstance[39m(result, asyncio[38;5;241m.[39mFuture):
[1;32m    313[0m         [38;5;66;03m# If the result is a Future, wrap it[39;00m
[1;32m    314[0m         new_future [38;5;241m=[39m asyncio[38;5;241m.[39mensure_future(result)

File [0;32m~/MEGA/Honorar/Generative AI/script/.venv/lib/python3.10/site-packages/llama_index/core/agent/runner/base.py:579[0m, in [0;36mAgentRunner._chat[0;34m(self, message, chat_history, tool_choice, mode)[0m
[1;32m    576[0m dispatcher[38;5;241m.[39mevent(AgentChatWithStepStartEvent(user_msg[38;5;241m=[39mmessage))
[1;32m    577[0m [38;5;28;01mwhile[39;00m [38;5;28;01mTrue[39;00m:
[1;32m    578[0m     [38;5;66;03m# pass step queue in as argument, assume step executor is stateless[39;00m
[0;32m--&gt; 579[0m     cur_step_output [38;5;241m=[39m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_run_step[49m[43m([49m
[1;32m    580[0m [43m        [49m[43mtask[49m[38;5;241;43m.[39;49m[43mtask_id[49m[43m,[49m[43m [49m[43mmode[49m[38;5;241;43m=[39;49m[43mmode[49m[43m,[49m[43m [49m[43mtool_choice[49m[38;5;241;43m=[39;49m[43mtool_choice[49m
[1;32m    581[0m [43m    [49m[43m)[49m
[1;32m    583[0m     [38;5;28;01mif[39;00m cur_step_output[38;5;241m.[39mis_last:
[1;32m    584[0m         result_output [38;5;241m=[39m cur_step_output

File [0;32m~/MEGA/Honorar/Generative AI/script/.venv/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:311[0m, in [0;36mDispatcher.span.&lt;locals&gt;.wrapper[0;34m(func, instance, args, kwargs)[0m
[1;32m    308[0m             _logger[38;5;241m.[39mdebug([38;5;124mf[39m[38;5;124m"[39m[38;5;124mFailed to reset active_span_id: [39m[38;5;132;01m{[39;00me[38;5;132;01m}[39;00m[38;5;124m"[39m)
[1;32m    310[0m [38;5;28;01mtry[39;00m:
[0;32m--&gt; 311[0m     result [38;5;241m=[39m [43mfunc[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
[1;32m    312[0m     [38;5;28;01mif[39;00m [38;5;28misinstance[39m(result, asyncio[38;5;241m.[39mFuture):
[1;32m    313[0m         [38;5;66;03m# If the result is a Future, wrap it[39;00m
[1;32m    314[0m         new_future [38;5;241m=[39m asyncio[38;5;241m.[39mensure_future(result)

File [0;32m~/MEGA/Honorar/Generative AI/script/.venv/lib/python3.10/site-packages/llama_index/core/agent/runner/base.py:412[0m, in [0;36mAgentRunner._run_step[0;34m(self, task_id, step, input, mode, **kwargs)[0m
[1;32m    408[0m [38;5;66;03m# TODO: figure out if you can dynamically swap in different step executors[39;00m
[1;32m    409[0m [38;5;66;03m# not clear when you would do that by theoretically possible[39;00m
[1;32m    411[0m [38;5;28;01mif[39;00m mode [38;5;241m==[39m ChatResponseMode[38;5;241m.[39mWAIT:
[0;32m--&gt; 412[0m     cur_step_output [38;5;241m=[39m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43magent_worker[49m[38;5;241;43m.[39;49m[43mrun_step[49m[43m([49m[43mstep[49m[43m,[49m[43m [49m[43mtask[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
[1;32m    413[0m [38;5;28;01melif[39;00m mode [38;5;241m==[39m ChatResponseMode[38;5;241m.[39mSTREAM:
[1;32m    414[0m     cur_step_output [38;5;241m=[39m [38;5;28mself[39m[38;5;241m.[39magent_worker[38;5;241m.[39mstream_step(step, task, [38;5;241m*[39m[38;5;241m*[39mkwargs)

File [0;32m~/MEGA/Honorar/Generative AI/script/.venv/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:311[0m, in [0;36mDispatcher.span.&lt;locals&gt;.wrapper[0;34m(func, instance, args, kwargs)[0m
[1;32m    308[0m             _logger[38;5;241m.[39mdebug([38;5;124mf[39m[38;5;124m"[39m[38;5;124mFailed to reset active_span_id: [39m[38;5;132;01m{[39;00me[38;5;132;01m}[39;00m[38;5;124m"[39m)
[1;32m    310[0m [38;5;28;01mtry[39;00m:
[0;32m--&gt; 311[0m     result [38;5;241m=[39m [43mfunc[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
[1;32m    312[0m     [38;5;28;01mif[39;00m [38;5;28misinstance[39m(result, asyncio[38;5;241m.[39mFuture):
[1;32m    313[0m         [38;5;66;03m# If the result is a Future, wrap it[39;00m
[1;32m    314[0m         new_future [38;5;241m=[39m asyncio[38;5;241m.[39mensure_future(result)

File [0;32m~/MEGA/Honorar/Generative AI/script/.venv/lib/python3.10/site-packages/llama_index/core/callbacks/utils.py:41[0m, in [0;36mtrace_method.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper[0;34m(self, *args, **kwargs)[0m
[1;32m     39[0m callback_manager [38;5;241m=[39m cast(CallbackManager, callback_manager)
[1;32m     40[0m [38;5;28;01mwith[39;00m callback_manager[38;5;241m.[39mas_trace(trace_id):
[0;32m---&gt; 41[0m     [38;5;28;01mreturn[39;00m [43mfunc[49m[43m([49m[38;5;28;43mself[39;49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/MEGA/Honorar/Generative AI/script/.venv/lib/python3.10/site-packages/llama_index/core/agent/react/step.py:818[0m, in [0;36mReActAgentWorker.run_step[0;34m(self, step, task, **kwargs)[0m
[1;32m    815[0m [38;5;129m@trace_method[39m([38;5;124m"[39m[38;5;124mrun_step[39m[38;5;124m"[39m)
[1;32m    816[0m [38;5;28;01mdef[39;00m [38;5;21mrun_step[39m([38;5;28mself[39m, step: TaskStep, task: Task, [38;5;241m*[39m[38;5;241m*[39mkwargs: Any) [38;5;241m-[39m[38;5;241m&gt;[39m TaskStepOutput:
[1;32m    817[0m [38;5;250m    [39m[38;5;124;03m"""Run step."""[39;00m
[0;32m--&gt; 818[0m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_run_step[49m[43m([49m[43mstep[49m[43m,[49m[43m [49m[43mtask[49m[43m)[49m

File [0;32m~/MEGA/Honorar/Generative AI/script/.venv/lib/python3.10/site-packages/llama_index/core/agent/react/step.py:576[0m, in [0;36mReActAgentWorker._run_step[0;34m(self, step, task)[0m
[1;32m    572[0m reasoning_steps, is_done [38;5;241m=[39m [38;5;28mself[39m[38;5;241m.[39m_process_actions(
[1;32m    573[0m     task, tools, output[38;5;241m=[39mchat_response
[1;32m    574[0m )
[1;32m    575[0m task[38;5;241m.[39mextra_state[[38;5;124m"[39m[38;5;124mcurrent_reasoning[39m[38;5;124m"[39m][38;5;241m.[39mextend(reasoning_steps)
[0;32m--&gt; 576[0m agent_response [38;5;241m=[39m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_get_response[49m[43m([49m
[1;32m    577[0m [43m    [49m[43mtask[49m[38;5;241;43m.[39;49m[43mextra_state[49m[43m[[49m[38;5;124;43m"[39;49m[38;5;124;43mcurrent_reasoning[39;49m[38;5;124;43m"[39;49m[43m][49m[43m,[49m[43m [49m[43mtask[49m[38;5;241;43m.[39;49m[43mextra_state[49m[43m[[49m[38;5;124;43m"[39;49m[38;5;124;43msources[39;49m[38;5;124;43m"[39;49m[43m][49m
[1;32m    578[0m [43m[49m[43m)[49m
[1;32m    579[0m [38;5;28;01mif[39;00m is_done:
[1;32m    580[0m     task[38;5;241m.[39mextra_state[[38;5;124m"[39m[38;5;124mnew_memory[39m[38;5;124m"[39m][38;5;241m.[39mput(
[1;32m    581[0m         ChatMessage(content[38;5;241m=[39magent_response[38;5;241m.[39mresponse, role[38;5;241m=[39mMessageRole[38;5;241m.[39mASSISTANT)
[1;32m    582[0m     )

File [0;32m~/MEGA/Honorar/Generative AI/script/.venv/lib/python3.10/site-packages/llama_index/core/agent/react/step.py:437[0m, in [0;36mReActAgentWorker._get_response[0;34m(self, current_reasoning, sources)[0m
[1;32m    435[0m     [38;5;28;01mraise[39;00m [38;5;167;01mValueError[39;00m([38;5;124m"[39m[38;5;124mNo reasoning steps were taken.[39m[38;5;124m"[39m)
[1;32m    436[0m [38;5;28;01melif[39;00m [38;5;28mlen[39m(current_reasoning) [38;5;241m==[39m [38;5;28mself[39m[38;5;241m.[39m_max_iterations:
[0;32m--&gt; 437[0m     [38;5;28;01mraise[39;00m [38;5;167;01mValueError[39;00m([38;5;124m"[39m[38;5;124mReached max iterations.[39m[38;5;124m"[39m)
[1;32m    439[0m [38;5;28;01mif[39;00m [38;5;28misinstance[39m(current_reasoning[[38;5;241m-[39m[38;5;241m1[39m], ResponseReasoningStep):
[1;32m    440[0m     response_step [38;5;241m=[39m cast(ResponseReasoningStep, current_reasoning[[38;5;241m-[39m[38;5;241m1[39m])

[0;31mValueError[0m: Reached max iterations.</code></pre>
<p>As you can see, the model request ends up with errors. The model is not powerful enough to answer in the structured manner we need for the function-calling of the tool. To circumvent this, we can try a function-calling-finetuned model:</p>
<p>We can try to solve this issue by using a language model that is finetuned on function calling:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>fc_llm <span class="op">=</span> LMStudio(model_name<span class="op">=</span><span class="st">"phi-3-mini-4k-instruct-function-calling"</span>,</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>        base_url<span class="op">=</span><span class="st">"http://localhost:1234/v1"</span>,</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    temperature<span class="op">=</span><span class="fl">0.2</span>,</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    request_timeout<span class="op">=</span><span class="dv">600</span>)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>agent <span class="op">=</span> ReActAgent.from_tools(tools<span class="op">=</span>[find_references_tool],llm<span class="op">=</span>fc_llm, verbose<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> agent.chat(<span class="st">"What is the meaning of Query, Key and Value in the context of self-attention?"</span>)</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">str</span>(response))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>&gt; Running step 717acece-81ed-4bda-9cad-23a7184fc33c. Step input: What is the meaning of Query, Key and Value in the context of self-attention?
Thought: (Implicit) I can answer without any more tools!
Answer:  In the context of self-attention, "Query" refers to a vector that represents the current input or query. It acts as a filter for determining which parts of the input should be attended to more heavily.

"Key" refers to another set of vectors associated with each element in the input sequence. These keys are used by the self-attention mechanism to determine how much attention should be given to each part of the input. The output of the query-key computation is a weighting factor that determines the relative importance of each element in the input.

Finally, "Value" refers to the actual content associated with each element in the input sequence. These values are combined with the weighted keys using a specified operation (usually some form of matrix multiplication) to produce an output vector that represents the attended-to information from the input.
 In the context of self-attention, "Query" refers to a vector that represents the current input or query. It acts as a filter for determining which parts of the input should be attended to more heavily.

"Key" refers to another set of vectors associated with each element in the input sequence. These keys are used by the self-attention mechanism to determine how much attention should be given to each part of the input. The output of the query-key computation is a weighting factor that determines the relative importance of each element in the input.

Finally, "Value" refers to the actual content associated with each element in the input sequence. These values are combined with the weighted keys using a specified operation (usually some form of matrix multiplication) to produce an output vector that represents the attended-to information from the input.</code></pre>
<p>This model does not run into an issue with the structured output, it does not try to use the tool anymore though.</p>
<p>One way to try to solve this issue is to adapt the agent-prompt:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(agent.get_prompts()[<span class="st">'agent_worker:system_prompt'</span>].template)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>You are designed to help with a variety of tasks, from answering questions to providing summaries to other types of analyses.

## Tools

You have access to a wide variety of tools. You are responsible for using the tools in any sequence you deem appropriate to complete the task at hand.
This may require breaking the task into subtasks and using different tools to complete each subtask.

You have access to the following tools:
{tool_desc}


## Output Format

Please answer in the same language as the question and use the following format:

```
Thought: The current language of the user is: (user's language). I need to use a tool to help me answer the question.
Action: tool name (one of {tool_names}) if using a tool.
Action Input: the input to the tool, in a JSON format representing the kwargs (e.g. {{"input": "hello world", "num_beams": 5}})
```

Please ALWAYS start with a Thought.

NEVER surround your response with markdown code markers. You may use code markers within your response if you need to.

Please use a valid JSON format for the Action Input. Do NOT do this {{'input': 'hello world', 'num_beams': 5}}.

If this format is used, the user will respond in the following format:

```
Observation: tool response
```

You should keep repeating the above format till you have enough information to answer the question without using any more tools. At that point, you MUST respond in one of the following two formats:

```
Thought: I can answer without using any more tools. I'll use the user's language to answer
Answer: [your answer here (In the same language as the user's question)]
```

```
Thought: I cannot answer the question with the provided tools.
Answer: [your answer here (In the same language as the user's question)]
```

## Current Conversation

Below is the current conversation consisting of interleaving human and assistant messages.</code></pre>
<p>This we can adapt in the following way:</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> llama_index.core <span class="im">import</span> PromptTemplate</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>new_agent_template_str <span class="op">=</span> <span class="st">"""</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="st">You are designed to help answer questions based on a collection of paper-excerpts.</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="st">## Tools</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="st">You have access to tools that allow you to query paper-content. You are responsible for using the tools in any sequence you deem appropriate to complete the task at hand.</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="st">This may require breaking the task into subtasks and using different tools to complete each subtask. Do not answer without tool-usage if a tool can be used to answer a question. Do try to find a text passage to back up your claims whenever possible. Do not answer without reference if the appropriate text is available in the tools you have access to.</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a><span class="st">You have access to the following tools:</span></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a><span class="sc">{tool_desc}</span></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a><span class="st">## Output Format</span></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a><span class="st">Please answer in the same language as the question and use the following format:</span></span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a><span class="st">\`\`\`</span></span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a><span class="st">Thought: The current language of the user is: (user's language). I need to use a tool to help me answer the question.</span></span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a><span class="st">Action: tool name (one of </span><span class="sc">{tool_names}</span><span class="st">) if using a tool.</span></span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a><span class="st">Action Input: the input to the tool, in a JSON format representing the kwargs (e.g. </span><span class="sc">{{</span><span class="st">"input": "hello world", "num_beams": 5</span><span class="sc">}}</span><span class="st">)</span></span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a><span class="st">\`\`\`</span></span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a><span class="st">Please ALWAYS start with a Thought.</span></span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a><span class="st">NEVER surround your response with markdown code markers. You may use code markers within your response if you need to.</span></span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a><span class="st">...</span></span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a><span class="st">## Current Conversation</span></span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a><span class="st">Below is the current conversation consisting of interleaving human and assistant messages.</span></span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span></span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a>new_agent_template <span class="op">=</span> PromptTemplate(new_agent_template_str)</span>
<span id="cb19-34"><a href="#cb19-34" aria-hidden="true" tabindex="-1"></a>agent.update_prompts(</span>
<span id="cb19-35"><a href="#cb19-35" aria-hidden="true" tabindex="-1"></a>    {<span class="st">"agent_worker:system_prompt"</span>: new_agent_template}</span>
<span id="cb19-36"><a href="#cb19-36" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>We can test this new prompt with the same question:</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> agent.chat(<span class="st">"What is the meaning of Query, Key and Value in the context of self-attention?"</span>)</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">str</span>(response))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>&gt; Running step 6d31cdb7-9acb-4322-8b13-42e869d76a8d. Step input: What is the meaning of Query, Key and Value in the context of self-attention?
Thought: (Implicit) I can answer without any more tools!
Answer:  In the context of self-attention, "Query" refers to a vector that represents the current input or query. It acts as a filter for determining which parts of the input should be attended to more heavily.

"Key" refers to another set of vectors associated with each element in the input sequence. These keys are used by the self-attention mechanism to determine how much attention should be given to each part of the input. The output of the query-key computation is a weighting factor that determines the relative importance of each element in the input.

Finally, "Value" refers to the actual content associated with each element in the input sequence. These values are combined with the weighted keys using a specified operation (usually some form of matrix multiplication) to produce an output vector that represents the attended-to information from the input.
 In the context of self-attention, "Query" refers to a vector that represents the current input or query. It acts as a filter for determining which parts of the input should be attended to more heavily.

"Key" refers to another set of vectors associated with each element in the input sequence. These keys are used by the self-attention mechanism to determine how much attention should be given to each part of the input. The output of the query-key computation is a weighting factor that determines the relative importance of each element in the input.

Finally, "Value" refers to the actual content associated with each element in the input sequence. These values are combined with the weighted keys using a specified operation (usually some form of matrix multiplication) to produce an output vector that represents the attended-to information from the input.</code></pre>
<p>The model still tries to answer without the tool.</p>
<p>Let’s try to ask a more specific question:</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> agent.chat(<span class="st">"How does the paper 'Attention is all you need' define the term self attention?"</span>)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">str</span>(response))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>&gt; Running step 550e01a4-d723-4c9b-ab96-44f163f1284f. Step input: How does the paper 'Attention is all you need' define the term self attention?
Thought: (Implicit) I can answer without any more tools!
Answer:  The paper "Attention Is All You Need" defines self-attention as a mechanism for computing a weighted sum of values, where the weights are computed by performing a dot product between keys and queries, followed by applying a softmax function to obtain normalized probabilities. This allows each element in the input sequence to attend selectively to other elements based on their relative importance, which is determined by the similarity between the key and query vectors. The resulting weighted sum of values forms the output representation for that particular position in the sequence.
 The paper "Attention Is All You Need" defines self-attention as a mechanism for computing a weighted sum of values, where the weights are computed by performing a dot product between keys and queries, followed by applying a softmax function to obtain normalized probabilities. This allows each element in the input sequence to attend selectively to other elements based on their relative importance, which is determined by the similarity between the key and query vectors. The resulting weighted sum of values forms the output representation for that particular position in the sequence.</code></pre>
<p>Still no dice.</p>
<p>One solution to this problem is to just use a bigger model:</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>llm <span class="op">=</span> LMStudio(model_name<span class="op">=</span><span class="st">"llama-3.2-3b-instruct"</span>, <span class="co">#3 Billion instead of 1</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>        base_url<span class="op">=</span><span class="st">"http://localhost:1234/v1"</span>,</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>    temperature<span class="op">=</span><span class="fl">0.2</span>,</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>    request_timeout<span class="op">=</span><span class="dv">600</span>)</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>agent <span class="op">=</span> ReActAgent.from_tools(tools<span class="op">=</span>[find_references_tool],llm<span class="op">=</span>llm, verbose<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> agent.chat(<span class="st">"How does the paper 'Attention is all you need' define the term self attention?"</span>)</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">str</span>(response))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>&gt; Running step ca2821e4-0d40-4619-9291-dc8594d537c8. Step input: How does the paper 'Attention is all you need' define the term self attention?
Thought: The current language of the user is English. I need to use a tool to help me answer the question.
Action: find_references
Action Input: {'question': 'What defines self-attention in the context of Attention is all you Need', 'properties': AttributedDict([('title', 'Question'), ('type', 'string')])}
Observation: Error: find_references() got an unexpected keyword argument 'properties'
&gt; Running step 99f9538c-7098-44b5-b2f8-700a6cd08da8. Step input: None
Thought: The tool find_references does not accept a 'properties' argument. I should have checked the documentation before using the tool.
Action: research
Action Input: {'query': 'self-attention in Attention is all you need paper definition', 'num_results': 3}
Observation: Error: No such tool named `research`.
&gt; Running step 49191a4e-3189-4c4a-a313-82f437e44cf4. Step input: None
Thought: I don't have a tool called 'research'. I should have used find_references again to get the definition of self-attention.
Action: find_references
Action Input: {'question': 'What defines self-attention in the context of Attention is all you need'}
Observation: Error: name 'q' is not defined
&gt; Running step 170262a7-dacd-4b04-b583-735f06972b4d. Step input: None
Thought: The tool find_references expects a JSON object with a 'question' key, but I passed it as a keyword argument. I should have used the 'question' string directly.
Action: find_references
Action Input: {'question': 'What defines self-attention in the context of Attention is all you need'}
Observation: Error: name 'q' is not defined
&gt; Running step c3dc0f39-6af7-45f0-a68f-8c22a7c3a570. Step input: None
Thought: The error message still indicates that 'q' is not defined, but I passed a valid 'question' string. This suggests that there might be an issue with the tool's implementation.
Answer: Self-attention in the context of the paper "Attention is all you Need" refers to a mechanism where the model attends to all positions in an input sequence simultaneously and weighs their importance. It's a key component of the Transformer architecture, introduced by Vaswani et al. in their 2017 paper, not just "Attention is all you Need".
Self-attention in the context of the paper "Attention is all you Need" refers to a mechanism where the model attends to all positions in an input sequence simultaneously and weighs their importance. It's a key component of the Transformer architecture, introduced by Vaswani et al. in their 2017 paper, not just "Attention is all you Need".</code></pre>
<p>This is not always feasible though.</p>
<p>Another way to use the retrieval-pipeline is to not give a weak model the opportunity to mess up the tool calling. This can be implemented by using a query-engine instead of the retriever. This directly wraps the retrieval in a LLM-Summarization-Module that only returns summaries.</p>
<p>Doing this, we can use two separate models for each part of the task - one for the planning and answering and one for the structured summarization:</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>query_engine <span class="op">=</span> index.as_query_engine(use_async<span class="op">=</span><span class="va">False</span>, llm<span class="op">=</span>fc_llm, verbose<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> query_engine.query(<span class="st">"What is the meaning of Query, Key and Value in the context of self-attention?"</span>)</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">str</span>(response))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code> In the context of self-attention, a "query" represents the input to be processed by the attention mechanism, while "keys" and "values" are derived from the same input. The keys correspond to specific features or aspects of the input data, while the values represent the corresponding outputs that result from applying those key-value pairs through an attention function. Essentially, these three components help facilitate selective focus on different parts of the input data during processing by assigning varying weights to different elements based on their importance in achieving a desired outcome.</code></pre>
<p>Finally an answer we can work with!</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
📝 Task
</div>
</div>
<div class="callout-body-container callout-body">
<p>Build a llamaindex-application that allows you to <a href="https://docs.llamaindex.ai/en/stable/examples/chat_engine/chat_engine_best/">chat</a> with the climate_fever evidence.</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="document-chunking" class="level3">
<h3 class="anchored" data-anchor-id="document-chunking">Document chunking</h3>
<p>The examples we looked at until now were all working with short text-snippets that comforably fit into the context window of a LLM. If you think about usual usecases for RAG-systems, this is not the most common case though. Usually, you will have a base of documents that can span multiple 1000’s of tokens and you want to be able to answer questions about these documents. Furthermore, you do not only want to know which document might be relevant, but ideally also which part of the document matches your question best.</p>
<p>This is where the process of doctument chunking or document splitting comes into play. There is a series of possible approaches to split a document, the most common, so called <strong>naive chunking</strong> method, is to use a structural element of the document though. This means that you parse the documents into sentences, paragraphs or pages and then use these as chunks that you individually embed and store in your vector database. To prevent loss of relevant context when splitting a document into chunks, it is additionally common to add some <strong>overlap</strong> between the chunks. This tries to solve the lost context problem, does however create reduncencies in the data.</p>
<p>An alternative approach is to use <strong>semantic chunking</strong>. This means that you split a document into chunks based on their meaning. Jina.ai explained in a blogpost <span class="citation" data-cites="LateChunkingLongContext2024">(<a href="#ref-LateChunkingLongContext2024" role="doc-biblioref"><em>Late <span>Chunking</span> in <span>Long-Context Embedding Models</span></em>, 2024</a>)</span> their so called “late chunking” method. which iteratively runs the whole document through the attention head of the transformer to gain embeddings per token, and then averages these embeddings per naive chunk. This way, the chunks are still structure based but contain semantic information about the whole context. <!-- Haystack does not implement this feature yet, though [it is planned](https://github.com/deepset-ai/haystack/issues/8111). --></p>
<p>Another approach to semantic chunking is described on the doc-pages of <a href="https://docs.llamaindex.ai/en/stable/examples/node_parsers/semantic_chunking/">LlamaIndex</a>. In their approach to semantic chunking, an adaptive splitting-rule is used, that splits the documents based on semantic similarity of sentences. This means that sentences that are semantically similar are grouped together into chunks.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
📝 Task
</div>
</div>
<div class="callout-body-container callout-body">
<p>Implement a document chunking strategy for a book of your choice from the <a href="https://huggingface.co/datasets/manu/project_gutenberg">project_gutenberg</a> dataset.</p>
<p>You can use any approach you like, but you should explain your choice and why it is appropriate for this dataset.</p>
</div>
</div>
</section>
<section id="query-expansiontransformation" class="level3">
<h3 class="anchored" data-anchor-id="query-expansiontransformation">Query Expansion/Transformation</h3>
<p>Until now, we have based our retrieval on the assumption, that the question the user formulates is a good representation of their information need. This is not always the case though. Often, users do not know what they are looking for or they use synonyms or paraphrases that are not present in the documents. If the question is not formulated well, or if it is too specific, the system might not be able to find relevant documents. To improve the quality of the questions, we can use <strong>query expansion</strong>. This means that we take the original question and expand it with additional information to make it more specific and to increase the chances of finding relevant documents. This can be done in multiple ways, one common approach is to use a generative model to generate multiple queries based on the original question. Another approach is to use a keyword extraction algorithm to extract keywords from the question and then use these keywords to expand the query.</p>
<p>The most basic way to implement a query-expansion is to build a tool that instructs a LLM to give multiple alternate formulations of the original query. Though this will probably work, there are more refined methods.</p>
<p>Llamaindex implements two more sophisticated approaches to transform queries:</p>
<ol type="1">
<li><p>Hypothetical Document Embeddings (HyDe): A LLM is instructed to generate a hypothetical document that answers the query. This document is then used to query the index</p></li>
<li><p>Multi-Step Query Transformations: After a first execution of a (complex) query against an index, the answer is used to iteratively formulate follow-up questions that are then executed against the index.</p></li>
</ol>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
📝 Task
</div>
</div>
<div class="callout-body-container callout-body">
<p>Implement query expansion for the climate_fever dataset using llamaindex. <a href="https://docs.llamaindex.ai/en/stable/module_guides/querying/pipeline/usage_pattern/#defining-a-custom-query-component">This</a> might be helpful.</p>
<p>Experiment with different prompts and temperatures.</p>
</div>
</div>
</section>
</section>
<section id="further-readings" class="level2">
<h2 class="anchored" data-anchor-id="further-readings">Further Readings</h2>
<ul>
<li><p><a href="https://www.deepset.ai/blog/llms-retrieval-augmentation">This blogpost</a> by DeepSet gives a good overview of the concept of RAG</p></li>
<li><p><a href="https://qdrant.tech/articles/what-is-a-vector-database/">This blogpost</a> by qdrant about (their) vector store and its inner workings</p></li>
</ul>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-bowmanLargeAnnotatedCorpus2015" class="csl-entry" role="listitem">
Bowman, S. R., Angeli, G., Potts, C., &amp; Manning, C. D. (2015). A large annotated corpus for learning natural language inference. In L. Màrquez, C. Callison-Burch, &amp; J. Su (Eds.), <em>Proceedings of the 2015 <span>Conference</span> on <span>Empirical Methods</span> in <span>Natural Language Processing</span></em> (pp. 632–642). Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/D15-1075">https://doi.org/10.18653/v1/D15-1075</a>
</div>
<div id="ref-devlinBERTPretrainingDeep2019a" class="csl-entry" role="listitem">
Devlin, J., Chang, M.-W., Lee, K., &amp; Toutanova, K. (2019). <em><span>BERT</span>: <span class="nocase">Pre-training</span> of <span>Deep Bidirectional Transformers</span> for <span>Language Understanding</span></em> (arXiv:1810.04805). arXiv. <a href="https://doi.org/10.48550/arXiv.1810.04805">https://doi.org/10.48550/arXiv.1810.04805</a>
</div>
<div id="ref-goyalComparativeAnalysisDifferent2022" class="csl-entry" role="listitem">
Goyal, K., &amp; Sharma, M. (2022). Comparative <span>Analysis</span> of <span>Different Vectorizing Techniques</span> for <span>Document Similarity</span> using <span>Cosine Similarity</span>. <em>2022 <span>Second International Conference</span> on <span>Advanced Technologies</span> in <span>Intelligent Control</span>, <span>Environment</span>, <span>Computing</span> &amp; <span>Communication Engineering</span> (<span>ICATIECE</span>)</em>, 1–5. <a href="https://doi.org/10.1109/ICATIECE56365.2022.10046766">https://doi.org/10.1109/ICATIECE56365.2022.10046766</a>
</div>
<div id="ref-jiangScalingSentenceEmbeddings2023a" class="csl-entry" role="listitem">
Jiang, T., Huang, S., Luan, Z., Wang, D., &amp; Zhuang, F. (2023). <em>Scaling <span>Sentence Embeddings</span> with <span>Large Language Models</span></em> (arXiv:2307.16645). arXiv. <a href="https://doi.org/10.48550/arXiv.2307.16645">https://doi.org/10.48550/arXiv.2307.16645</a>
</div>
<div id="ref-LateChunkingLongContext2024" class="csl-entry" role="listitem">
<em>Late <span>Chunking</span> in <span>Long-Context Embedding Models</span></em>. (2024). https://jina.ai/news/late-chunking-in-long-context-embedding-models.
</div>
<div id="ref-reimersSentenceBERTSentenceEmbeddings2019a" class="csl-entry" role="listitem">
Reimers, N., &amp; Gurevych, I. (2019). <em>Sentence-<span>BERT</span>: <span>Sentence Embeddings</span> using <span>Siamese BERT-Networks</span></em> (arXiv:1908.10084). arXiv. <a href="https://doi.org/10.48550/arXiv.1908.10084">https://doi.org/10.48550/arXiv.1908.10084</a>
</div>
<div id="ref-steckCosineSimilarityEmbeddingsReally2024" class="csl-entry" role="listitem">
Steck, H., Ekanadham, C., &amp; Kallus, N. (2024). Is <span>Cosine-Similarity</span> of <span>Embeddings Really About Similarity</span>? <em>Companion <span>Proceedings</span> of the <span>ACM Web Conference</span> 2024</em>, 887–890. <a href="https://doi.org/10.1145/3589335.3651526">https://doi.org/10.1145/3589335.3651526</a>
</div>
<div id="ref-tunstallEfficientFewShotLearning2022" class="csl-entry" role="listitem">
Tunstall, L., Reimers, N., Jo, U. E. S., Bates, L., Korat, D., Wasserblat, M., &amp; Pereg, O. (2022). <em>Efficient <span>Few-Shot Learning Without Prompts</span></em> (arXiv:2209.11055). arXiv. <a href="https://arxiv.org/abs/2209.11055">https://arxiv.org/abs/2209.11055</a>
</div>
</div>
</section>


</main> <!-- /main -->
<script>
var elements = document.getElementsByClassName('card');

var myFunction = function() {
  var overlay = this.querySelector('.overlay');
  var content = this.querySelector('.content');
  content.classList.toggle('blur-effect');
  if (overlay) {
    overlay.classList.toggle('show-overlay');
  }
}

for (var i = 0; i < elements.length; i++) {
    elements[i].addEventListener('click', myFunction, false);
    myFunction.call(elements[i]);
}

</script>
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../content/agent_basics.html" class="pagination-link" aria-label="Agent basics">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">Agent basics</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../content/function_calling.html" class="pagination-link" aria-label="Function Calling">
        <span class="nav-page-text"><span class="chapter-title">Function Calling</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p><a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" style="padding-right: 10px;"><img alt="Creative Commons Lizenzvertrag" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png"></a></p>
</div>   
    <div class="nav-footer-center">
<p>All images are generated using Python, R, draw.io, Flux or Stable Diffusion XL if not indicated otherwise.</p>
<div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/MBrede/generative_ai/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>