---
toc-title: '![](../imgs/history.jpg){width=240px}<br> <h3>Getting started with (L)LMs</h3>'
---


# Getting started with (L)LMs

This chapter is a brief introduction to the basics of language models and how to use them in practice. It assumes that you have some basic knowledge about machine learning and natural language processing. 

## Language Model Basics

### A short history of natural language processing

![BOW-representation of sentences.](../imgs/bow.gif){#fig-BOW width=100%}

The **Bag Of Words (BOW)** method represents text data by counting the frequency of each word in a given document or corpus. It treats all words as independent and ignores their order, making it suitable for tasks like text classification, for which it was traditionally the gold-standard. However, BOW has limitations when it comes to capturing semantic relationships between words and gets utterly useless if confronted with words not represented in the corpus. Additionally, it does not take into account the order of words in a sentence, which can be crucial for understanding its meaning. For example, the sentences "The cat is on the mat" and "The mat is on the cat" have different meanings despite having the same set of words.

![CBOW-representation of corpus.](../imgs/cbow.gif){#fig-CBOW width=100%}

The **Continuous Bag Of Words (CBOW)** method extends traditional BOW by representing words as dense vectors in a continuous space. CBOW predicts a target word based on its context, learning meaningful word representations from large amounts of text data.

![FastText^[Well, kind of. One of the major advantages of fasttext was the introduction of subword information which were left out of this illustration to save on space. This meant that uncommon words that were either absent or far and few between in the training corpus could be represented by common syllables. The display like it is here is far closer to fasttext's spiritual successor word2vec [@mikolovEfficientEstimationWord2013].] Model using CBOW-Method to predict missing word.](../imgs/training.gif){#fig-fasttext width=100%}

fastText [@bojanowskiEnrichingWordVectors2017], an open-source library developed by Facebook, builds upon the CBOW method and introduces significant improvements. It incorporates subword information and employs hierarchical softmax for efficient training on large-scale datasets. Even with limited data, fastText can learn meaningful word representations. fastText and its predecessor Word2Vec are considered precursors to modern language models due to their introduction of **Embeddings**, which laid the foundation for many modern NLP methods.

![Model using CBOW-Method to predict missing word.](../imgs/embeddings.gif){#fig-embed width=100%}

**Language Model Embeddings** are learned by predicting the next word in a sequence. These embeddings capture semantic and syntactic relationships between words, enabling them to understand context effectively. Since these embeddings represent the conditional probability distribution that language models learn to comprehend natural language, they can be reused by other models for tasks such as text classification or text retrieval. But more on this later.

Still, these models did not really solve the inherent issue of the order of words in a sentence. The input of models of this generation still used a dummyfied version of the corpus to represent context, which loses a lot of information.

![RNN-model, in this cased trained on text prediction.](../imgs/rnn.gif){#fig-rnn width=100%}

Traditionally, this was approached by feeding these embeddings into **Recurrent Neural Networks (RNNs)**. These models could learn to keep track of long-term dependencies in text data and improve the understanding of context. However, RNNs suffered from the vanishing gradient problem, which made it difficult for them to capture long-range dependencies effectively. 

![LSTM-model, also trained on text prediction.](../imgs/lstm.gif){#fig-lstm width=100%}

**Long Short-Term Memory (LSTM) networks** addressed this issue by introducing a mechanism called "gates" that allowed information to flow through the network more efficiently, but were, as the RNNs before, notoriuosly slow in training since only one word could be processed at a time. Additionally, a single LSTM  is still only able to process the input sequence from left to right, which is not ideal for inputs that contain ambiguos words that need context after them to fully understand their meaning. 
Take the following part of a sentence:

> The plant was growing

The word plant get's wildly differing meanings, depending on how the sentence continues:

> The plant was growing rapidly in the sunny corner of the garden.

> The plant was growing to accommodate more machinery for production.

A model that only processes the input sequence from left to right would just not be able to understand the meaning of "plant" in this context.


The ELMo model [@petersDeepContextualizedWord2018], which stands for Embeddings from Language Models, is an extension of LSTMs that improved contextual word representations. ELMo uses bidirectional LSTM layers to capture both past and future context, enabling it to understand the meaning of words in their surrounding context. This resulted in ELMo outperforming other models of it's era on a variety of natural language processing tasks. Still as each of the LSTM-Layer were only able to process one part of the sequence at a time, it was still unfortunately slow in training and inference. It's performance additionally decreased with the length of the input sequence since LSTM-cells have a better information retention than RNNs but are still not able to keep track of dependencies over long sequences.

### Attention is all you need

<!-- The **multi-headed attention layer** is a core component of the Transformer architecture used in models like BERT and GPT. Its purpose is to allow the model to focus on different parts of the input sequence simultaneously, capturing a variety of relationships between tokens.

Here's how it works:

1. **Inputs:** The input to the multi-headed attention layer is a set of vectors representing tokens in a sequence. These vectors are often the output of an embedding layer or a previous layer in the Transformer.

2. **Linear Transformations:** For each attention head, the input is linearly transformed into three different vectors for each token: **queries (Q)**, **keys (K)**, and **values (V)**. These are derived by multiplying the input vectors with learned weight matrices:
   - \( Q = XW_Q \)
   - \( K = XW_K \)
   - \( V = XW_V \)

3. **Scaled Dot-Product Attention:**
   - Attention is computed by comparing the **queries** to the **keys**. The result is a score that represents how much each token (key) contributes to the token being processed (query).
   - Scores are calculated using the dot product of queries and keys, scaled by the square root of the dimension of the keys (\(d_k\)) for numerical stability:
     \[
     \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
     \]
   - The softmax ensures that the scores are normalized to sum to 1, converting them into attention weights.

4. **Multiple Heads:**
   - Instead of using a single attention mechanism, the multi-head attention layer runs multiple attention mechanisms (heads) in parallel, each with its own set of learned parameters.
   - This allows the model to focus on different types of relationships in the data (e.g., syntactic and semantic relationships).
   - The outputs of all heads are concatenated and passed through a final linear transformation.

5. **Output:**
   - The output is a representation of the input sequence that combines information from all attention heads. This output is passed to subsequent layers for further processing.

---

### Query, Key, and Value

In the context of attention mechanisms, **query (Q)**, **key (K)**, and **value (V)** are concepts borrowed from information retrieval systems, where they define how to look up information:

- **Query (Q):** Represents what you're looking for. In multi-headed attention, this is a transformed version of the input token for which the model is calculating attention.

- **Key (K):** Represents the reference information against which the query is compared. Each token in the input sequence has a corresponding key.

- **Value (V):** Represents the actual information associated with a key. Values are the data retrieved when a query matches a key.

In practice:
1. The **query** determines the relevance of each token (key).
2. The attention mechanism assigns higher weights to tokens with keys that match the query.
3. The **value** provides the actual data that is aggregated to form the output.

For example, in a sentence:
- If the query represents "What is the verb related to this subject?", the keys might encode all the token relationships, and the value for the matched key (the verb) contributes to the output representation.

---

### Why Use Q, K, and V?

This decomposition allows the model to:
1. **Capture Contextual Relationships:** By computing attention between all tokens, the model can understand dependencies regardless of their position in the sequence.
2. **Enable Parallelism:** The Q, K, and V matrices can be processed efficiently in parallel on modern hardware like GPUs.
3. **Adapt Dynamically:** The learned transformations of Q, K, and V adapt during training to capture various linguistic and contextual features.

By having multiple heads, the multi-headed attention layer can learn to focus on different aspects of the data simultaneously, improving the model's ability to understand complex sequences. -->

In their seminal paper "Attention is all you need", @vaswaniAttentionAllYou2023 described the transformer architecture.

As the paper's title neatly suggests, the major breakthrough presented in this paper was the introduction of the so-called self-attention mechanism. This mechanism allows the model to "focus" on different parts of the input to a) determine the appropriate context for each word and b) to improve it's performance on differing tasks by allowing the model to filter unnecessary information.

This mechanism is implemented using so called attention heads, which each consist of three components:
1. Query (Q): Represents what we are looking for in the input sequence
2. Key (K): Represents the features that will be used to match with the query.
3. Value (V): The information that is associated with each key and will be used to update the representation of the word if it matches the query.

The implementation of the multi-headed attention meachnism allowed to solve all major issues of the language modelling approaches of the previous generation. It firstly allows the input of a whole text-sequence at once, rendering the training and inference far speedier then the recursive approaches. Furthermore, the multi-head attention mechanism allows the model to focus on different parts of the input sequence simultaneously, enabling it to capture more complex relationships between words and improve its understanding of context without losing information about long-term dependencies. This mechanism also implicitly solves the bidirectionality-issue since each word can be taken into account when processsing every other word in the sequence.

<!-- Absatz mit transfomer-gif, das einmal Aufmerksamkeit illustriert -->

@fig-TransformerVsLSTM, taken from @kaplanScalingLawsNeural2020a, shows the test performance of Transformer models compared to LSTM-based models as a function of model size and context length.
Transformers outperform LSTMs with increasing context length. 

![Comparison of Transformer- and LSTM-performance based on Model size and context length. Taken from @kaplanScalingLawsNeural2020a](../imgs/transformer_vs_lstm.png){#fig-TransformerVsLSTM width=100%}

Furthermore, @kaplanScalingLawsNeural2020a and @hoffmannTrainingComputeOptimalLarge2022 after them postulated performace power-laws (see also #fig-powerlaw) that suggest that the performance of a Transformer directly scales with the models size and data availability. Though the task of prediction of natural language poses a non-zero limit to the performanve, it is suggested that this limit is not reached for any of the currently available models.^[Incidentally, we might run out of data to train on before reaching that limit [@villalobosPositionWillWe2024].]

![Performance power law for transformer models. Taken from @kaplanScalingLawsNeural2020a](../imgs/power_law.png){#fig-powerlaw width=100%}

The advances made through leveraging transformer-based architectures for language modelling led to a family of general-purpose language models. Unlike the approaches before, these models were not trained for a specific task but rather on a general text base with the intention of allowing specific fine-tuning to adapt to a task.
Classic examples of these early general-purpose natural language generating Transformer models are the Generative Pre-trained Transformer (the predecessor of ChatGPT you all know), first described in @radfordImprovingLanguageUnderstanding2018, and the "Bidirectional Encoder Representations from Transformers" (BERT) architecture and training procedure, described by @devlinBERTPretrainingDeep2019.

This general-purpose architecture is the base of modern LLMs as we know them today and most applications we will discuss in this course.

## Choosing open source models



## Basics of using open source models (Huggingface, Ollama, LLM-Studio, Llama.cpp, ...)

Now it is your turn!
In your project-groups, you will each have to build a small "Hello World"-style application that uses an open source model.

1. Choose a small model using the sources we discussed before.
2. Each group is to use one of the following frameworks
    * [Huggingface](https://huggingface.co/)
    * [Ollama](https://ollama.com/)
    * [LM-Studio](https://lmstudio.ai/) from python
    * [Llama.cpp](https://github.com/ggerganov/llama.cpp)
    to load and use the model in your application.
3. Present your results and your experiences with the frameworks in a short presentation.
4. Submit your code and report on moodle.

## Further Readings

* The Attention is all you need-paper [@vaswaniAttentionAllYou2023] and the brilliant video discussing it by Umar Jamil [@umarjamilAttentionAllYou2023]


## References

