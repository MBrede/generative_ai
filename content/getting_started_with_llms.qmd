---
toc-title: '![](../imgs/history.jpg){width=240px}<br> <h3>Getting started with (L)LMs</h3>'
---


# Getting started with (L)LMs

This chapter is a brief introduction to the basics of language models and how to use them in practice. It assumes that you have some basic knowledge about machine learning and natural language processing. 

## Language Model Basics

### Traditional Methods: Bag Of Words (BOW)
![BOW-representation of sentences.](../imgs/bow.gif){#fig-BOW width=100%}

The **Bag Of Words (BOW)** method represents text data by counting the frequency of each word in a given document or corpus. It treats all words as independent and ignores their order, making it suitable for tasks like text classification, for which it was traditionally the gold-standard. However, BOW has limitations when it comes to capturing semantic relationships between words and gets utterly useless if confronted with words not represented in the corpus. Additionally, it does not take into account the order of words in a sentence, which can be crucial for understanding its meaning. For example, the sentences "The cat is on the mat" and "The mat is on the cat" have different meanings despite having the same set of words.

![CBOW-representation of corpus.](../imgs/cbow.gif){#fig-CBOW width=100%}

The **Continuous Bag Of Words (CBOW)** method extends traditional BOW by representing words as dense vectors in a continuous space. CBOW predicts a target word based on its context, learning meaningful word representations from large amounts of text data.

![FastText^[Well, kind of. One of the major advantages of fasttext was the introduction of subword information which were left out of this illustration to save on space. This meant that uncommon words that were either absent or far and few between in the training corpus could be represented by common syllables.] Model using CBOW-Method to predict missing word.](../imgs/training.gif){#fig-fasttext width=100%}

fastText, an open-source library developed by Facebook, builds upon the CBOW method and introduces significant improvements. It incorporates subword information and employs hierarchical softmax for efficient training on large-scale datasets. Even with limited data, fastText can learn meaningful word representations. fastText and its predecessor Word2Vec are considered precursors to modern language models due to their introduction of **Embeddings**, which laid the foundation for many modern NLP methods.

![Model using CBOW-Method to predict missing word.](../imgs/embeddings.gif){#fig-embed width=100%}

**Language Model Embeddings** are learned by predicting the next word in a sequence. These embeddings capture semantic and syntactic relationships between words, enabling them to understand context effectively. Since these embeddings represent the conditional probability distribution that language models learn to comprehend natural language, they can be reused by other models for tasks such as text classification or text retrieval. But more on this later.

Still, these models did not really solve the inherent issue of the order of words in a sentence. The input of models of this generation still used a dummyfied version of the corpus to represent context, which loses a lot of information.
Traditionally, this was approached by feeding these embeddings into **Recurrent Neural Networks (RNNs)** or **Long Short-Term Memory (LSTM) networks**. These models could learn to keep track of long-term dependencies in text data and improve the understanding of context. However, RNNs suffered from the vanishing gradient problem, which made it difficult for them to capture long-range dependencies effectively. LSTMs addressed this issue by introducing a mechanism called "gates" that allowed information to flow through the network more efficiently, but were, as the RNNs before, notoriuosly slow in training since only one word could be processed at a time.




## Choosing open source models

## Basics of using open source models (Huggingface, Ollama, LLM-Studio, Llama.cpp, ...)

## Further Readings



