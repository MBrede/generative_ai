<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Getting started with (L)LMs – Generative AI</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../content/prompting.html" rel="next">
<link href="../content/project_details.html" rel="prev">
<link href="../cover.jpg" rel="icon" type="image/jpeg">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-7c56b0cb14979e59e30aba88ccee8faa.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar docked slimcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../content/getting_started_with_llms.html">Language Models</a></li><li class="breadcrumb-item"><a href="../content/getting_started_with_llms.html"><span class="chapter-title">Getting started with (L)LMs</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../index.html" class="sidebar-logo-link">
      <img src="../cover.jpg" alt="" class="sidebar-logo light-content py-0 d-lg-inline d-none">
      <img src="../cover.jpg" alt="" class="sidebar-logo dark-content py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Generative AI</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/MBrede/generative_ai" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../Generative-AI.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/orga.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Organizational Details</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/project_details.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Project Details</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Language Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/getting_started_with_llms.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">Getting started with (L)LMs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/prompting.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Prompting</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/function_calling.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Towards Agents: chatbots and function calling</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/embeddings.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Embedding-based LLM-systems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/agent_basics.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Agent basics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/agent_interaction.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">LLM pipelines</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Image Generation</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/diff_models.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">AI image generation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/generation_in_agent_pipelines.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">AI image generation pipelines</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Other</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/model_context_protocol.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Model Context Protocol</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Finetuning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/finetuning_approaches.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Finetuning Approaches</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/alignment.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Alignment</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="2">
    <h2 id="toc-title"><img src="../imgs/history.jpg" class="img-fluid" width="240"><br><br>
</h2><h3 class="anchored"><br>
Getting started with (L)LMs<br>
</h3>
   
  <ul>
  <li><a href="#language-model-basics" id="toc-language-model-basics" class="nav-link active" data-scroll-target="#language-model-basics">Language Model Basics</a>
  <ul class="collapse">
  <li><a href="#a-short-history-of-natural-language-processing" id="toc-a-short-history-of-natural-language-processing" class="nav-link" data-scroll-target="#a-short-history-of-natural-language-processing">A short history of natural language processing</a></li>
  <li><a href="#attention-is-all-you-need" id="toc-attention-is-all-you-need" class="nav-link" data-scroll-target="#attention-is-all-you-need">Attention is all you need</a></li>
  </ul></li>
  <li><a href="#choosing-open-source-models" id="toc-choosing-open-source-models" class="nav-link" data-scroll-target="#choosing-open-source-models">Choosing open source models</a></li>
  <li><a href="#basics-of-using-open-source-models" id="toc-basics-of-using-open-source-models" class="nav-link" data-scroll-target="#basics-of-using-open-source-models">Basics of using open source models</a></li>
  <li><a href="#further-readings" id="toc-further-readings" class="nav-link" data-scroll-target="#further-readings">Further Readings</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/MBrede/generative_ai/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../content/getting_started_with_llms.html">Language Models</a></li><li class="breadcrumb-item"><a href="../content/getting_started_with_llms.html"><span class="chapter-title">Getting started with (L)LMs</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-title">Getting started with (L)LMs</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>This chapter provides a brief introduction to the history and function of modern language models, focusing on their practical use in text generation tasks. It will then give a short introduction on how to utilize pretrained language models for your own applications.</p>
<section id="language-model-basics" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="language-model-basics">Language Model Basics</h2>
<p>Language models have diverse applications, including speech recognition, machine translation, text generation, and question answering. While we’ll concentrate on <strong>text generation</strong> for this course, understanding the general concept of language models is crucial. Given language’s inherent complexity and ambiguity, a fundamental challenge in NLP is creating structured representations that can be employed downstream. This section will first explore the evolution of these representations before introducing the transformer architecture, which forms the foundation of most modern language models.</p>
<section id="a-short-history-of-natural-language-processing" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="a-short-history-of-natural-language-processing">A short history of natural language processing</h3>
<div id="fig-BOW" class="enlarge-onhover gif-image quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-BOW-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../imgs/bow.png" class="enlarge-onhover gif-image img-fluid figure-img" width="100">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-BOW-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Fig&nbsp;3.1: BOW-representation of sentences.
</figcaption>
</figure>
</div>
<p>The <strong>Bag Of Words (BOW)</strong> method represents text data by counting the frequency of each word in a given document or corpus. It treats all words as independent and ignores their order, making it suitable for tasks like text classification, for which it was traditionally the gold-standard. However, BOW has limitations when it comes to capturing semantic relationships between words and gets utterly useless if confronted with words not represented in the corpus. Additionally, it does not take into account the order of words in a sentence, which can be crucial for understanding its meaning. For example, the sentences “The cat is on the mat” and “The mat is on the cat” have different meanings despite having the same set of words.</p>
<div id="fig-CBOW" class="enlarge-onhover gif-image quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-CBOW-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../imgs/cbow.png" class="enlarge-onhover gif-image img-fluid figure-img" width="100">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-CBOW-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Fig&nbsp;3.2: CBOW-representation of corpus.
</figcaption>
</figure>
</div>
<p>The <strong>Continuous Bag Of Words (CBOW)</strong> method extends traditional BOW by representing words as dense vectors in a continuous space. CBOW predicts a target word based on its context, learning meaningful word representations from large amounts of text data.</p>
<div id="fig-fasttext" class="enlarge-onhover gif-image quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-fasttext-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../imgs/training.png" class="enlarge-onhover gif-image img-fluid figure-img" width="100">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-fasttext-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Fig&nbsp;3.3: Shallow Model using CBOW-Method to predict missing word.
</figcaption>
</figure>
</div>
<p>fastText <span class="citation" data-cites="bojanowskiEnrichingWordVectors2017">(<a href="#ref-bojanowskiEnrichingWordVectors2017" role="doc-biblioref">Bojanowski et al., 2017</a>)</span>, an open-source library developed by Facebook, builds upon the CBOW method and introduces significant improvements. It incorporates subword information and employs hierarchical softmax for efficient training on large-scale datasets. Even with limited data, fastText can learn meaningful word representations. fastText and its predecessor Word2Vec are considered precursors to modern language models due to their introduction of <strong>Embeddings</strong>, which laid the foundation for many modern NLP methods. <a href="#fig-fasttext" class="quarto-xref">Figure&nbsp;<span>3.3</span></a> illustrates this fastText-architecture<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> <a href="#fig-fasttext" class="quarto-xref">Figure&nbsp;<span>3.3</span></a> additionally illustrates quite nicely the learning paradigm that modern language models still use - the so called <strong>masked language modelling (MLM)</strong>. This paradigm presents the language model with the semi-supervised task of predicting a masked (i.e., missing) token from a presented sequence of tokenized text. <strong>Token</strong> means one of the word(-parts) that the model has represented in its vocabulary. This prediction is represented as probabilities of all possible tokens.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Well, kind of. One of the major advantages of fasttext was the introduction of subword information which were left out of this illustration to save on space. This meant that uncommon words that were either absent or far and few between in the training corpus could be represented by common syllables. The display like it is here is far closer to fasttext’s spiritual predecessor word2vec <span class="citation" data-cites="mikolovEfficientEstimationWord2013">(<a href="#ref-mikolovEfficientEstimationWord2013" role="doc-biblioref">Mikolov et al., 2013</a>)</span>.</p></div></div><div id="fig-embed" class="enlarge-onhover gif-image quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-embed-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../imgs/embeddings.png" class="enlarge-onhover gif-image img-fluid figure-img" width="100">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-embed-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Fig&nbsp;3.4: Model using CBOW-Method to predict missing word.
</figcaption>
</figure>
</div>
<p><strong>Language Model Embeddings</strong> are learned by predicting the next/missing token in a sequence. The utilisation of word-parts instead of whole words as tokens was another invention introduced by fastText <span class="citation" data-cites="bojanowskiEnrichingWordVectors2017">(<a href="#ref-bojanowskiEnrichingWordVectors2017" role="doc-biblioref">Bojanowski et al., 2017</a>)</span>, that allowed the model to generalize to new, unknown words when moving to inference. Embeddings are the representation the model learns to map the context-tokens to a multiclass classification of the missing token in the space of all possible tokens. These embeddings capture semantic and syntactic relationships between words, enabling them to understand context effectively. Since these embeddings represent the conditional probability distribution that language models learn to comprehend natural language, they can be reused by other models for tasks such as text classification or text retrieval. But more on this later.</p>
<p>Still, these models did not really solve the inherent issue of the order of words in a sentence. The input of models of this generation still used a dummyfied version of the corpus to represent context, which loses a lot of information.</p>
<div id="fig-rnn" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rnn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../imgs/rnns.png" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rnn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Fig&nbsp;3.5: Illustration of a simple RNN-model, (exaggeratingly) illustrating the issue of the model “forgetting” parts of the input when processing long sequences.
</figcaption>
</figure>
</div>
<p>Traditionally, this was approached by feeding these embeddings into <strong>Recurrent Neural Networks (RNNs)</strong>. These models could learn to keep track of sequential dependencies in text data and improve the understanding of context. However, RNNs suffered from their architecture’s inherent inability to retain information over long sequences. Simple RNN- cells<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> iterate through a sequence and use both their last output and the next sequence element as input to predict the next output. This makes it hard for them to learn long-term dependencies, since they have to compress all information into one vector (<a href="#fig-rnn" class="quarto-xref">Figure&nbsp;<span>3.5</span></a>).</p>
<!-- ![LSTM-model, also trained on text prediction.](../imgs/lstm.gif){#fig-lstm width=100%} -->
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;And pretty much all of the more complex variants</p></div></div><p><strong>Long Short-Term Memory (LSTM) networks</strong> addressed this issue by introducing a mechanism called “gates” that allowed information to flow through the network selectively and more efficiently, but were, as the RNNs before, notoriously slow in training since only one word could be processed at a time. Additionally, a single LSTM is still only able to process the input sequence from left to right, which is not ideal for inputs that contain ambiguous words that need context after them to fully understand their meaning. Take the following part of a sentence:</p>
<blockquote class="blockquote">
<p>The plant was growing</p>
</blockquote>
<p>The word plant get’s wildly differing meanings, depending on how the sentence continues:</p>
<blockquote class="blockquote">
<p>The plant was growing rapidly in the sunny corner of the garden.</p>
</blockquote>
<blockquote class="blockquote">
<p>The plant was growing to accommodate more machinery for production.</p>
</blockquote>
<p>A model that only processes the input sequence from left to right would just not be able to understand the meaning of “plant” in this context.</p>
<p>The ELMo model <span class="citation" data-cites="petersDeepContextualizedWord2018">(<a href="#ref-petersDeepContextualizedWord2018" role="doc-biblioref">Peters et al., 2018</a>)</span>, which stands for Embeddings from Language Models, is an extension of LSTMs that improved contextual word representations. ELMo uses bidirectional LSTM layers to capture both past and future context, enabling it to understand the meaning of words in their surrounding context. This resulted in ELMo outperforming other models of its era on a variety of natural language processing tasks. Still as each of the LSTM-Layer were only able to process one part of the sequence at a time, it was still unfortunately slow in training and inference. Its performance additionally decreased with the length of the input sequence since LSTM-cells have a better information retention than RNNs but are still not able to keep track of dependencies over long sequences.</p>
</section>
<section id="attention-is-all-you-need" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="attention-is-all-you-need">Attention is all you need</h3>
<p>In their transformative paper “Attention is all you need”, <span class="citation" data-cites="vaswaniAttentionAllYou2023a">Vaswani et al. (<a href="#ref-vaswaniAttentionAllYou2023a" role="doc-biblioref">2023</a>)</span> described the transformer architecture.</p>
<p>As the paper’s title neatly suggests, the major breakthrough presented in this paper was the introduction of the so-called self-attention mechanism. This mechanism allows the model to “focus” on different parts of the input to a) determine the appropriate context for each word and b) to improve its performance on differing tasks by allowing the model to filter unnecessary information.</p>
<section id="self-attention-mechanism" class="level4">
<h4 class="anchored" data-anchor-id="self-attention-mechanism">Self-Attention Mechanism</h4>
<p>The self-attention mechanism relies on three components: <strong>Query (Q)</strong>, <strong>Key (K)</strong>, and <strong>Value (V)</strong>, inspired by concepts in information retrieval. Imagine you search for a specific term in a library (query), match it against the catalogue (key), and use this information about the catalogue to update your personal priority of search terms (value).</p>
<p>In practice, for each word in a sentence, the model calculates:</p>
<ol type="1">
<li><strong>Relevance Scores</strong>: Compare each Query vector (Q) with every Key vector (K) in the sequence using the dot product. These scores measure how much focus one word should have on another.</li>
<li><strong>Attention Weights</strong>: Normalize the scores using a softmax function to ensure they sum to 1, distributing focus proportionally across all words.</li>
<li><strong>Weighted Sum</strong>: Multiply each Value vector (V) by its corresponding attention weight to compute the final representation.</li>
</ol>
</section>
<section id="calculating-attention" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="calculating-attention">Calculating Attention</h4>
<p>For a sequence of words, the attention scores are computed as: <span class="math display">\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(Q\)</span> represents the query matrix.</li>
<li><span class="math inline">\(K\)</span> is the key matrix.</li>
<li><span class="math inline">\(V\)</span> is the value matrix.</li>
<li><span class="math inline">\(d_k\)</span> is the dimensionality of the key vectors, ensuring scale invariance.</li>
</ul>
<p>Let’s first illustrate this concept with a practical example (not specifically from the context of NLP) to later circle back to its application in the transformer architecture.</p>
<p>We look at a retrieval task in which we query in a domain that has 5 attributes describing the items in it. The aforementioned “lookup” is then implemented by calculating the dot product between the query and the transposed keys resulting in a vector of weights for each input-aspect.</p>
<p>As a simplification, we assume that all aspects can be described in binary terms. A hypothetical 1x5 query matrix (Q) represents the aspects we are querying in a 5-dimensional space, while a transposed 1x5 key matrix (K) represents the aspects of the search space. The dot product between these matrices results in a scalar that reflects the alignment or similarity between the query and the key, effectively indicating how many aspects of the query align with the search space.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="getting_started_with_llms_files/figure-html/1x5/1x5-1.png" class="img-fluid figure-img" width="768"></p>
</figure>
</div>
</div>
</div>
<p>If we now add a series of items we want to query for to our matrix <span class="math inline">\(K\)</span>, the result will be a vector representing the amount of matches, each item has with our query:</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="getting_started_with_llms_files/figure-html/1x5/5x5-1.png" class="img-fluid figure-img" width="768"></p>
</figure>
</div>
</div>
</div>
<p>The result is a vector of scores that indicate the matches of the query per key. This principle does obviously also work for more than one query by adding more rows to our Query matrix <span class="math inline">\(Q\)</span>. This does result in a matrix, in which each row indicates the amount of matching keys for each query:</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="getting_started_with_llms_files/figure-html/5x5/5x5-1.png" class="img-fluid figure-img" width="768"></p>
</figure>
</div>
</div>
</div>
<p>Instead of binary indicators, the <span class="math inline">\(Q\)</span> and <span class="math inline">\(K\)</span> matrices in the attention mechanism are filled with floats. This does still result in the same kind of matched-key-result, although the results are now more like degrees of relevance instead of absolute matches:</p>
<p><span class="math display">\[
Q \times K^T =
\]</span></p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="getting_started_with_llms_files/figure-html/QxKT-1.png" class="img-fluid figure-img" width="768"></p>
</figure>
</div>
</div>
</div>
<p>As you can already see in this small example, the values of individual cells can get relatively high compared to the rest of the matrix. As you remember - we want to use this product to rank our values. If these numbers are too large, it might lead to numerical instability or incorrect results. To address this issue, we will scale down the dot-product by dividing it with <span class="math inline">\(\sqrt{d_n}\)</span>, where <span class="math inline">\(d_n\)</span> is the dimension of the aspect space (in our case 5).</p>
<p><span class="math display">\[
\frac{Q \times K^T}{\sqrt{d_n}} =
\]</span></p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="getting_started_with_llms_files/figure-html/QxKT/sqrt(dn)-1.png" class="img-fluid figure-img" width="768"></p>
</figure>
</div>
</div>
</div>
<p>Since we want to use this matrix for filtering our dataset, we would prefer the weights to sum up to one. To achieve that, we will apply a softmax function on each row of the matrix (remember that the rows currently represent the key-weighted aspects for each query). The resulting matrix with scaled weights for each aspect is then multiplied with the value-matrix that contains one datapoint in each row, described by 5 aspects along the columns.</p>
<p><span class="math display">\[
\text{softmax}(\frac{Q \times K^T}{\sqrt{d_n}}) \times V =
\]</span></p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="getting_started_with_llms_files/figure-html/softmax_QxKT/sqrt(dn)V-1.png" class="img-fluid figure-img" width="768"></p>
</figure>
</div>
</div>
</div>
<p>The result is now an attention matrix in the sense that it tells us the importance of each value’s aspect for our query. In the specific example, the third aspect seems to be the most important aspect for our forth query. The crucial advantage is, that all aspects of all queries can be simultaneously compared with all aspects of all values without the necessity of sequential processing.</p>
<p>Though this general idea of weighting aspects in the sense of self-attention<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> to process a sequence without disadvantages of the distances of the items was used before <span class="citation" data-cites="bahdanauNeuralMachineTranslation2014">(<a href="#ref-bahdanauNeuralMachineTranslation2014" role="doc-biblioref">Bahdanau, 2014</a>)</span>, the major contribution of the paper was the complete reliance on this mechanism without the need of LSTM/RNN parts. That their suggested architecture works is in part due to the utilisation of multiple self-attention layers, each learning its own weights for <span class="math inline">\(Q\)</span>, <span class="math inline">\(K\)</span> and <span class="math inline">\(V\)</span>. This allows the model to learn more complex patterns and dependencies between words in a sentence. You can think of it as allowing the model to focus on different parts of the input sequence at different stages of processing. The outputs of the multiple heads are then concatenated and linearly transformed into the final output representation using a series of fully connected feed-forward layers.</p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;<em>self</em> in the sense of the model weighting its own embeddings, queries, keys and values</p></div></div><p>This small example is already pretty close to the general attention-mechanism described by <span class="citation" data-cites="vaswaniAttentionAllYou2023a">Vaswani et al. (<a href="#ref-vaswaniAttentionAllYou2023a" role="doc-biblioref">2023</a>)</span> (see also <a href="#fig-multihead" class="quarto-xref">Figure&nbsp;<span>3.6</span></a>), though the actual language model learns its own weights for <span class="math inline">\(Q\)</span>, <span class="math inline">\(K\)</span> and <span class="math inline">\(V\)</span>.</p>
<div id="fig-multihead" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-multihead-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../imgs/multi-head.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-multihead-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Fig&nbsp;3.6: Multi-headed attention as depicted in <span class="citation" data-cites="vaswaniAttentionAllYou2023a">Vaswani et al. (<a href="#ref-vaswaniAttentionAllYou2023a" role="doc-biblioref">2023</a>)</span>
</figcaption>
</figure>
</div>
<p>Instead of 5x5 matrices, the attenion mechanism as described in the paper implements <span class="math inline">\(d_n \times d_c\)</span><a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> matrices, where <span class="math inline">\(d_n\)</span> is the dimension of the embedding space<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> and <span class="math inline">\(d_c\)</span> is the size of the context window. In the original paper, <span class="citation" data-cites="vaswaniAttentionAllYou2023a">Vaswani et al. (<a href="#ref-vaswaniAttentionAllYou2023a" role="doc-biblioref">2023</a>)</span> implement the context-window as the same size as the embedding space (i.e., <span class="math inline">\(d_n = d_c\)</span>). In <a href="#fig-illustratedAttention" class="quarto-xref">Figure&nbsp;<span>3.7</span></a> you can see a brilliant illustration of the multiheaded-attention mechanism at work.</p>
<div class="no-row-height column-margin column-container"><div id="fn4"><p><sup>4</sup>&nbsp;<span class="math inline">\(\frac{d_n}{h} \times \frac{d_c}{h}\)</span> actually, the paper used feed-forward layers to reduce the dimensionality of each attention header to reduce the computational cost.</p></div><div id="fn5"><p><sup>5</sup>&nbsp;I.e., the dimensionality used to represent each word’s meaning. In the previous toy-example illustrating the concept of embeddings (<a href="#fig-embed" class="quarto-xref">Figure&nbsp;<span>3.4</span></a>), this would be the width of the hidden layer (8). In the case of transformers, this is usually 512 or 1024. These embeddings are learned during training and are a simple transformation of the one-hot vectors returned by the models tokenizer.</p></div></div><div id="fig-illustratedAttention" class="enlarge-onhover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-illustratedAttention-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../imgs/illustrated_attention.png" class="enlarge-onhover img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-illustratedAttention-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Fig&nbsp;3.7: Illustration of the multi-headed attention mechanism. Taken from <span class="citation" data-cites="hussainTutorialOpensourceLarge2024">Hussain et al. (<a href="#ref-hussainTutorialOpensourceLarge2024" role="doc-biblioref">2024</a>)</span>
</figcaption>
</figure>
</div>
<p>The implementation of the multi-headed attention mechanism allowed to solve all major issues of the language modelling approaches of the previous generation<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>. It firstly allows the input of a whole text-sequence at once, rendering the training and inference far speedier then the recursive approaches. Furthermore, the multi-head attention mechanism allows the model to focus on different parts of the input sequence simultaneously, enabling it to capture more complex relationships between words and improve its understanding of context without losing information about long-term dependencies. This mechanism also implicitly solves the bidirectionality-issue since each word can be taken into account when processing every other word in the sequence.</p>
<div class="no-row-height column-margin column-container"><div id="fn6"><p><sup>6</sup>&nbsp;Well, kind of. Transformers are far superior language models due to their ability to parallely process long sequences without issues with stretched context - these advantages come at a price though. GPT-3s training is estimated to have emitted around 502 metric tons of carbon <span class="citation" data-cites="AIAAICChatGPTTraining">(<a href="#ref-AIAAICChatGPTTraining" role="doc-biblioref"><em><span>AIAAIC - ChatGPT training emits 502 metric tons of carbon</span></em>, n.d.</a>)</span>. The computational cost of the architecture as described here does additionally scale quadratically with context window size.</p></div></div><p>The description until now omitted one final but key detail - we only spoke about the weight matrices <span class="math inline">\(Q\)</span>, <span class="math inline">\(K\)</span> and <span class="math inline">\(V\)</span>. Each of these weight matrices are actually the product of the learned weights and the input vectors. In other words, each of the three matrices is calculated as follows:</p>
<p><span class="math display">\[
\begin{array}{lcl}
    Q  &amp;=&amp; XW_Q \\
    K  &amp;=&amp; XW_k \\
    V  &amp;=&amp; XW_v
\end{array}
\]</span></p>
<p>where <span class="math inline">\(W_{Q, k, v}\)</span> are the learned weight matrices and <span class="math inline">\(X\)</span> is the input matrix. This input matrix consists of a) the learned embeddings of the tokenized input-parts and b) the added, so called positional encoding.<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn7"><p><sup>7</sup>&nbsp;While we are talking about omitted details, the whole architecture implements its layers as residual layers. This means that the output of each layer is added to the input of the layer before, before it is passed on to the next layer. But this detail is irrelevant for our understanding of the central mechanism.</p></div></div><p>The positional encoding is a vector that encodes the position of each token in the input sequence. It is added to the embedding of each token to provide the model with information about the order of the tokens in the sequence. The positional encoding is calculated as follows:</p>
<p><span class="math display">\[
\begin{array}{lcl}
PE_{(pos, 2i)} &amp;=&amp; sin(\frac{pos}{10000^{\frac{2i}{d_{model}}}}) \\
PE_{(pos, 2i+1)} &amp;=&amp; cos(\frac{pos}{10000^{\frac{2i}{d_{model}}}})
\end{array}
\]</span></p>
<p>Where <span class="math inline">\(i\)</span> is the dimension and <span class="math inline">\(pos\)</span> is the position. Those 2 formulas are not the most intuitive, what they do is to add a unique offset to each embedding though, that allows the model to infer and weigh the token’s positions in the matrix on it’s own. <a href="#fig-positional" class="quarto-xref">Figure&nbsp;<span>3.8</span></a> illustrates the pattern this specific combination of sin and cos creates for each sequence-position and embedding-dimension.</p>
<div class="cell enlarge-onhover" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-positional" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center" alt="A heatmap of the positional encoding for 50 dimensions and 512 embedding-dimensions. The y-axis represents the position and the x-axis represents the dimension of the embedded token in the input sequence. The color represents the value of the encoding.">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-positional-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="getting_started_with_llms_files/figure-html/fig-positional-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" alt="A heatmap of the positional encoding for 50 dimensions and 512 embedding-dimensions. The y-axis represents the position and the x-axis represents the dimension of the embedded token in the input sequence. The color represents the value of the encoding." width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-positional-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Fig&nbsp;3.8: The positional encoding for 50 tokens and 512 embedding-dimensions. The y-axis represents the position and the x-axis represents the dimension of the embedded token in the input sequence. The color represents the value of the encoding.
</figcaption>
</figure>
</div>
</div>
</div>
<p>These parts alltogether are all building-blocks of the basic transformer architecture. As you can see in <a href="#fig-classicTransformerPicture" class="quarto-xref">Figure&nbsp;<span>3.9</span></a>, all parts depicted by <span class="citation" data-cites="vaswaniAttentionAllYou2023a">Vaswani et al. (<a href="#ref-vaswaniAttentionAllYou2023a" role="doc-biblioref">2023</a>)</span> are parts we have discussed until now.</p>
<div id="fig-classicTransformerPicture" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-classicTransformerPicture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../imgs/transformer_architecture.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-classicTransformerPicture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Fig&nbsp;3.9: The transformer architecture as depicted in <span class="citation" data-cites="vaswaniAttentionAllYou2023a">Vaswani et al. (<a href="#ref-vaswaniAttentionAllYou2023a" role="doc-biblioref">2023</a>)</span>
</figcaption>
</figure>
</div>
<p>The Encoder half uses the embedding -&gt; encoding -&gt; multi-headed-attention -&gt; feed-forward structure to create a semantic representation of the sequence. The Decoder half uses the same structure, but with an additional masked multi-head attention layer to prevent the model from looking at future tokens. This is necessary because we want to generate a sequence token by token.</p>
<p>The architecture described by <span class="citation" data-cites="vaswaniAttentionAllYou2023a">Vaswani et al. (<a href="#ref-vaswaniAttentionAllYou2023a" role="doc-biblioref">2023</a>)</span> implements both an encoder and a decoder half. In practice, many modern language models do not use this full architecture. Instead, they implement either encoder-only or decoder-only variants, each optimized for different tasks.</p>
<p>Encoder-only models like BERT <span class="citation" data-cites="devlinBERTPretrainingDeep2019a">(<a href="#ref-devlinBERTPretrainingDeep2019a" role="doc-biblioref">Devlin et al., 2019</a>)</span> use bidirectional attention across the entire input sequence. Since they can attend to all tokens simultaneously, they excel at tasks requiring deep understanding of complete contexts - such as text classification, named entity recognition, or semantic similarity. These models cannot generate text sequentially since they process all tokens at once.</p>
<p>Decoder-only models like GPT <span class="citation" data-cites="radfordImprovingLanguageUnderstanding2018">(<a href="#ref-radfordImprovingLanguageUnderstanding2018" role="doc-biblioref">Radford et al., 2018</a>)</span> use masked attention to prevent tokens from attending to future positions. This unidirectional constraint makes them naturally suited for autoregressive generation, where each token is predicted based only on preceding context. While this limits their ability to leverage future context, it enables them to generate coherent text token by token. Encoder-decoder models retain both components and are primarily used for sequence-to-sequence tasks like translation or summarization, where the encoder processes the input and the decoder generates the output.</p>
<p>The choice between these architectures represents a fundamental trade-off between understanding and generation capabilities. For an interactive visualization of a decoder-only architecture in action, <a href="https://bbycroft.net/llm">this visualization</a> by Brendan Bycroft provides an excellent walkthrough of how tokens flow through each layer during inference.</p>
<p><a href="#fig-TransformerVsLSTM" class="quarto-xref">Figure&nbsp;<span>3.10</span></a>, taken from <span class="citation" data-cites="kaplanScalingLawsNeural2020a">Kaplan et al. (<a href="#ref-kaplanScalingLawsNeural2020a" role="doc-biblioref">2020</a>)</span>, shows the test performance of Transformer models compared to LSTM-based models as a function of model size and context length. Transformers outperform LSTMs with increasing context length.</p>
<div id="fig-TransformerVsLSTM" class="enlarge-onhover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-TransformerVsLSTM-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../imgs/transformer_vs_lstm.png" class="enlarge-onhover img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-TransformerVsLSTM-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Fig&nbsp;3.10: Comparison of Transformer- and LSTM-performance based on Model size and context length. Taken from <span class="citation" data-cites="kaplanScalingLawsNeural2020a">Kaplan et al. (<a href="#ref-kaplanScalingLawsNeural2020a" role="doc-biblioref">2020</a>)</span>
</figcaption>
</figure>
</div>
<p>Furthermore, <span class="citation" data-cites="kaplanScalingLawsNeural2020a">Kaplan et al. (<a href="#ref-kaplanScalingLawsNeural2020a" role="doc-biblioref">2020</a>)</span> and <span class="citation" data-cites="hoffmannTrainingComputeOptimalLarge2022a">Hoffmann et al. (<a href="#ref-hoffmannTrainingComputeOptimalLarge2022a" role="doc-biblioref">2022</a>)</span> after them postulated performace power-laws (see also <a href="#fig-powerlaw" class="quarto-xref">Figure&nbsp;<span>3.11</span></a>) that suggest that the performance of a Transformer directly scales with the models size and data availability. Though the task of prediction of natural language poses a non-zero limit to the performance, it is suggested that this limit is not reached for any of the currently available models.<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn8"><p><sup>8</sup>&nbsp;Incidentally, we might run out of data to train on before reaching that limit <span class="citation" data-cites="villalobosPositionWillWe2024">(<a href="#ref-villalobosPositionWillWe2024" role="doc-biblioref">Villalobos et al., 2024</a>)</span>.</p></div></div><div id="fig-powerlaw" class="enlarge-onhover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-powerlaw-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../imgs/power_law.png" class="enlarge-onhover img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-powerlaw-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Fig&nbsp;3.11: Performance power law for transformer models. Taken from <span class="citation" data-cites="kaplanScalingLawsNeural2020a">Kaplan et al. (<a href="#ref-kaplanScalingLawsNeural2020a" role="doc-biblioref">2020</a>)</span>
</figcaption>
</figure>
</div>
<p>The advances made through leveraging transformer-based architectures for language modelling led to a family of general-purpose language models. Unlike the approaches before, these models were not trained for a specific task but rather on a general text base with the intention of allowing specific fine-tuning to adapt to a task. Classic examples of these early general-purpose natural language generating Transformer models are the Generative Pre-trained Transformer (the predecessor of ChatGPT you all know), first described in <span class="citation" data-cites="radfordImprovingLanguageUnderstanding2018">Radford et al. (<a href="#ref-radfordImprovingLanguageUnderstanding2018" role="doc-biblioref">2018</a>)</span>, and the “Bidirectional Encoder Representations from Transformers” (BERT) architecture and training procedure, described by <span class="citation" data-cites="devlinBERTPretrainingDeep2019a">Devlin et al. (<a href="#ref-devlinBERTPretrainingDeep2019a" role="doc-biblioref">2019</a>)</span>.</p>
<p>This general-purpose architecture is the base of modern LLMs as we know them today and most applications we will discuss in this course.</p>
</section>
</section>
</section>
<section id="choosing-open-source-models" class="level2">
<h2 class="anchored" data-anchor-id="choosing-open-source-models">Choosing open source models</h2>
<p>The 2023 release of ChatGPT by OpenAI has sparked a lot of interest in large language models (LLMs) and their capabilities. This has also led to an increase in the number of available open-source LLMs. The selection of a model for your application is always a trade-off between performance, size, and computational requirements.</p>
<p>Although <span class="citation" data-cites="kaplanScalingLawsNeural2020a">Kaplan et al. (<a href="#ref-kaplanScalingLawsNeural2020a" role="doc-biblioref">2020</a>)</span> showed a relationship between performance and model-size, the resources available will most probably limit you to smaller models. Additionally, a lot of tasks can be solved by smaller models if they are appropriately fine-tuned <span class="citation" data-cites="hsiehDistillingStepStepOutperforming2023">(<a href="#ref-hsiehDistillingStepStepOutperforming2023" role="doc-biblioref">Hsieh et al., 2023</a>)</span>.</p>
<p>A good idea when choosing an open source model is to start small and test whether the performace is sufficient for your use case. If not, you can always try a larger model later on.</p>
<p>Additionally, it is good practice to check the license of the model you want to use. Some models are only available under a non-commercial license, which means that you cannot use them for commercial purposes.</p>
<p>Thirdly, you should make sure that the model you choose is appropriate for your use case. For example, if you want to use a model for text generation, you should make sure that it was trained on a dataset that is similar to the data you will be using. If you want to use a model for translation, you should make sure that it was trained on a dataset that includes the languages you are interested in. A lot of usecases do already have benchmark datasets that can be used to pit models against each other and evaluate there appropriateness for a given use case based on a few key metrics.</p>
<p>A good starting point for getting an overview about such metrics and benchmarks is <a href="https://huggingface.co">Hugging Face</a>. This platform has long cemented itself as the go-to place for getting access to open source models, but also provides a lot of resources for evaluating and comparing them. <a href="https://huggingface.co/collections/open-llm-leaderboard/the-big-benchmarks-collection-64faca6335a7fc7d4ffe974a">This page</a> provides an overview of benchmarks, leaderboards and comparisons for a variety of tasks.</p>
</section>
<section id="basics-of-using-open-source-models" class="level2">
<h2 class="anchored" data-anchor-id="basics-of-using-open-source-models">Basics of using open source models</h2>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>📝 Task
</div>
</div>
<div class="callout-body-container callout-body">
<p>Now it is your turn! In your project-groups, you will each have to build a small application that uses an open source model to generate code.</p>
<ol type="1">
<li><p>Choose a small model (&lt; 3B parameters) using the <a href="https://huggingface.co/collections/open-llm-leaderboard/the-big-benchmarks-collection-64faca6335a7fc7d4ffe974a">sources we discussed before</a>.</p></li>
<li><p>Each group is to use one of the following frameworks from python to load and use the model:</p>
<ul>
<li><a href="https://lmstudio.ai/">LM-Studio</a></li>
<li><a href="https://ollama.com/">Ollama</a></li>
<li><a href="https://huggingface.co/">Huggingface</a></li>
<li><a href="https://github.com/vllm-project/vllm">VLLM</a></li>
</ul></li>
</ol>
<p>All APIs above use the de facto standard of the OpenAI API scheme. This scheme presents multiple POST-endpoints, of which we will mostly use the chat-completion.</p>
<p>Your task is to prompt the model to generate a Python program that prints “Hello World” to the console. Use the following prompt:</p>
<pre><code>Write a Python program that prints "Hello World" to the console. 
Provide only the code without any explanations.</code></pre>
<p>You can either directly call the API using python requests:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> requests</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> requests.post(<span class="st">"http://&lt;your API endpoint&gt;:&lt;port&gt;/v1/chat/completions"</span>,</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>json <span class="op">=</span> {</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">"model"</span>: <span class="st">"qwen2.5-coder-1.5b"</span>,</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">"messages"</span>:[</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>            <span class="st">"role"</span>: <span class="st">"user"</span>,</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>            <span class="st">"content"</span>: <span class="st">"Write a Python program that prints </span><span class="ch">\"</span><span class="st">Hello World</span><span class="ch">\"</span><span class="st"> to the console. Provide only the code without any explanations."</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    ]})</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> json.loads(response.content.decode())[<span class="st">"choices"</span>][<span class="dv">0</span>][<span class="st">"message"</span>][<span class="st">"content"</span>]</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(result)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Or you can call it using the wrapper in the OpenAI-python-module:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> openai <span class="im">import</span> OpenAI</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>client <span class="op">=</span> OpenAI(</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    api_key<span class="op">=</span><span class="st">'lm-studio'</span>,  </span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    base_url<span class="op">=</span><span class="st">"http://&lt;your API endpoint&gt;:&lt;port&gt;/v1"</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>chat_completion <span class="op">=</span> client.chat.completions.create(</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    messages<span class="op">=</span>[</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>            <span class="st">"role"</span>: <span class="st">"user"</span>,</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>            <span class="st">"content"</span>: <span class="st">"Write a Python program that prints </span><span class="ch">\"</span><span class="st">Hello World</span><span class="ch">\"</span><span class="st"> to the console. Provide only the code without any explanations."</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span><span class="st">"qwen2.5-coder-1.5b"</span>,</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(chat_completion.choices[<span class="dv">0</span>].message.content)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ol start="3" type="1">
<li><p>Test whether the generated code executes correctly.</p></li>
<li><p>Present your results and your experiences with the frameworks to the course. Cover the following aspects:</p>
<ul>
<li>Model selection rationale</li>
<li>Setup process and challenges</li>
<li>Generated code quality</li>
<li>Framework usability</li>
</ul></li>
<li><p>Submit your code and a brief report less than a page on moodle.</p></li>
</ol>
</div>
</div>
</section>
<section id="further-readings" class="level2">
<h2 class="anchored" data-anchor-id="further-readings">Further Readings</h2>
<ul>
<li><p><a href="https://heidloff.net/article/foundation-models-transformers-bert-and-gpt/">This</a> quite high-level blog-article about foundational models by <span class="citation" data-cites="heidloffFoundationModelsTransformers2023">Heidloff (<a href="#ref-heidloffFoundationModelsTransformers2023" role="doc-biblioref">2023</a>)</span></p></li>
<li><p>The Attention is all you need-paper <span class="citation" data-cites="vaswaniAttentionAllYou2023a">(<a href="#ref-vaswaniAttentionAllYou2023a" role="doc-biblioref">Vaswani et al., 2023</a>)</span> and the brilliant video discussing it by Umar Jamil <span class="citation" data-cites="umarjamilAttentionAllYou2023">(<a href="#ref-umarjamilAttentionAllYou2023" role="doc-biblioref">Umar Jamil, 2023</a>)</span></p></li>
<li><p><a href="https://stats.stackexchange.com/a/424127">This</a> <em>very</em> good answer on stack exchange that explains the attention-concept <span class="citation" data-cites="424127">(<a href="#ref-424127" role="doc-biblioref">(https://stats.stackexchange.com/users/95569/dontloo), n.d.</a>)</span></p></li>
</ul>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-AIAAICChatGPTTraining" class="csl-entry" role="listitem">
<em><span>AIAAIC - ChatGPT training emits 502 metric tons of carbon</span></em>. (n.d.). https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chatgpt-training-emits-502-metric-tons-of-carbon.
</div>
<div id="ref-bahdanauNeuralMachineTranslation2014" class="csl-entry" role="listitem">
Bahdanau, D. (2014). Neural machine translation by jointly learning to align and translate. <em>arXiv Preprint arXiv:1409.0473</em>. <a href="https://arxiv.org/abs/1409.0473">https://arxiv.org/abs/1409.0473</a>
</div>
<div id="ref-bojanowskiEnrichingWordVectors2017" class="csl-entry" role="listitem">
Bojanowski, P., Grave, E., Joulin, A., &amp; Mikolov, T. (2017). <em>Enriching <span>Word Vectors</span> with <span>Subword Information</span></em> (arXiv:1607.04606). arXiv. <a href="https://doi.org/10.48550/arXiv.1607.04606">https://doi.org/10.48550/arXiv.1607.04606</a>
</div>
<div id="ref-devlinBERTPretrainingDeep2019a" class="csl-entry" role="listitem">
Devlin, J., Chang, M.-W., Lee, K., &amp; Toutanova, K. (2019). <em><span>BERT</span>: <span class="nocase">Pre-training</span> of <span>Deep Bidirectional Transformers</span> for <span>Language Understanding</span></em> (arXiv:1810.04805). arXiv. <a href="https://doi.org/10.48550/arXiv.1810.04805">https://doi.org/10.48550/arXiv.1810.04805</a>
</div>
<div id="ref-heidloffFoundationModelsTransformers2023" class="csl-entry" role="listitem">
Heidloff, N. (2023). Foundation <span>Models</span>, <span>Transformers</span>, <span>BERT</span> and <span>GPT</span>. In <em>Niklas Heidloff</em>. https://heidloff.net/article/foundation-models-transformers-bert-and-gpt/.
</div>
<div id="ref-hoffmannTrainingComputeOptimalLarge2022a" class="csl-entry" role="listitem">
Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. de L., Hendricks, L. A., Welbl, J., Clark, A., Hennigan, T., Noland, E., Millican, K., Driessche, G. van den, Damoc, B., Guy, A., Osindero, S., Simonyan, K., Elsen, E., … Sifre, L. (2022). <em>Training <span>Compute-Optimal Large Language Models</span></em> (arXiv:2203.15556). arXiv. <a href="https://doi.org/10.48550/arXiv.2203.15556">https://doi.org/10.48550/arXiv.2203.15556</a>
</div>
<div id="ref-hsiehDistillingStepStepOutperforming2023" class="csl-entry" role="listitem">
Hsieh, C.-Y., Li, C.-L., Yeh, C.-K., Nakhost, H., Fujii, Y., Ratner, A., Krishna, R., Lee, C.-Y., &amp; Pfister, T. (2023). <em>Distilling <span class="nocase">Step-by-Step</span>! <span>Outperforming Larger Language Models</span> with <span>Less Training Data</span> and <span>Smaller Model Sizes</span></em> (arXiv:2305.02301). arXiv. <a href="https://doi.org/10.48550/arXiv.2305.02301">https://doi.org/10.48550/arXiv.2305.02301</a>
</div>
<div id="ref-424127" class="csl-entry" role="listitem">
(https://stats.stackexchange.com/users/95569/dontloo), dontloo. (n.d.). <em>What exactly are keys, queries, and values in attention mechanisms?</em> Cross Validated.
</div>
<div id="ref-hussainTutorialOpensourceLarge2024" class="csl-entry" role="listitem">
Hussain, Z., Binz, M., Mata, R., &amp; Wulff, D. U. (2024). A tutorial on open-source large language models for behavioral science. <em>Behavior Research Methods</em>, <em>56</em>(8), 8214–8237. <a href="https://doi.org/10.3758/s13428-024-02455-8">https://doi.org/10.3758/s13428-024-02455-8</a>
</div>
<div id="ref-kaplanScalingLawsNeural2020a" class="csl-entry" role="listitem">
Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., &amp; Amodei, D. (2020). <em>Scaling <span>Laws</span> for <span>Neural Language Models</span></em> (arXiv:2001.08361). arXiv. <a href="https://doi.org/10.48550/arXiv.2001.08361">https://doi.org/10.48550/arXiv.2001.08361</a>
</div>
<div id="ref-mikolovEfficientEstimationWord2013" class="csl-entry" role="listitem">
Mikolov, T., Chen, K., Corrado, G., &amp; Dean, J. (2013). <em>Efficient <span>Estimation</span> of <span>Word Representations</span> in <span>Vector Space</span></em> (arXiv:1301.3781). arXiv. <a href="https://doi.org/10.48550/arXiv.1301.3781">https://doi.org/10.48550/arXiv.1301.3781</a>
</div>
<div id="ref-petersDeepContextualizedWord2018" class="csl-entry" role="listitem">
Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., &amp; Zettlemoyer, L. (2018). <em>Deep contextualized word representations</em> (arXiv:1802.05365). arXiv. <a href="https://doi.org/10.48550/arXiv.1802.05365">https://doi.org/10.48550/arXiv.1802.05365</a>
</div>
<div id="ref-radfordImprovingLanguageUnderstanding2018" class="csl-entry" role="listitem">
Radford, A., Narasimhan, K., Salimans, T., &amp; Sutskever, I. (2018). <em>Improving language understanding with unsupervised learning</em>.
</div>
<div id="ref-umarjamilAttentionAllYou2023" class="csl-entry" role="listitem">
Umar Jamil. (2023). <em>Attention is all you need (<span>Transformer</span>) - <span>Model</span> explanation (including math), <span>Inference</span> and <span>Training</span></em>.
</div>
<div id="ref-vaswaniAttentionAllYou2023a" class="csl-entry" role="listitem">
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., &amp; Polosukhin, I. (2023). <em>Attention <span>Is All You Need</span></em> (arXiv:1706.03762). arXiv. <a href="https://doi.org/10.48550/arXiv.1706.03762">https://doi.org/10.48550/arXiv.1706.03762</a>
</div>
<div id="ref-villalobosPositionWillWe2024" class="csl-entry" role="listitem">
Villalobos, P., Ho, A., Sevilla, J., Besiroglu, T., Heim, L., &amp; Hobbhahn, M. (2024, June). Position: <span>Will</span> we run out of data? <span>Limits</span> of <span>LLM</span> scaling based on human-generated data. <em>Forty-First <span>International Conference</span> on <span>Machine Learning</span></em>.
</div>
</div>
</section>


</main> <!-- /main -->
<script>
var elements = document.getElementsByClassName('card');

var myFunction = function() {
  var overlay = this.querySelector('.overlay');
  var content = this.querySelector('.content');
  content.classList.toggle('blur-effect');
  if (overlay) {
    overlay.classList.toggle('show-overlay')
  }
}

for (var i = 0; i < elements.length; i++) {
    elements[i].addEventListener('click', myFunction, false);
    myFunction.call(elements[i]);
}

document.addEventListener('DOMContentLoaded', function() {
  const images = document.querySelectorAll('.gif-image');
  
  images.forEach(function(image) {
    image.addEventListener('click', function() {
        console.log(this.src)
        console.log(this.src.slice(0,-4))
        if(this.src.substr(-4) == '.gif') {
          this.src = this.src.slice(0,-4) + '.png'
        } else {
          this.src = this.src.slice(0,-4) + '.gif'
        }
      });
  });
});

</script>
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../content/project_details.html" class="pagination-link" aria-label="Project Details">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">Project Details</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../content/prompting.html" class="pagination-link" aria-label="Prompting">
        <span class="nav-page-text"><span class="chapter-title">Prompting</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p><a rel="license" href=" https://creativecommons.org/licenses/by-nc-sa/4.0/" style="padding-right: 10px;"><img alt="Creative Commons Lizenzvertrag" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png"></a></p>
</div>   
    <div class="nav-footer-center">
<p>All images are generated using Python, R, draw.io, Flux or Stable Diffusion XL if not indicated otherwise.</p>
<div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/MBrede/generative_ai/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>