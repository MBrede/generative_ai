---
toc-title: '![](../imgs/robotic_agent.png){width=240px}<br> <h3>Agent basics</h3>'
---

# Agent basics

<!-- ## Fundamentals of agents and train-of-thought prompting -->

Before we get into agents proper, let us first discuss prompting strategies for solving complex problems. Below are two examples for using custom prompts that help the LLM in answering more complex problems by first divide the problem into smaller steps before solving each step separately. 


### Chain-of-Thought prompting

Chain-of-Thought (CoT) prompting refers to the technique of giving the LLM hints in the user input on how to solve the problem step by step, similar to what we did above [@weiChainThoughtPromptingElicits2023]. In the original paper, this was used with few-shot prompting (giving the LLM examples in the prompt), see figure below. But it is also possible to use it with zero-shot prompting (i.e. without examples) by invoking the magical words "Let's think step by step" [@kojimaLargeLanguageModels2023]^[Note that these informations get old *fast*. Newer LLMs may have this functionality build in already]. 

![Chain-of-Thought prompting illustrated [@weiChainThoughtPromptingElicits2023]](../imgs/chain_of_thought.png)


::: {.callout-note}
## üìù Task

Now it is your turn!

1. Open a notebook and connect it with a local LLM (e.g. using Ollama or LM Studio).
2. Try letting your LLM solve a complex task e.g. the one given above ("The cafeteria had 23 apples. If they used 20 to
make lunch and bought 6 more, how many apples
do they have?")
    - using the standard prompt (i.e. using the question as is)
    - using few shot examples, with step by step walkthrough
    - without examples but adding "let's think step by step." 
3. Try other, more interesting questions. 
:::


### Tree of Thoughts

Tree of Thoughts (ToT) is a generalization on CoT prompting. The papers on ToT [@longLargeLanguageModel2023], [@yaoTreeThoughtsDeliberate2023], are somewhat complex, so we will not discuss them in detail here. In short, LLMs are used to generate thoughts, that serve as intermediate steps towards the solution. The difference to CoT is basically, that several thougts are generated at each step, creating a tree-like structure. This tree is then searched using breadth-first search or depth-first search until a solution is found. A simplified example using just a custom prompt is given by [@hulbertUsingTreeThoughtPrompting2023]: 

    Imagine three different experts are answering this question.
    All experts will write down 1 step of their thinking,
    then share it with the group.
    Then all experts will go on to the next step, etc.
    If any expert realises they're wrong at any point then they leave.
    The question is...


::: {.callout-note}
## üìù Task

- Try it: in your notebook, also use the prompt above to answer your questions. 

:::

Now that we have the prompting strategies covered, we can start asking the important questions. 


## What is an agent? 

"An AI agent is a system that uses an LLM to decide the control flow of an application." [@WhatAIAgent2024]

In the context of large language models, agents are LLM-based systems that can solve complex tasks. In slightly more technical terms, an agent is a wrapper layer around an LLM, that uses specialized prompts to repeatedly call LLMs to solve complicated tasks step by step. Additionally, an agent also has a set of tools, that it can decide to call for solving steps it cannot solve on its own. Let's illustrate this with an example.

Imagine asking a question like: 

__"What were the key learnings from the Generative AI elective module in WiSe 24/25 at FH Kiel?"__

Could you just ask an LLM that question and expect a correct answer? 

It is in theory possible, that an LLM could answer that directly, but only if it was trained on this information, that is, if a text describing the module exists, is accessible from the web and was used in training the model. However, usually we can not expect the LLM to have this knowledge. 

Let's think for a moment how a human would answer that (one that did not attend the module). We would probably try to get a copy of the script, maybe we saved the script to our hard drive or other data storage. Maybe we could search the web for a description or text version of the module. Having obtained a copy of the script, we would probably read it. Then, we would try to distill the information hidden therein, to answer the question. 

So, for our LLM to answer that question, it needs to be able to perform several tasks:

- Searching the web for relevant documents
- searching in a local file storage or other database
- Reading and understanding a document
- Summarizing the content of a document
- Answering questions based on the summary of a document

This is where agents come into play. Agents are LLM-based systems that can solve complex tasks by performing several subtasks in sequence, using an LLM to decide which subtask to perform next. In our example, the agent would first search the web for relevant documents, then read and understand them, summarize them and finally answer the question based on the summary.


## Agent framework

![Architecture of the agent framework [@LLMAgentsNextra2024]](../imgs/agent-framework.png)

To facilitate this, an agent system consists of several components:

- **Agent**: the agent core acting as coordinator
- **Planning**: Assists the agent in breaking down the complex task into subtasks
- **Tools**: functions that the agent can use to perform a specific task
- **Memory**: used to store information about previous interactions with the agent

We will describe each of them below. 


### Agent

This is a general-purpose LLM, that functions as the brain and main decision-making component of an agent. It determines which tools to use and how to combine their results to solve complex tasks. The agent core uses the output of the previous tool as input for the next tool. It also uses an LLM to decide when to stop using tools and return a final answer. The behavior of the agent and the tools, it has at its disposal, is defined by a prompt template.


### Planning

Planning is the process of breaking down a complex task into subtasks and deciding which tools to use for each subtask. The planning module is usually also an LLM, it can be one fine-tuned to this specific task or receive a specialized prompt. It uses techniques similar to CoT or ToT prompting to generate a plan of action.  

<!-- or __ReAct__ [@yaoReActSynergizingReasoning2023].
 and __Reflexion__ [@shinnReflexionLanguageAgents2023]. 
We will discuss these in more detail later. -->


### Tools

Tools are functions that the agent can use to perform a specific task. They can be pre-defined or dynamically generated based on the user's needs. Tools can be simple, such as a calculator, or complex, such as a web search engine. Tools can also be other agents, allowing for the creation of multi-agent systems. In our example, the tools would be a web search engine and a document reader. Other popular tools are a data store or a python interpreter. 


### Memory

Memory is used to store information about previous interactions with the agent. This allows the agent to remember past conversations and use this information in future interactions. Memory can be short-term, such as a conversation buffer, or long-term, such as a database. Memory can also be used to store the results of previous tool uses, allowing the agent to reuse them if necessary.

### ReAct

ReAct (short for Synergizing Reasoning and Acting) is a technique based on CoT, that updates its reasoning after each step of tool use [@yaoReActSynergizingReasoning2023]. This allows the agent to react (pun intended) to unforeseen results during the step-by-step solution i.e. failed tool use. The agent can then follow a different chain of thoughts. This makes it very well suited to tool use. An illustration is given in the figure below. 

![Comparison of ReAct with other prompting techniques [@yaoReActSynergizingReasoning2023]](../imgs/ReAct.png)


## Examples of agent-frameworks (Llamaindex, LangChain & Haystack)

There are *a lot* of agent frameworks out there. In this module we will focus on three of them: LlamaIndex, LangChain and Haystack. They all have their own strengths and weaknesses, but they all share the same basic architecture as described above. We will describe each of them below.

 - [Llamaindex](https://github.com/run-llama/llama_index): LlamaIndex is a data framework for your LLM applications. It provides a central interface to connect your LLMs and your data. It also provides a set of tools to help you build your own applications, such as a document reader, a web search engine, a data store, etc.  - [LangChain](https://github.com/hwchase17/langchain): LangChain is a framework for developing applications powered by language models. It provides a set of tools to help you build your own applications, such as a document reader, a web search engine, a data store, etc. It also provides a set of agents that can use these tools to solve complex tasks.
 - [Haystack](https://github.com/deepset-ai/haystack): Haystack is an open source NLP framework that enables you to build production-ready applications around LLMs and other models. It provides a set of tools to help you build your own applications, such as a document reader, a web search engine, a data store, etc. It also provides a set of agents that can use these tools to solve complex tasks.

 <!-- - [AutoGen](https://github.com/microsoft/autogen): AutoGen is an open source framework for developing large language model (LLM) applications using multiple LLMs that can collaborate with each other. It enables the development of multi-agent systems, where each agent has a specific role and can communicate with other agents to solve complex tasks. -->

::: {.callout-note}
## üìù Task

Let's try them!

1. Choose one of the frameworks below. Llamaindex seems like a good starting point, but feel free to take one of the other if you feel confident. In your notebook, build an agent, that can use your local LLM. 
    - [Llamaindex](https://docs.llamaindex.ai/en/stable/understanding/agent/) combine this approach with [this notebook](https://docs.llamaindex.ai/en/stable/examples/llm/lmstudio/) to make it work with LM Studio.

    - [Langchain](https://python.langchain.com/docs/tutorials/agents/)
    - [Haystack](https://docs.haystack.deepset.ai/docs/agent_overview)
    - (optional) another framework of your choice
2. Choose a small task for your agent, e.g. answering questions about a specific topic, summarizing a document, etc. (use the one in the respective tutorial)
3. Create more tools to be used by the agent. 
:::




## Further Readings

- [This paper](@wangSurveyLargeLanguage2024) compares different planning strategies
- In addition to the websites listed above see also [@IntroductionLLMAgents2023]


## References