<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Prompting ‚Äì Generative AI</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../content/function_calling.html" rel="next">
<link href="../content/getting_started_with_llms.html" rel="prev">
<link href="../cover.jpg" rel="icon" type="image/jpeg">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-7c56b0cb14979e59e30aba88ccee8faa.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar docked slimcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../content/getting_started_with_llms.html">Language Models</a></li><li class="breadcrumb-item"><a href="../content/prompting.html"><span class="chapter-title">Prompting</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../index.html" class="sidebar-logo-link">
      <img src="../cover.jpg" alt="" class="sidebar-logo light-content py-0 d-lg-inline d-none">
      <img src="../cover.jpg" alt="" class="sidebar-logo dark-content py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Generative AI</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/MBrede/generative_ai" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../Generative-AI.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/orga.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Organizational Details</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/project_details.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Project Details</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Language Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/getting_started_with_llms.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Getting started with (L)LMs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/prompting.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">Prompting</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/function_calling.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Function Calling</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/agent_basics.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Agent basics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/embeddings.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Embedding-based LLM-systems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/agent_interaction.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">LLM pipelines</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Image Generation</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/diff_models.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">AI image generation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/generation_in_agent_pipelines.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">AI image generation pipelines</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Other</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/model_context_protocol.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Model Context Protocol</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Finetuning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/finetuning_approaches.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Finetuning Approaches</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/alignment.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Alignment</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="2">
    <h2 id="toc-title"><img src="../imgs/prompting.jpg" class="img-fluid" width="240"><br><br>
</h2><h3 class="anchored"><br>
Prompting<br>
</h3>
   
  <ul>
  <li><a href="#sec-instruct" id="toc-sec-instruct" class="nav-link active" data-scroll-target="#sec-instruct">Instruct-tuned models</a></li>
  <li><a href="#prompting-strategies" id="toc-prompting-strategies" class="nav-link" data-scroll-target="#prompting-strategies">Prompting strategies</a></li>
  <li><a href="#generation-of-synthetic-texts" id="toc-generation-of-synthetic-texts" class="nav-link" data-scroll-target="#generation-of-synthetic-texts">Generation of synthetic texts</a></li>
  <li><a href="#temperature" id="toc-temperature" class="nav-link" data-scroll-target="#temperature">Temperature</a></li>
  <li><a href="#understanding-and-mitigating-hallucinations" id="toc-understanding-and-mitigating-hallucinations" class="nav-link" data-scroll-target="#understanding-and-mitigating-hallucinations">Understanding and Mitigating Hallucinations</a>
  <ul class="collapse">
  <li><a href="#what-are-hallucinations" id="toc-what-are-hallucinations" class="nav-link" data-scroll-target="#what-are-hallucinations">What are Hallucinations?</a></li>
  <li><a href="#why-hallucinations-occur-a-statistical-perspective" id="toc-why-hallucinations-occur-a-statistical-perspective" class="nav-link" data-scroll-target="#why-hallucinations-occur-a-statistical-perspective">Why Hallucinations Occur: A Statistical Perspective</a></li>
  <li><a href="#hallucinations-as-compression-failures" id="toc-hallucinations-as-compression-failures" class="nav-link" data-scroll-target="#hallucinations-as-compression-failures">Hallucinations as Compression Failures</a></li>
  <li><a href="#why-hallucinations-persist-after-training" id="toc-why-hallucinations-persist-after-training" class="nav-link" data-scroll-target="#why-hallucinations-persist-after-training">Why Hallucinations Persist After Training</a></li>
  </ul></li>
  <li><a href="#further-readings" id="toc-further-readings" class="nav-link" data-scroll-target="#further-readings">Further Readings</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/MBrede/generative_ai/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../content/getting_started_with_llms.html">Language Models</a></li><li class="breadcrumb-item"><a href="../content/prompting.html"><span class="chapter-title">Prompting</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-prompting" class="quarto-section-identifier"><span class="chapter-title">Prompting</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Prompting describes the utilization of the ability of language models to use zero or few-shot instrutions to perform a task. This ability, which we briefly touched on when we were discussing the history of language models (i.e., the paper by <span class="citation" data-cites="radfordLanguageModelsAre2019">Radford et al. (<a href="#ref-radfordLanguageModelsAre2019" role="doc-biblioref">2019</a>)</span>), is one of the most important aspects of modern large language models.</p>
<p>Prompting can be used for various tasks such as text generation, summarization, question answering, and many more.</p>
<section id="sec-instruct" class="level2">
<h2 class="anchored" data-anchor-id="sec-instruct">Instruct-tuned models</h2>
<p>Instruct-tuned models are trained on a dataset (for an example, see <a href="#fig-instructData" class="quarto-xref">Figure&nbsp;<span>4.1</span></a>) that consists of instructions and their corresponding outputs, seperated by special tokens. This is different from the pretraining phase of language models where they are trained on large amounts of text data without any specific task in mind. The goal of instruct-tuning is to make the model better at following instructions and generating more accurate and relevant outputs.</p>
<div id="fig-instructData" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-instructData-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../imgs/instruct_dataset.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-instructData-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Fig&nbsp;4.1: An example for a dataset that can be used for instruct-finetuning. This dataset can be found on <a href="https://huggingface.co/datasets/rajpurkar/squad">huggingface</a>
</figcaption>
</figure>
</div>
<p>These finetuning-datasets are formatted into a specific structure, usually in the form of a chat template. As you can see in the following quite simple example from the <a href="https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct?chat_template=default">SmolLM2-Huggingface-repo</a>, the messages are separated by special tokens and divided into system-message and messages indicated by the relevant role:</p>
<pre><code>{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '&lt;|im_start|&gt;system
You are a helpful AI assistant named SmolLM, trained by Hugging Face&lt;|im_end|&gt;
' }}{% endif %}{{'&lt;|im_start|&gt;' + message['role'] + '
' + message['content'] + '&lt;|im_end|&gt;' + '
'}}{% endfor %}{% if add_generation_prompt %}{{ '&lt;|im_start|&gt;assistant
' }}{% endif %}</code></pre>
<p>There are usually three types of messages used in the context of instruction tuning and the usage of instruct models:</p>
<ol type="1">
<li><strong>System prompts</strong> which tell the model its general role and behavior.</li>
<li><strong>User prompts</strong>, which contain the actual instructions or questions for the model to respond to.</li>
<li><strong>Assistant prompts</strong>, which are the responses generated by the model based on the user‚Äôs input, or, in terms of the training phase, the answers that the model should learn to generate.</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>üìù Task
</div>
</div>
<div class="callout-body-container callout-body">
<p>Test the difference between instruct and non-instruct-models.</p>
<p>Do this by trying to get a gpt2-version (i.e., <a href="https://model.lmstudio.ai/download/QuantFactory/gpt2-xl-GGUF">‚ÄúQuantFactory/gpt2-xl-GGUF‚Äù</a>) and a small Instruct-Model (i.e., <a href="https://model.lmstudio.ai/download/lmstudio-community/Qwen3-0.6B-GGUF">‚ÄúQwen/Qwen3-0.6B‚Äù</a> to write a small poem about the inception of the field of language modelling.</p>
<p>Use LM-Studio to test this. Do also play around with the system prompt and note the effect of changing it.</p>
</div>
</div>
<div class="card">
<div class="content">
<div id="fig-poems" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-poems-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-poems" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-llamapoem" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-llamapoem-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../imgs/poem_by_qwen.png" class="img-fluid figure-img" data-ref-parent="fig-poems">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-llamapoem-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) A poem written by Qwen3 0.6B - a model with Instruct-Finetuning
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-poems" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-gpt2poem" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-gpt2poem-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../imgs/gpt2poem.png" class="img-fluid figure-img" data-ref-parent="fig-poems">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-gpt2poem-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) A ‚Äúpoem‚Äù written by GPT2 - a model without Instruct-Finetuning
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-poems-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Fig&nbsp;4.2: A poem and a ‚Äúpoem‚Äù
</figcaption>
</figure>
</div>
</div>
<div class="overlay">
<p>Show answer</p>
</div>
</div>
</section>
<section id="prompting-strategies" class="level2">
<h2 class="anchored" data-anchor-id="prompting-strategies">Prompting strategies</h2>
<p>The results of a prompted call to a LM is highly dependent on the exact wording of the prompt. This is especially true for more complex tasks, where the model needs to perform multiple steps in order to solve the task. It is not for naught that the field of ‚Äúprompt engineering‚Äù has emerged. There is a veritable plethora of resources available online that discuss different strategies for prompting LMs. It has to be said though, that the strategies that work and don‚Äôt work can vary greatly between models and tasks. A bit of general advice that holds true for nearly all models though, is to</p>
<ol type="a">
<li>define the task in as many small steps as possible</li>
<li>to be as literal and descriptive as possible and</li>
<li>to provide examples if possible.</li>
</ol>
<p>Since the quality of results is so highly dependent on the chosen model, it is good practice to test candidate strategies against each other and therefore to define a target on which the quality of results can be evaluated. One example for such a target could be a benchmark dataset that contains multiple examples of the task at hand.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>üìù Task
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>1.</strong> Test the above-mentioned prompting strategies on the <a href="https://huggingface.co/datasets/mteb/mtop_intent/viewer/de">MTOP Intent Dataset</a> and evaluate the results against each other. The dataset contains instructions and labels indicating on which task the instruction was intended to prompt. Use a python script to call one of the following three models in LM-Studio for this:</p>
<ol type="1">
<li><a href="https://model.lmstudio.ai/download/lmstudio-community/Phi-4-mini-instruct-GGUF">Phi 4 mini</a></li>
<li><a href="https://model.lmstudio.ai/download/lmstudio-community/Qwen3-0.6B-GGUF">Qwen3 0.6B</a></li>
<li><a href="https://model.lmstudio.ai/download/aimlresearch2023/llama-3.3-1b-it-merged-Q6_K-GGUF">Llama 3.3 1B</a></li>
</ol>
<p>Use the <a href="https://en.wikipedia.org/wiki/F-score">F1-score</a> implemented in <a href="https://scikit-learn.org/1.5/modules/generated/sklearn.metrics.f1_score.html">scikit learn</a> to evaluate your results.</p>
<p>Since the dataset has a whole series of labels, use the following python-snippet (or your own approach) to extract only examples using the ‚ÄúGET_MESSAGE‚Äù-label:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> []</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">'data/de_test.jsonl'</span>, <span class="st">'r'</span>) <span class="im">as</span> f:</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> line <span class="kw">in</span> f:</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>        data.append(json.loads(line))</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>possible_labels <span class="op">=</span> <span class="bu">list</span>(<span class="bu">set</span>([entry[<span class="st">'label_text'</span>] <span class="cf">for</span> entry <span class="kw">in</span> data]))</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>texts_to_classify <span class="op">=</span> [</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    {<span class="st">'example'</span>: entry[<span class="st">'text'</span>],</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>     <span class="st">'label'</span>: <span class="st">'GET_MESSAGE'</span> <span class="cf">if</span> entry[<span class="st">'label_text'</span>] <span class="op">==</span> <span class="st">'GET_MESSAGE'</span> </span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>              <span class="cf">else</span> <span class="st">'OTHER'</span>} <span class="cf">for</span> entry <span class="kw">in</span> data</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>2.</strong> You do sometimes read very specific tips on how to improve your results. Here are three, that you can find from time to time:</p>
<ul>
<li>Do promise rewards (i.e., monetary tips) instead of threatening punishments</li>
<li>Do formulate using affirmation (‚Äú<em>Do the task</em>‚Äù) instead of negating behaviours to be avoided (‚Äú<em>Don‚Äôt do this mistake</em>‚Äù)</li>
<li>Let the model reason about the problem before giving an answer</li>
</ul>
<p>Check these strategies on whether they improve your results. If your first instruction already results in near-perfect classification, brainstorm a difficult task that you can validate qualitatively. Let the model write a recipe or describe Kiel for example.</p>
<p><strong>3.</strong> Present your results</p>
<p><strong>3.</strong> Upload your code to moodle</p>
</div>
</div>
</section>
<section id="generation-of-synthetic-texts" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="generation-of-synthetic-texts">Generation of synthetic texts</h2>
<p>As we discussed before, small models can perform on an acceptable level, if they are finetuned appropriately.</p>
<p>A good way to do this is to use a larger model to generate synthetic data that you then use for training the smaller model. This approach, sometimes called ‚Äúdistillation‚Äù <span class="citation" data-cites="xuSurveyKnowledgeDistillation2024">(<a href="#ref-xuSurveyKnowledgeDistillation2024" role="doc-biblioref">Xu et al., 2024</a>)</span> has been used successfully in many applications, for example for improving graph-database queries <span class="citation" data-cites="zhongSyntheT2CGeneratingSynthetic2024">(<a href="#ref-zhongSyntheT2CGeneratingSynthetic2024" role="doc-biblioref">Zhong et al., 2024</a>)</span>, for improving dataset search <span class="citation" data-cites="silvaImprovingDenseRetrieval2024">(<a href="#ref-silvaImprovingDenseRetrieval2024" role="doc-biblioref">Silva &amp; Barbosa, 2024</a>)</span> or the generation of spreadsheet-formulas <span class="citation" data-cites="singhEmpiricalStudyValidating2024">(<a href="#ref-singhEmpiricalStudyValidating2024" role="doc-biblioref">Singh et al., 2024</a>)</span>.</p>
<p>Since even the largest LLMs are not perfect in general and might be even worse on some specific niche tasks, evidence suggests that a validation strategy for data generated in this way is beneficial <span class="citation" data-cites="singhEmpiricalStudyValidating2024 kumarSelectiveFinetuningLLMlabeled2024">(<a href="#ref-kumarSelectiveFinetuningLLMlabeled2024" role="doc-biblioref">Kumar et al., 2024</a>; <a href="#ref-singhEmpiricalStudyValidating2024" role="doc-biblioref">Singh et al., 2024</a>)</span>.</p>
<p>Strategies to validate the synthetic data include:</p>
<ul>
<li>Using a human annotator to label part of the data to test the models output</li>
<li>Forcing the model to answer in a structured way that is automatically testable (e.g., by using JSON - see <a href="#tip-structuredOutput" class="quarto-xref">Tip&nbsp;<span>4.1</span></a> for an example on how to generate structured output using an API that follows the OpenAI-API-scheme.)</li>
<li>Forcing the model to return 2 or more answers and checking for consistency</li>
<li>Combining the two approaches above (i.e., forcing the model to return multiple structured outputs (JSON, XML, YAML, ‚Ä¶) and checking for consistency)</li>
<li>Using a second LLM/different prompt to rate the answers</li>
</ul>
<div id="tip-structuredOutput" class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip&nbsp;4.1: Structured Output
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>There are ways on forcing a language model to only generate output conforming to a specific format. We have already seen one in the examples of models being instruct-tuned to conform to a given chat-template. Another often used method is to use regular expressions to set the probabilities of the next token to be zero if it would not conform to a given (JSON-)scheme. The nicest way to use this feature in python is to define a <code>pydantic</code>-dataclass that defines the possible output formats and describes the expected field content to the model:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> openai <span class="im">import</span> OpenAI</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pydantic <span class="im">import</span> BaseModel, Field</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ClassificationResult(BaseModel):</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    description: <span class="bu">str</span> <span class="op">=</span> Field(description<span class="op">=</span><span class="st">"The description of the classification result."</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    label: <span class="bu">str</span> <span class="op">=</span> Field(pattern<span class="op">=</span><span class="vs">r'</span><span class="dv">^</span><span class="kw">(</span><span class="vs">GET_MESSAGE</span><span class="cf">|</span><span class="vs">OTHER</span><span class="kw">)</span><span class="dv">$</span><span class="vs">'</span>, description<span class="op">=</span><span class="st">"The classified label of the text."</span>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>client <span class="op">=</span> OpenAI(</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    api_key<span class="op">=</span><span class="st">'lm-studio'</span>,  </span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    base_url<span class="op">=</span><span class="st">"http://localhost:1234/v1"</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>test_dict <span class="op">=</span> {<span class="st">"example"</span>: <span class="st">"This is a test"</span>, <span class="st">"example2"</span>: <span class="st">"This is another test"</span>}</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> classify_one_sample(sample):</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>    chat_completion <span class="op">=</span> client.beta.chat.completions.parse(</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>        messages<span class="op">=</span>[</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>            {</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>                <span class="st">"role"</span>: <span class="st">"user"</span>,</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>                <span class="st">"content"</span>: <span class="ss">f"Classify the label of the following text: '</span><span class="sc">{</span>sample<span class="sc">}</span><span class="ss">' as 'GET_MESSAGE', 'OTHER'. /no_think </span><span class="sc">{</span>json_string<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>        ],</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>        model<span class="op">=</span><span class="st">"qwen3-0.6B"</span>,</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>        response_format<span class="op">=</span>ClassificationResult</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> chat_completion.choices[<span class="dv">0</span>].message.content</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>classify_one_sample(<span class="st">"Gib mir die Nachricht von Peter aus!"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>The descriptions are the internally added as context to the prompt and the output is limited to the given data-range. For more possibilities on setting constraints see the <a href="https://docs.pydantic.dev/latest/concepts/fields/"><code>pydantic</code>-docs</a>, for a more detailed explanation on how the token limitations work, see <a href="https://dottxt-ai.github.io/outlines/latest/reference/generation/structured_generation_explanation/">this article by outlined</a>.</p>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>üìù Task
</div>
</div>
<div class="callout-body-container callout-body">
<p>Using your script for batch-testing different prompts, generate synthetic data for a emotion detection task based on <a href="https://en.wikipedia.org/wiki/Paul_Ekman">Paul Ekman‚Äôs</a> six basic emotions: anger, disgust, fear, happiness, sadness and surprise<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>.</p>
<p>The generated data should consist of a sentence and the emotion that is expressed in it. Start by generating two examples for each emotion. Validate these results and adapt them if necessary. Then use these examples to generate 10 samples for each emotion.</p>
<p>Use one of the above mentioned (non-manual) strategies to validate the data you generated.</p>
<p>Upload your results to Moodle.</p>
</div>
</div>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Though this nomenclature has fallen a bit out of fashion</p></div></div></section>
<section id="temperature" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="temperature">Temperature</h2>
<p>You might have encountered eerily similar answers from the language model, especially in the last task. Talking of it - why does the model return different answers to the same prompt at all if we do use pretrained-models in the first place? Shouldn‚Äôt the utilization of the frozen weight-matrix result in the same answer, every time we run the model with the same input?</p>
<p>Yes, it should. And it does.</p>
<p>Remember that a language model trained on language generation as we discussed in the first session ends in a softmax-layer that returns probabilities for each token in the vocabulary. The generation-pipeline does not just use the token with the highest probability though, but samples from this distribution. This means, that even if the input is identical, the output will be different every time you run the model.</p>
<p>The temperature parameter controls the steepness of the softmax-function and thus the randomness of the sampling process. A higher temperature value results in more random outputs, while a lower temperature value results in more ‚Äúdeterministic‚Äù outputs. The temperatur, indicated as a float between 0 and 1<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>, is used to modulate the probabilities of the next token. This is done by adding a <span class="math inline">\(\frac{1}{Temp}\)</span> factor to the model-outputs before applying the softmax.</p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;Depending on the implementation, temperatures above 1 are also allowed. Temperatures above 1 are resultsing in strange behaviours - see <a href="#fig-temp" class="quarto-xref">Figure&nbsp;<span>4.3</span></a>.</p></div></div><p>This effectively changes the Sofmax-fomula from</p>
<p><span class="math display">\[
p_{Token} = \frac{e^{z_{Token}}}{\sum_{i=1}^k e^{z_{i}}}
\]</span></p>
<p>to <span class="math display">\[
p_{Token}(Temp) = \frac{e^{\frac{z_{Token}}{Temp}}}{\sum_{i=1}^k e ^{\frac{z_{i}}{Temp}}}
\]</span></p>
<p>Where</p>
<ul>
<li><span class="math inline">\(z_{Token}\)</span> is the output for a given token</li>
<li><span class="math inline">\(k\)</span> is the size of the vocabulary</li>
<li><span class="math inline">\(Temp\)</span> is the temperature parameter (0 &lt; <span class="math inline">\(Temp\)</span> &lt;= 1)</li>
</ul>
<p>The effect of this temperature can be seen in <a href="#fig-temp" class="quarto-xref">Figure&nbsp;<span>4.3</span></a>.</p>
<div class="cell enlarge-onhover" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-temp" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center" alt="A heatmap illustrating the effect of the temperature parameter on the softmax-output for a given input. The x-axis represents the temperature, the y-axis represents the token-position and the color represents the probability of the token.">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-temp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="prompting_files/figure-html/fig-temp-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" alt="A heatmap illustrating the effect of the temperature parameter on the softmax-output for a given input. The x-axis represents the temperature, the y-axis represents the token-position and the color represents the probability of the token." width="2400">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-temp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Fig&nbsp;4.3: The effect of the temperature parameter on the softmax-output for a given input. The x-axis represents the temperature, the y-axis represents the token-position and the color represents the probability of the token.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Most generation-frameworks do additionally provide a parameter called <em>top_k</em> or <em>top_p</em>. These parameters are used to limit the number of tokens that can be selected as the next token. This is done by sorting the probabilities in descending order and only considering the top k tokens or the top p percent of tokens.</p>
<p>Temperature is the mayor setting to control a LLMs ‚Äúcreativity‚Äù though.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>üìù Task
</div>
</div>
<div class="callout-body-container callout-body">
<p>Using the script provided for generating synthetic data, test the effect of the temperature parameter on the output of the model.</p>
<ul>
<li>Use the same prompt and the same model</li>
<li>Run the model with a temperature value of 0.1, 0.5, 1.0 and 2.0</li>
</ul>
</div>
</div>
</section>
<section id="understanding-and-mitigating-hallucinations" class="level2">
<h2 class="anchored" data-anchor-id="understanding-and-mitigating-hallucinations">Understanding and Mitigating Hallucinations</h2>
<p>While temperature controls the randomness of model outputs, a more fundamental challenge emerges when language models generate plausible yet incorrect information - a phenomenon known as <strong>hallucination</strong>.</p>
<section id="what-are-hallucinations" class="level3">
<h3 class="anchored" data-anchor-id="what-are-hallucinations">What are Hallucinations?</h3>
<p>Hallucinations occur when language models produce confident, plausible-sounding outputs that are factually incorrect or unsupported by their training data. These aren‚Äôt random errors - they‚Äôre systematic failures that arise from the statistical nature of language modeling itself.</p>
<p>Consider this example: When asked ‚ÄúWhat is Adam Tauman Kalai‚Äôs birthday?‚Äù, state-of-the-art models confidently produce different incorrect dates across multiple attempts, even when explicitly asked to respond only if they know the answer <span class="citation" data-cites="kalaiWhyLanguageModels2025">(<a href="#ref-kalaiWhyLanguageModels2025" role="doc-biblioref">Kalai et al., 2025</a>)</span>.</p>
</section>
<section id="why-hallucinations-occur-a-statistical-perspective" class="level3">
<h3 class="anchored" data-anchor-id="why-hallucinations-occur-a-statistical-perspective">Why Hallucinations Occur: A Statistical Perspective</h3>
<p><span class="citation" data-cites="kalaiWhyLanguageModels2025">Kalai et al. (<a href="#ref-kalaiWhyLanguageModels2025" role="doc-biblioref">2025</a>)</span> demonstrate that hallucinations emerge from the fundamental objective of language model training. They show that generating valid outputs is inherently harder than classifying output validity - a task where errors are well-understood in machine learning.</p>
<p>The key insight: even with perfect training data, the cross-entropy objective used in pretraining naturally leads to errors on certain types of facts. Specifically:</p>
<p><strong>Arbitrary Facts</strong>: Information without learnable patterns (like birthdays of obscure individuals) will be hallucinated at rates approximately equal to the fraction of such facts appearing exactly once in training data. If 20% of birthday facts appear only once, expect ~20% hallucination rate on birthdays.</p>
<p><strong>Poor Models</strong>: When model architectures cannot adequately represent certain patterns, systematic errors emerge. For example, models using only token-based representations struggle with character-level tasks like counting letters in ‚ÄúDEEPSEEK‚Äù.</p>
</section>
<section id="hallucinations-as-compression-failures" class="level3">
<h3 class="anchored" data-anchor-id="hallucinations-as-compression-failures">Hallucinations as Compression Failures</h3>
<p><span class="citation" data-cites="chlonPredictableCompressionFailures2025">Chlon et al. (<a href="#ref-chlonPredictableCompressionFailures2025" role="doc-biblioref">2025</a>)</span> provide a complementary information-theoretic perspective. They show that transformers minimize expected conditional description length over input orderings rather than the permutation-invariant description length. This makes them ‚ÄúBayesian in expectation, not in realization.‚Äù</p>
<p>Their framework introduces practical metrics for predicting hallucinations:</p>
<ul>
<li><strong>Information Sufficiency Ratio (ISR)</strong>: The ratio of available information to required information for a target reliability threshold</li>
<li><strong>Bits-to-Trust (B2T)</strong>: The amount of information needed to achieve a specific confidence level</li>
</ul>
<p>A key finding: hallucinations decrease by approximately 0.13 per additional nat of information, making the phenomenon quantitatively predictable rather than mysterious.</p>
</section>
<section id="why-hallucinations-persist-after-training" class="level3">
<h3 class="anchored" data-anchor-id="why-hallucinations-persist-after-training">Why Hallucinations Persist After Training</h3>
<p>Beyond pretraining, <span class="citation" data-cites="kalaiWhyLanguageModels2025">Kalai et al. (<a href="#ref-kalaiWhyLanguageModels2025" role="doc-biblioref">2025</a>)</span> argue that post-training and evaluation procedures actively reinforce hallucinations. Most benchmarks use binary grading (correct/incorrect) with no credit for expressing uncertainty. This creates an ‚Äúepidemic‚Äù of penalizing honest uncertainty - models that guess when unsure outperform those that appropriately abstain.</p>
<p>Consider two models:</p>
<ul>
<li>Model A: Accurately signals uncertainty, never hallucinates</li>
<li>Model B: Same as A but guesses instead of expressing uncertainty</li>
</ul>
<p>Model B will outperform A on most current benchmarks, despite being less trustworthy.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>üìù Task
</div>
</div>
<div class="callout-body-container callout-body">
<p>Test hallucination behavior on a small model using LM-Studio:</p>
<ol type="1">
<li>Use a small model (Qwen3-0.6B or similar) to answer 10 factual questions about rare entities</li>
<li>For each question, generate 3 responses with the same temperature</li>
<li>Document:
<ul>
<li>How often does the model give confident but incorrect answers?</li>
<li>How often does it appropriately express uncertainty?</li>
<li>How does response consistency relate to likely correctness?</li>
</ul></li>
<li>Now modify the prompt to explicitly encourage uncertainty expression (e.g., ‚ÄúOnly answer if you‚Äôre very confident, otherwise say you don‚Äôt know‚Äù)</li>
<li>Compare the results</li>
</ol>
<p>Upload your observations to Moodle.</p>
</div>
</div>
</section>
</section>
<section id="further-readings" class="level2">
<h2 class="anchored" data-anchor-id="further-readings">Further Readings</h2>
<ul>
<li><a href="https://www.promptingguide.ai/">This prompting-guide</a> has some nice general advice</li>
<li><a href="https://platform.openai.com/docs/guides/prompt-engineering">OpenAI</a> has its own set of tipps</li>
<li><a href="https://docs.cloud.deepset.ai/docs/prompt-engineering-guidelines">deepset</a>, the company behind Haystack, has a nice guide as well</li>
<li><a href="https://heidloff.net/article/fine-tune-small-llm-with-big-llm/">This blog-article</a>, again written by Heidloff <span class="citation" data-cites="heidloffFinetuningSmallLLMs2023">(<a href="#ref-heidloffFinetuningSmallLLMs2023" role="doc-biblioref">Heidloff, 2023</a>)</span></li>
</ul>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-chlonPredictableCompressionFailures2025" class="csl-entry" role="listitem">
Chlon, L., Karim, A., &amp; Chlon, M. (2025). <em>Predictable <span>Compression Failures</span>: <span>Why Language Models Actually Hallucinate</span></em> (arXiv:2509.11208). arXiv. <a href="https://doi.org/10.48550/arXiv.2509.11208">https://doi.org/10.48550/arXiv.2509.11208</a>
</div>
<div id="ref-heidloffFinetuningSmallLLMs2023" class="csl-entry" role="listitem">
Heidloff, N. (2023). Fine-tuning small <span>LLMs</span> with <span>Output</span> from large <span>LLMs</span>. In <em>Niklas Heidloff</em>. https://heidloff.net/article/fine-tune-small-llm-with-big-llm/.
</div>
<div id="ref-kalaiWhyLanguageModels2025" class="csl-entry" role="listitem">
Kalai, A. T., Nachum, O., Vempala, S. S., &amp; Zhang, E. (2025). <em>Why <span>Language Models Hallucinate</span></em> (arXiv:2509.04664). arXiv. <a href="https://doi.org/10.48550/arXiv.2509.04664">https://doi.org/10.48550/arXiv.2509.04664</a>
</div>
<div id="ref-kumarSelectiveFinetuningLLMlabeled2024" class="csl-entry" role="listitem">
Kumar, B., Amar, J., Yang, E., Li, N., &amp; Jia, Y. (2024). <em>Selective <span class="nocase">Fine-tuning</span> on <span class="nocase">LLM-labeled Data May Reduce Reliance</span> on <span>Human Annotation</span>: <span class="nocase">A Case Study Using Schedule-of-Event Table Detection</span></em> (arXiv:2405.06093). arXiv. <a href="https://doi.org/10.48550/arXiv.2405.06093">https://doi.org/10.48550/arXiv.2405.06093</a>
</div>
<div id="ref-radfordLanguageModelsAre2019" class="csl-entry" role="listitem">
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., &amp; Sutskever, I. (2019). Language models are unsupervised multitask learners. <em>OpenAI Blog</em>, <em>1</em>(8), 9.
</div>
<div id="ref-silvaImprovingDenseRetrieval2024" class="csl-entry" role="listitem">
Silva, L., &amp; Barbosa, L. (2024). Improving dense retrieval models with <span>LLM</span> augmented data for dataset search. <em>Knowledge-Based Systems</em>, <em>294</em>, 111740. <a href="https://doi.org/10.1016/j.knosys.2024.111740">https://doi.org/10.1016/j.knosys.2024.111740</a>
</div>
<div id="ref-singhEmpiricalStudyValidating2024" class="csl-entry" role="listitem">
Singh, U., Cambronero, J., Gulwani, S., Kanade, A., Khatry, A., Le, V., Singh, M., &amp; Verbruggen, G. (2024). <em>An <span>Empirical Study</span> of <span>Validating Synthetic Data</span> for <span>Formula Generation</span></em> (arXiv:2407.10657). arXiv. <a href="https://doi.org/10.48550/arXiv.2407.10657">https://doi.org/10.48550/arXiv.2407.10657</a>
</div>
<div id="ref-xuSurveyKnowledgeDistillation2024" class="csl-entry" role="listitem">
Xu, X., Li, M., Tao, C., Shen, T., Cheng, R., Li, J., Xu, C., Tao, D., &amp; Zhou, T. (2024). <em>A <span>Survey</span> on <span>Knowledge Distillation</span> of <span>Large Language Models</span></em> (arXiv:2402.13116). arXiv. <a href="https://doi.org/10.48550/arXiv.2402.13116">https://doi.org/10.48550/arXiv.2402.13116</a>
</div>
<div id="ref-zhongSyntheT2CGeneratingSynthetic2024" class="csl-entry" role="listitem">
Zhong, Z., Zhong, L., Sun, Z., Jin, Q., Qin, Z., &amp; Zhang, X. (2024). <em><span>SyntheT2C</span>: <span>Generating Synthetic Data</span> for <span>Fine-Tuning Large Language Models</span> on the <span>Text2Cypher Task</span></em> (arXiv:2406.10710). arXiv. <a href="https://doi.org/10.48550/arXiv.2406.10710">https://doi.org/10.48550/arXiv.2406.10710</a>
</div>
</div>
</section>


</main> <!-- /main -->
<script>
var elements = document.getElementsByClassName('card');

var myFunction = function() {
  var overlay = this.querySelector('.overlay');
  var content = this.querySelector('.content');
  content.classList.toggle('blur-effect');
  if (overlay) {
    overlay.classList.toggle('show-overlay')
  }
}

for (var i = 0; i < elements.length; i++) {
    elements[i].addEventListener('click', myFunction, false);
    myFunction.call(elements[i]);
}

document.addEventListener('DOMContentLoaded', function() {
  const images = document.querySelectorAll('.gif-image');
  
  images.forEach(function(image) {
    image.addEventListener('click', function() {
        console.log(this.src)
        console.log(this.src.slice(0,-4))
        if(this.src.substr(-4) == '.gif') {
          this.src = this.src.slice(0,-4) + '.png'
        } else {
          this.src = this.src.slice(0,-4) + '.gif'
        }
      });
  });
});

</script>
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "Óßã";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../content/getting_started_with_llms.html" class="pagination-link" aria-label="Getting started with (L)LMs">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">Getting started with (L)LMs</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../content/function_calling.html" class="pagination-link" aria-label="Function Calling">
        <span class="nav-page-text"><span class="chapter-title">Function Calling</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p><a rel="license" href=" https://creativecommons.org/licenses/by-nc-sa/4.0/" style="padding-right: 10px;"><img alt="Creative Commons Lizenzvertrag" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png"></a></p>
</div>   
    <div class="nav-footer-center">
<p>All images are generated using Python, R, draw.io, Flux or Stable Diffusion XL if not indicated otherwise.</p>
<div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/MBrede/generative_ai/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>