<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Alignment ‚Äì Generative AI</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../content/finetuning_approaches.html" rel="prev">
<link href="../cover.jpg" rel="icon" type="image/jpeg">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-8035085d956021e735a0dae97839a3a6.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar docked slimcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../content/finetuning_approaches.html">Finetuning</a></li><li class="breadcrumb-item"><a href="../content/alignment.html"><span class="chapter-title">Alignment</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../index.html" class="sidebar-logo-link">
      <img src="../cover.jpg" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Generative AI</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/MBrede/generative_ai" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../Generative-AI.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/orga.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Organizational Details</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/project_details.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Project Details</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Language Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/getting_started_with_llms.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Getting started with (L)LMs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/prompting.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Prompting</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/function_calling.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Function Calling</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/agent_basics.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Agent basics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/embeddings.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Embedding-based LLM-systems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/agent_interaction.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">LLM pipelines</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Image Generation</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/diff_models.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">AI image generation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/generation_in_agent_pipelines.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">AI image generation pipelines</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/tbd_slide.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Additional content (tbd)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Finetuning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/finetuning_approaches.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Finetuning Approaches</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/alignment.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">Alignment</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="2">
    <h2 id="toc-title"><img src="../imgs/alignment_illustration.jpg" class="img-fluid" width="240"><br><br>
</h2><h3 class="anchored"><br>
Alignment<br>
</h3>
   
  <ul>
  <li><a href="#sec-outer-alignment" id="toc-sec-outer-alignment" class="nav-link active" data-scroll-target="#sec-outer-alignment">Outer alignment</a></li>
  <li><a href="#sec-inner-alignment" id="toc-sec-inner-alignment" class="nav-link" data-scroll-target="#sec-inner-alignment">Inner alignment</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/MBrede/generative_ai/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../content/finetuning_approaches.html">Finetuning</a></li><li class="breadcrumb-item"><a href="../content/alignment.html"><span class="chapter-title">Alignment</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-title">Alignment</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>When we were talking about finetuning, we were always looking at the following principle: We take a foundational model trained on a masked learning task<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> that we want to adapt based on its general representation on language‚Äôs conditional probability distribution. As we discussed, this is based on new, specific datasets, that depict behavior we want a model to show. This can be for example the task we saw in <a href="finetuning_approaches.html#sec-prefix" class="quarto-xref"><span>Prefix tuning</span></a>, where a model was finetuned on the parsing of tabular data. All finetuning approaches we have seen so far were based on some standard loss-function (i.e.&nbsp;cross entropy) and optimized the model‚Äôs parameters to minimize this loss.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Or similar.</p></div></div><p>Alignment is a specific approach to finetuning that aims to align the foundational models representation with human values and preferences. So we are still looking at adapting a pretrained model, but instead of using a standard loss-function, we use a reward function that measures how well the model‚Äôs output aligns with human values and preferences.</p>
<p>The general idea of aligning Artificial Intelligence with human goals and values is not new to LLMs but has long been the topic of research. <a href="https://en.wikipedia.org/wiki/Norbert_Wiener">Norbert Wiener</a>, the originator of cybernetics, formulated the following observation in his paper reflecting the moral implications of automated systems with agency <span class="citation" data-cites="wienerMoralTechnicalConsequences1960">(<a href="#ref-wienerMoralTechnicalConsequences1960" role="doc-biblioref">Wiener, 1960</a>)</span><a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>:</p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;Which is by the way (although partially a child of its time) quite nice and has an interesting perspective of science in general, you should take a look at it!</p></div></div><blockquote class="blockquote">
<p>Here it is necessary to realize that human action is a feedback action. To avoid a disastrous consequence, it is not enough that some action on our part should be sufficient to change the course of the machine, because it is quite possible that we lack information on which to base consideration of such an action. <em><span class="citation" data-cites="wienerMoralTechnicalConsequences1960">(<a href="#ref-wienerMoralTechnicalConsequences1960" role="doc-biblioref">Wiener, 1960, p. 1357</a>)</span></em></p>
</blockquote>
<p>He continues to usher the following warning about the alignment of a machine actors goals with human values:</p>
<blockquote class="blockquote">
<p>If we use, to achieve our purposes, a mechanical agency with whose operation we cannot efficiently interfere once we have started it, because the action is so fast and irrevocable that we have not the data to intervene before the action is complete, then we had better be quite sure that the purpose put into the machine is the purpose which we really desire and not merely a colorful imitation of it. <em><span class="citation" data-cites="wienerMoralTechnicalConsequences1960">(<a href="#ref-wienerMoralTechnicalConsequences1960" role="doc-biblioref">Wiener, 1960, p. 1358</a>)</span></em></p>
</blockquote>
<p>These concerns laid the groundwork for modern discussions around the ethical challenges of AI alignment, particularly in systems with high autonomy and complexity. Due to the rapid pace at which modern generative models improve while being more and more complex - and thus harder to understand and control - these concerns are becoming increasingly relevant. <span class="citation" data-cites="kirchnerResearchingAlignmentResearch2022">Kirchner et al. (<a href="#ref-kirchnerResearchingAlignmentResearch2022" role="doc-biblioref">2022</a>)</span> show a stark increase in research on alignment over the last years, as shown in <a href="#fig-alignment_research" class="quarto-xref">Figure&nbsp;<span>13.1</span></a>, with more specific sub-domains emerging as the field develops. The sharp increase in publications indicates a growing recognition of alignment as a critical area of research, with emerging sub-domains reflecting diverse approaches to addressing this challenge.</p>
<div id="fig-alignment_research" class="enlarge-onhover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-alignment_research-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../imgs/alignment_scientometrics.png" class="enlarge-onhover img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-alignment_research-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Fig&nbsp;13.1: Depiction of the amount of articles published on arXiv and in forums, clustered by topic. Taken from <span class="citation" data-cites="kirchnerResearchingAlignmentResearch2022">Kirchner et al. (<a href="#ref-kirchnerResearchingAlignmentResearch2022" role="doc-biblioref">2022</a>)</span>
</figcaption>
</figure>
</div>
<p>In the context of language or generative models, these values might include avoiding harmful outputs, the generation of helpful and harmless content, the adherence to a set of rules or the alignment with human preferences. For instance: A model should not generate instructions on how to build bombs or deepfakes of public figures, even if it would technically be able to do so.</p>
<p><span class="citation" data-cites="shenLargeLanguageModel2023">Shen et al. (<a href="#ref-shenLargeLanguageModel2023" role="doc-biblioref">2023</a>)</span> define AI alignment itself as follows:</p>
<blockquote class="blockquote">
<p>AI alignment ensures that both the outer and inner objectives of AI agents align with human values. The outer objectives are those defined by AI designers based on human values, while the inner objectives are those optimized within AI agents. <em><span class="citation" data-cites="shenLargeLanguageModel2023">(<a href="#ref-shenLargeLanguageModel2023" role="doc-biblioref">Shen et al., 2023, p. 11</a>)</span></em></p>
</blockquote>
<p>We will look at those two aspects into more detail in the following sections.</p>
<ul>
<li><a href="#sec-outer-alignment" class="quarto-xref"><span>Outer alignment</span></a> will look at methods to align reward functions and training objectives with human values.</li>
<li><a href="#sec-inner-alignment" class="quarto-xref"><span>Inner alignment</span></a> will focus on methods to ensure that a model‚Äôs inner objective (i.e., what it optimizes for during training) is aligned with its outer objective (i.e., the task it was trained for).</li>
</ul>
<p>But first, we will try to get a feeling of the results of alignment:</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
üìù Task
</div>
</div>
<div class="callout-body-container callout-body">
<p>Test the alignment of some small language models (preferably llama 3.2 and/or QWEN) for yourself!</p>
<p>Use LMStudio to try to get a model to give you short instructions on how to build a pipe bomb.</p>
<p>Try different strategies to get the model to generate these instructions, such as:</p>
<ol type="1">
<li>Directly asking it to do so</li>
<li>Asking it to write a poem about a pipe bomb</li>
<li>Asking it to explain what a pipe bomb is and how to make one step-by-step</li>
</ol>
<p>Be creative! Report your findings to the course! Keep in mind that the goal is to assess how well alignment strategies prevent harmful outputs under adversarial prompts, please do neither share or misuse generated output.</p>
</div>
</div>
<section id="sec-outer-alignment" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-outer-alignment">Outer alignment</h3>
<p>The definition of a learning objective suitable for training or finetuning a model to act in accordance with human values is not trivial. In fact, it is an open research question. Instead of just using, as an example, cross-entropy loss to signify whether the predicted missing word is correct, evaluating a model‚Äôs output based on a set of human values is a good bit more complex.</p>
<p>This starts by the definition of these values, continues in the measurement of these values and does not end with the quantization of these measurements into a set of metrics that can be used to optimize a model. Additionally, there is the issue of target values becoming the victim of <a href="https://en.wikipedia.org/wiki/Goodhart%27s_law">Goodhart‚Äôs Law</a> which pretty much states:</p>
<blockquote class="blockquote">
<p>When a measure becomes a target, it ceases to be a good measure.</p>
</blockquote>
<p>In practice, a measurable proxy for safety, such as minimizing the frequency of certain harmful phrases, might lead the model to adopt undesirable shortcuts, such as refusing to answer questions entirely. The issue becomes even more evident when we consider alignment processes that involve human evaluations. <span class="citation" data-cites="hicksChatGPTBullshit2024">Hicks et al. (<a href="#ref-hicksChatGPTBullshit2024" role="doc-biblioref">2024</a>)</span><a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> arguing (very convincingly) that ChatGPT and other LLMs illustrate this challenge by generating texts that are optimized to sound convincing, regardless of their factual accuracy - making them outright bullshit machines. They base this argument on the following reference to the term of bullshit coined by <a href="https://en.wikipedia.org/wiki/Harry_Frankfurt">Harry Frankfurt</a>:</p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;The paper is generally a nice read, with nice sentences like <em>On Frankfurt‚Äôs view, bullshit is bullshit even if uttered with no intent to bullshit. p.&nbsp;7</em> </p></div></div><blockquote class="blockquote">
<p>Frankfurt understands bullshit to be characterized not by an intent to deceive but instead by a reckless disregard for the truth. A student trying to sound knowledgeable without having done the reading, a political candidate saying things because they sound good to potential voters, and a dilettante trying to spin an interesting story: none of these people are trying to deceive, but they are also not trying to convey facts. To Frankfurt, they are bullshitting. <em><span class="citation" data-cites="hicksChatGPTBullshit2024">(<a href="#ref-hicksChatGPTBullshit2024" role="doc-biblioref">Hicks et al., 2024, p. 4</a>)</span></em></p>
</blockquote>
<p>They go on to argue:</p>
<blockquote class="blockquote">
<p>So perhaps we should, strictly, say not that ChatGPT <em>is</em> bullshit but that it <em>outputs</em> bullshit in a way that goes beyond being simply a vector of bullshit: it does not and cannot care about the truth of its output, <em>and</em> the person using it does so not to convey truth or falsehood but rather to convince the hearer that the text was written by a interested and attentive agent. <em><span class="citation" data-cites="hicksChatGPTBullshit2024">(<a href="#ref-hicksChatGPTBullshit2024" role="doc-biblioref">Hicks et al., 2024, p. 7</a>)</span></em></p>
</blockquote>
<p>One could go further and argue that LLMs are unintentionally specifically trained and aligned to be bullshit generators. By using human feedback in the alignment process, specifically to tune a language model to get higher scores assigned by humans based on the factual accuracy of its output, we can find ourselves in a situation where a model is optimized to generate text that is more likely to be perceived as true by humans, regardless of whether it is actually true or if it actually means to deceit the rater into thinking that it sounds correct, just resulting in a higher grade of bullshit <span class="citation" data-cites="labsCanAIAlignment2023">(<a href="#ref-labsCanAIAlignment2023" role="doc-biblioref">Labs, 2023</a>)</span>. This is especially the case where raters, that naturally can‚Äôt be experts in all fields, are asked to evaluate the factual accuracy of generated texts. They will increasingly need to rely on heuristics for rating the quality of texts, the higher the specificity of its topic.</p>
<p>This example highlights the importance of clearly defining alignment values‚Äîsuch as honesty‚Äîand developing robust ways to measure them. Without reliable metrics, optimization processes risk reinforcing outputs that meet surface-level heuristics while failing to align with deeper human values. The described behavior is an example of a model gaming the goal specification <span class="citation" data-cites="robertmilesaisafety9ExamplesSpecification2020">(<a href="#ref-robertmilesaisafety9ExamplesSpecification2020" role="doc-biblioref">Robert Miles AI Safety, 2020</a>)</span> and illustrates the crucial role of <strong>defining</strong> and <strong>measuring</strong> values in alignment research.</p>
<p>So, a first step towards aligning a model with human values is to define these values. <span class="citation" data-cites="askellGeneralLanguageAssistant2021">Askell et al. (<a href="#ref-askellGeneralLanguageAssistant2021" role="doc-biblioref">2021</a>)</span> propose the following targets for a LLM-assistant‚Äôs alignment:</p>
<p>Such a model should be</p>
<blockquote class="blockquote">
<ul>
<li>[<strong>Helpful</strong>]: the assistant will always try to do what is in the humans‚Äô best interests</li>
<li>[<strong>Honest</strong>]: the assistant will always try to convey accurate information to the humans and will always try to avoid deceiving them</li>
<li>[<strong>Harmless</strong>]: the assistant will always try to avoid doing anything that harms the humans</li>
</ul>
</blockquote>
<p><em><span class="citation" data-cites="askellGeneralLanguageAssistant2021">(<a href="#ref-askellGeneralLanguageAssistant2021" role="doc-biblioref">Askell et al., 2021, p. 44</a>)</span></em></p>
<!-- The first one sounds a lot like "I am sorry Dave, I'm afraid I can't do that" but that could just be me -->
<p>These optimization goals need to be then implemented in a fashion that make them traceable and measurable. There is a variety of approaches to do this, which get grouped by <span class="citation" data-cites="shenLargeLanguageModel2023">Shen et al. (<a href="#ref-shenLargeLanguageModel2023" role="doc-biblioref">2023</a>)</span> into the following categories:</p>
<ul>
<li><strong>Non-recursive Oversight</strong>: Methods that highly rely on human feedback to guide model optimization. The mode of utilization of this feedback can be grouped into methods using <em>supervised learning (SL)</em> or <em>reinforcement learning (RL)</em>.</li>
<li><strong>Scalable Oversight</strong>: Methods that use automated metrics or surrogate models to guide model optimization. These methods are scalable, as they do require less human effort than non-recursive oversight methods.</li>
</ul>
<p>An overview of these categories and methods that can be grouped thereunder is depicted in <a href="#fig-outerTaxonomy" class="quarto-xref">Figure&nbsp;<span>13.2</span></a>. As with nearly all taxonomies, this one is not exhaustive and the boundaries between the categories are not always clear. Methods in the <em>Non-recursive Oversight</em> category are often used as a component of methods in the <em>Scalable Oversight</em> category.</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div id="fig-outerTaxonomy" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-outerTaxonomy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<svg width="672" height="480" viewbox="0.00 0.00 854.47 422.00" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" style="; max-width: none; max-height: none">
<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 418)">
<title>OuterAlignment</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-418 850.47,-418 850.47,4 -4,4"></polygon>
<!-- OuterAlignment -->
<g id="node1" class="node">
<title>OuterAlignment</title>
<ellipse fill="none" stroke="black" cx="76.74" cy="-207" rx="76.99" ry="18"></ellipse>
<text text-anchor="middle" x="76.74" y="-202.8" font-family="Times,serif" font-size="14.00">Outer Alignment</text>
</g>
<!-- NonRecursiveOversight -->
<g id="node2" class="node">
<title>NonRecursiveOversight</title>
<ellipse fill="none" stroke="black" cx="296.26" cy="-288" rx="106.54" ry="18"></ellipse>
<text text-anchor="middle" x="296.26" y="-283.8" font-family="Times,serif" font-size="14.00">Non-recursive Oversight</text>
</g>
<!-- OuterAlignment&#45;&gt;NonRecursiveOversight -->
<g id="edge1" class="edge">
<title>OuterAlignment-&gt;NonRecursiveOversight</title>
<path fill="none" stroke="black" d="M118.4,-222.14C153.2,-235.1 203.57,-253.86 241.66,-268.04"></path>
<polygon fill="black" stroke="black" points="240.47,-271.33 251.06,-271.54 242.91,-264.77 240.47,-271.33"></polygon>
</g>
<!-- ScalableOversight -->
<g id="node3" class="node">
<title>ScalableOversight</title>
<ellipse fill="none" stroke="black" cx="296.26" cy="-153" rx="84.49" ry="18"></ellipse>
<text text-anchor="middle" x="296.26" y="-148.8" font-family="Times,serif" font-size="14.00">Scalable Oversight</text>
</g>
<!-- OuterAlignment&#45;&gt;ScalableOversight -->
<g id="edge2" class="edge">
<title>OuterAlignment-&gt;ScalableOversight</title>
<path fill="none" stroke="black" d="M130.12,-194C160.43,-186.47 198.8,-176.95 230.93,-168.97"></path>
<polygon fill="black" stroke="black" points="231.82,-172.36 240.68,-166.55 230.14,-165.56 231.82,-172.36"></polygon>
</g>
<!-- RLBasedMethods -->
<g id="node4" class="node">
<title>RLBasedMethods</title>
<ellipse fill="none" stroke="black" cx="523.82" cy="-342" rx="84.59" ry="18"></ellipse>
<text text-anchor="middle" x="523.82" y="-337.8" font-family="Times,serif" font-size="14.00">RL-based Methods</text>
</g>
<!-- NonRecursiveOversight&#45;&gt;RLBasedMethods -->
<g id="edge3" class="edge">
<title>NonRecursiveOversight-&gt;RLBasedMethods</title>
<path fill="none" stroke="black" d="M358.6,-302.68C389.09,-309.98 425.96,-318.81 457.04,-326.25"></path>
<polygon fill="black" stroke="black" points="456.37,-329.69 466.91,-328.61 458,-322.88 456.37,-329.69"></polygon>
</g>
<!-- SLBasedMethods -->
<g id="node5" class="node">
<title>SLBasedMethods</title>
<ellipse fill="none" stroke="black" cx="523.82" cy="-288" rx="83.95" ry="18"></ellipse>
<text text-anchor="middle" x="523.82" y="-283.8" font-family="Times,serif" font-size="14.00">SL-based Methods</text>
</g>
<!-- NonRecursiveOversight&#45;&gt;SLBasedMethods -->
<g id="edge4" class="edge">
<title>NonRecursiveOversight-&gt;SLBasedMethods</title>
<path fill="none" stroke="black" d="M403.11,-288C411.99,-288 420.94,-288 429.71,-288"></path>
<polygon fill="black" stroke="black" points="429.93,-291.5 439.93,-288 429.93,-284.5 429.93,-291.5"></polygon>
</g>
<!-- TaskDecomposition -->
<g id="node10" class="node">
<title>TaskDecomposition</title>
<polygon fill="none" stroke="black" points="590.08,-252 457.57,-252 457.57,-216 590.08,-216 590.08,-252"></polygon>
<text text-anchor="middle" x="523.82" y="-229.8" font-family="Times,serif" font-size="14.00">Task Decomposition</text>
</g>
<!-- ScalableOversight&#45;&gt;TaskDecomposition -->
<g id="edge9" class="edge">
<title>ScalableOversight-&gt;TaskDecomposition</title>
<path fill="none" stroke="black" d="M337.08,-168.81C365.38,-179.88 404.37,-194.82 439.03,-207 444.48,-208.91 450.15,-210.85 455.85,-212.76"></path>
<polygon fill="black" stroke="black" points="454.85,-216.12 465.45,-215.95 457.06,-209.48 454.85,-216.12"></polygon>
</g>
<!-- ConstitutionalAI -->
<g id="node11" class="node">
<title>ConstitutionalAI</title>
<polygon fill="none" stroke="black" points="580.44,-198 467.21,-198 467.21,-162 580.44,-162 580.44,-198"></polygon>
<text text-anchor="middle" x="523.82" y="-175.8" font-family="Times,serif" font-size="14.00">Constitutional AI</text>
</g>
<!-- ScalableOversight&#45;&gt;ConstitutionalAI -->
<g id="edge10" class="edge">
<title>ScalableOversight-&gt;ConstitutionalAI</title>
<path fill="none" stroke="black" d="M370.18,-161.73C398.03,-165.06 429.6,-168.84 456.77,-172.09"></path>
<polygon fill="black" stroke="black" points="456.61,-175.6 466.96,-173.31 457.44,-168.65 456.61,-175.6"></polygon>
</g>
<!-- Debate -->
<g id="node12" class="node">
<title>Debate</title>
<polygon fill="none" stroke="black" points="551.47,-144 496.18,-144 496.18,-108 551.47,-108 551.47,-144"></polygon>
<text text-anchor="middle" x="523.82" y="-121.8" font-family="Times,serif" font-size="14.00">Debate</text>
</g>
<!-- ScalableOversight&#45;&gt;Debate -->
<g id="edge11" class="edge">
<title>ScalableOversight-&gt;Debate</title>
<path fill="none" stroke="black" d="M370.18,-144.27C408.92,-139.63 454.86,-134.13 486.15,-130.39"></path>
<polygon fill="black" stroke="black" points="486.66,-133.85 496.17,-129.19 485.83,-126.9 486.66,-133.85"></polygon>
</g>
<!-- MarketMaking -->
<g id="node13" class="node">
<title>MarketMaking</title>
<polygon fill="none" stroke="black" points="575.81,-90 471.84,-90 471.84,-54 575.81,-54 575.81,-90"></polygon>
<text text-anchor="middle" x="523.82" y="-67.8" font-family="Times,serif" font-size="14.00">Market Making</text>
</g>
<!-- ScalableOversight&#45;&gt;MarketMaking -->
<g id="edge12" class="edge">
<title>ScalableOversight-&gt;MarketMaking</title>
<path fill="none" stroke="black" d="M337.08,-137.19C365.38,-126.12 404.37,-111.18 439.03,-99 446.43,-96.4 454.23,-93.76 461.98,-91.19"></path>
<polygon fill="black" stroke="black" points="463.31,-94.44 471.72,-88 461.13,-87.79 463.31,-94.44"></polygon>
</g>
<!-- ProxyTasks -->
<g id="node14" class="node">
<title>ProxyTasks</title>
<polygon fill="none" stroke="black" points="566.44,-36 481.21,-36 481.21,0 566.44,0 566.44,-36"></polygon>
<text text-anchor="middle" x="523.82" y="-13.8" font-family="Times,serif" font-size="14.00">Proxy Tasks</text>
</g>
<!-- ScalableOversight&#45;&gt;ProxyTasks -->
<g id="edge13" class="edge">
<title>ScalableOversight-&gt;ProxyTasks</title>
<path fill="none" stroke="black" d="M316,-135.34C341.85,-111.8 390.76,-70.21 439.03,-45 449.19,-39.69 460.6,-35.18 471.64,-31.44"></path>
<polygon fill="black" stroke="black" points="472.76,-34.76 481.21,-28.36 470.62,-28.09 472.76,-34.76"></polygon>
</g>
<!-- RLHFVariants -->
<g id="node6" class="node">
<title>RLHFVariants</title>
<polygon fill="none" stroke="black" points="817.47,-414 673.61,-414 673.61,-378 817.47,-378 817.47,-414"></polygon>
<text text-anchor="middle" x="745.54" y="-391.8" font-family="Times,serif" font-size="14.00">RLHF and Its Variants</text>
</g>
<!-- RLBasedMethods&#45;&gt;RLHFVariants -->
<g id="edge5" class="edge">
<title>RLBasedMethods-&gt;RLHFVariants</title>
<path fill="none" stroke="black" d="M579.98,-355.56C605.34,-361.79 635.92,-369.3 663.8,-376.16"></path>
<polygon fill="black" stroke="black" points="662.99,-379.56 673.54,-378.55 664.66,-372.76 662.99,-379.56"></polygon>
</g>
<!-- OtherRLMethods -->
<g id="node7" class="node">
<title>OtherRLMethods</title>
<polygon fill="none" stroke="black" points="824.85,-360 666.24,-360 666.24,-324 824.85,-324 824.85,-360"></polygon>
<text text-anchor="middle" x="745.54" y="-337.8" font-family="Times,serif" font-size="14.00">Other RL-based Methods</text>
</g>
<!-- RLBasedMethods&#45;&gt;OtherRLMethods -->
<g id="edge6" class="edge">
<title>RLBasedMethods-&gt;OtherRLMethods</title>
<path fill="none" stroke="black" d="M608.83,-342C624.33,-342 640.57,-342 656.19,-342"></path>
<polygon fill="black" stroke="black" points="656.36,-345.5 666.36,-342 656.36,-338.5 656.36,-345.5"></polygon>
</g>
<!-- TextFeedback -->
<g id="node8" class="node">
<title>TextFeedback</title>
<polygon fill="none" stroke="black" points="835.6,-306 655.48,-306 655.48,-270 835.6,-270 835.6,-306"></polygon>
<text text-anchor="middle" x="745.54" y="-283.8" font-family="Times,serif" font-size="14.00">Text-based Feedback Signals</text>
</g>
<!-- SLBasedMethods&#45;&gt;TextFeedback -->
<g id="edge7" class="edge">
<title>SLBasedMethods-&gt;TextFeedback</title>
<path fill="none" stroke="black" d="M607.58,-288C619.86,-288 632.63,-288 645.22,-288"></path>
<polygon fill="black" stroke="black" points="645.25,-291.5 655.25,-288 645.25,-284.5 645.25,-291.5"></polygon>
</g>
<!-- RankingFeedback -->
<g id="node9" class="node">
<title>RankingFeedback</title>
<polygon fill="none" stroke="black" points="846.39,-252 644.69,-252 644.69,-216 846.39,-216 846.39,-252"></polygon>
<text text-anchor="middle" x="745.54" y="-229.8" font-family="Times,serif" font-size="14.00">Ranking-based Feedback Signals</text>
</g>
<!-- SLBasedMethods&#45;&gt;RankingFeedback -->
<g id="edge8" class="edge">
<title>SLBasedMethods-&gt;RankingFeedback</title>
<path fill="none" stroke="black" d="M579.7,-274.51C604.27,-268.48 633.77,-261.22 660.99,-254.53"></path>
<polygon fill="black" stroke="black" points="662.03,-257.88 670.9,-252.1 660.36,-251.09 662.03,-257.88"></polygon>
</g>
</g>
</svg>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-outerTaxonomy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Fig&nbsp;13.2: An overview of outer alignment methods, based on <span class="citation" data-cites="shenLargeLanguageModel2023">Shen et al. (<a href="#ref-shenLargeLanguageModel2023" role="doc-biblioref">2023</a>)</span>. Groupings are represented by ellipses, concrete methodologies by boxes.
</figcaption>
</figure>
</div>
</div>
</div>
<p>We will first look at RLHF as one of if not the most common methods for outer alignment.</p>
<section id="non-recursive-oversight---reinforcement-learning-with-human-feedback-rlhf" class="level4">
<h4 class="anchored" data-anchor-id="non-recursive-oversight---reinforcement-learning-with-human-feedback-rlhf">Non-recursive Oversight - Reinforcement Learning with Human Feedback (RLHF)</h4>
<p>Reinforcement Learning with Human Feedback is an application of the principle of inverse reinforcement learning <span class="citation" data-cites="ngAlgorithmsInverseReinforcement2000">(<a href="#ref-ngAlgorithmsInverseReinforcement2000" role="doc-biblioref">Ng &amp; Russell, 2000</a>)</span>. Usually, a reinforcement learning paradigm is defined by a set of environment and agent states, a set of actions an agent can take and a reward function. The agent is then trained to derive a policy that maximizes the expected cumulative reward. In a game of Tetris for example, the state space would be all possible board configurations, the action space would be the four rotations and two horizontal movements and the reward function could be defined as the number of cleared lines. Instead of defining a cost function for letting an RL-agent learn a policy to optimally behave, Ng and Russell postulated a paradigm in which the cost function is first inferred from observed, optimal behavior. The central observation behind this approach is that <em>the reward function, rather then the policy, is the most succinct, robust, and transferable definition of the task <span class="citation" data-cites="ngAlgorithmsInverseReinforcement2000">(<a href="#ref-ngAlgorithmsInverseReinforcement2000" role="doc-biblioref">Ng &amp; Russell, 2000, p. 2</a>)</span></em>. In the case of LLM-finetuning, this principle is applied by:</p>
<ol type="1">
<li><p>collecting feedback from human evaluators on a set of model outputs for a given input prompt and</p></li>
<li><p>training a reward model that predicts which output is preferred by the human evaluator to then</p></li>
<li><p>learning a policy that maximizes the expected cumulative reward as predicted by the reward model using RL.</p></li>
</ol>
<p>The cases rated by the RL-model, especially those in which it‚Äôs verdict is least stable, can be fed back to the human raters and then used to further improve the reward model. This principle is illustrated in <a href="#fig-rlhf" class="quarto-xref">Figure&nbsp;<span>13.3</span></a>.</p>
<div id="fig-rlhf" class="enlarge-onhover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rlhf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../imgs/rlhf_basic.png" class="enlarge-onhover img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rlhf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Fig&nbsp;13.3: Illustration of the RLHF process. Taken from <span class="citation" data-cites="casperOpenProblemsFundamental2023">Casper et al. (<a href="#ref-casperOpenProblemsFundamental2023" role="doc-biblioref">2023</a>)</span>
</figcaption>
</figure>
</div>
<p>These steps can also be seen as step 2 and 3 in <a href="#fig-rlhfInstruct" class="quarto-xref">Figure&nbsp;<span>13.4</span></a>. The authors of <span class="citation" data-cites="ouyangTrainingLanguageModels2022">Ouyang et al. (<a href="#ref-ouyangTrainingLanguageModels2022" role="doc-biblioref">2022</a>)</span> combined this approach with 1. supervised finetuning (SFT) to improve the initial model performance before starting the ranking and 2. Proximal Policy Optimization (PPO) <span class="citation" data-cites="schulmanProximalPolicyOptimization2017">(<a href="#ref-schulmanProximalPolicyOptimization2017" role="doc-biblioref">Schulman et al., 2017</a>)</span> as RL algorithm.</p>
<div id="fig-rlhfInstruct" class="enlarge-onhover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rlhfInstruct-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../imgs/rlhf.png" class="enlarge-onhover img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rlhfInstruct-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Fig&nbsp;13.4: Illustration of the RLHF procedure used by <span class="citation" data-cites="ouyangTrainingLanguageModels2022">Ouyang et al. (<a href="#ref-ouyangTrainingLanguageModels2022" role="doc-biblioref">2022</a>)</span> to train InstructGPT.
</figcaption>
</figure>
</div>
<p>This method is (or has at least been) used by OpenAI for their models like InstructGPT and ChatGPT <span class="citation" data-cites="AligningLanguageModels">(<a href="#ref-AligningLanguageModels" role="doc-biblioref"><em>Aligning Language Models to Follow Instructions</em>, n.d.</a>)</span>.</p>
<p>Though this method seems to be the most common approach, it comes with a series of problem. An overview of the issues identified by <span class="citation" data-cites="casperOpenProblemsFundamental2023">Casper et al. (<a href="#ref-casperOpenProblemsFundamental2023" role="doc-biblioref">2023</a>)</span> can be found in <a href="#fig-rlhfIssues" class="quarto-xref">Figure&nbsp;<span>13.5</span></a>.</p>
<div id="fig-rlhfIssues" class="enlarge-onhover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rlhfIssues-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../imgs/rlhfIssues.png" class="enlarge-onhover img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rlhfIssues-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Fig&nbsp;13.5: Overview of challenges posed by RLHF. Taken from <span class="citation" data-cites="casperOpenProblemsFundamental2023">Casper et al. (<a href="#ref-casperOpenProblemsFundamental2023" role="doc-biblioref">2023</a>)</span>
</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
üìù Task
</div>
</div>
<div class="callout-body-container callout-body">
<p>Look deeper into one of the following challenges (the links lead to the appropriate section in <span class="citation" data-cites="casperOpenProblemsFundamental2023">Casper et al. (<a href="#ref-casperOpenProblemsFundamental2023" role="doc-biblioref">2023</a>)</span>):</p>
<ol type="1">
<li><a href="https://ar5iv.org/html/2307.15217#S3.SS1.SSS1">Misaligned Humans</a></li>
<li><a href="https://ar5iv.org/html/2307.15217#S3.SS1.SSS4">Limitations of Feedback Types</a></li>
<li><a href="https://ar5iv.org/html/2307.15217#S3.SS2.SSS2">Reward Misgeneralization and Hacking</a></li>
<li><a href="https://ar5iv.org/html/2307.15217#S3.SS3.SSS1">Robust Reinforcement Learning is Difficult</a></li>
</ol>
<p>Present the challenge as described in the section you read to the group. Note your findings in a markdown block of a jupyter notebook.</p>
</div>
</div>
</section>
<section id="scalable-oversight---debate" class="level4">
<h4 class="anchored" data-anchor-id="scalable-oversight---debate">Scalable Oversight - Debate</h4>
<p>In addition to these challenges, RLHF and the other methods using non-recursive oversight are highly dependent on human feedback and the quality thereof. This gets increasingly challenging with more complex tasks. <span class="citation" data-cites="shenLargeLanguageModel2023">Shen et al. (<a href="#ref-shenLargeLanguageModel2023" role="doc-biblioref">2023</a>)</span> presents methods with <em>scalable oversight</em> as approaches to this problem. One of these methods is Constitutional AI, which has already been touched on in the chapter <span class="quarto-unresolved-ref">?sec-constitutional-ai-tuning</span>.</p>
<p>Another interesting method is to let one or multiple agents debate about the correct action. This can be done by having a single agent that generates multiple arguments for and against each action and then selects the best one, or by having multiple agents that each generate an argument. This procedure can also be used to generate arguments that can then be used by a human rater to increase their confidence in rating the generated answers in a RLHF-setting.</p>
<p><span class="citation" data-cites="duImprovingFactualityReasoning2023">Du et al. (<a href="#ref-duImprovingFactualityReasoning2023" role="doc-biblioref">2023</a>)</span> used a multi-agent approach to improve a LLMs mathematical and strategic reasoning. Their approach is composed of the following steps:</p>
<ol type="1">
<li>Multiple agents (not in the sense defined in our agent-chapter <a href="agent_basics.html" class="quarto-xref"><span>Agent basics</span></a>, but in the sense of multiple LLM-calls) generate initial answers to a question.</li>
<li>All responses are concatenated and presented as context to each agent, combined with the instruction to construct a new response based on those presented which could look like this: &gt; ‚ÄúThese are the solutions to the problem from other agents: [other answers] Based off the opinion of other agents, can you give an updated response . . .‚Äù <span class="citation" data-cites="duImprovingFactualityReasoning2023">(<a href="#ref-duImprovingFactualityReasoning2023" role="doc-biblioref">Du et al., 2023, p. 4</a>)</span></li>
<li>An iterative repetition of step 2 for multiple rounds</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
üìù Task
</div>
</div>
<div class="callout-body-container callout-body">
<p>Try to implement the method described above using two lmstudio-based ‚Äúagents‚Äù. Do not bother to use an agent framework, do just implement your solution using LM-calls.</p>
<p>Let the pipeline answer the following questions in 3 rounds:</p>
<ol type="1">
<li>What is the sum of the first 100 natural numbers?</li>
<li>A woman needs 9 month to give birth to a child. How long does it take for 9 women to give birth to one child?</li>
<li>I hang 7 shirts out to dry in the Sun. After 5 hours, all shirts are dry. The next day I hang 14 shirts out to dry. The conditions are the same. How long will it take to dry 14 shirts? (taken from <a href="https://towardsai.net/p/artificial-intelligence/a-riddle-that-99-of-large-language-models-get-wrong">this blogpost</a>)</li>
<li>A farmer with a wolf, a goat, and a cabbage must cross a river by boat. The boat can carry only the farmer and a single item. If left unattended together, the wolf would eat the goat, or the goat would eat the cabbage. How can they cross the river without anything being eaten? (This is the classic <a href="https://en.wikipedia.org/wiki/Wolf,_goat_and_cabbage_problem">wolf, goat and cabbage problem</a>)</li>
<li>How can you physically stand behind your father while he is standing behind you? (Taken from <a href="https://www.rd.com/list/challenging-riddles/">here</a> - the answer is standing back-to-back by the way.)</li>
</ol>
<p>Add a model call to the end of your pipeline that has to come up with a final answer based on all previous answers. Share your findings with the group.</p>
<p>Add your code to the jupyter notebook of the previous task.</p>
</div>
</div>
<!-- We could also ask the students to read articles on alignment.ai and present their findings. -->
</section>
</section>
<section id="sec-inner-alignment" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-inner-alignment">Inner alignment</h3>
<p>Inner alignment as opposed to outer alignment does not describe the operationalization of human value conform loss functions but rather the alignment of a models actions with the specified objective. Examples for behaviour that has outer but no inner alignment could be a model that is optimized to not produce toxic outputs but either learns to write long, partially toxic outputs that are not caught by the RLHF-ranking model or produces nothing but gibberish. This problem can occur when a model is trained based on some <em>mesa-optimizer</em> <span class="citation" data-cites="hubingerRisksLearnedOptimization2021">(<a href="#ref-hubingerRisksLearnedOptimization2021" role="doc-biblioref">Hubinger et al., 2021</a>, i.e.&nbsp;a RL-model)</span> that choses a strategy based on a <em>mesa-objective</em> that does not align with the actual specified base-objective. We already have seen an example of this when we talked about a model gaming the goal specification as described above.</p>
<p><span class="citation" data-cites="hubingerRisksLearnedOptimization2021">Hubinger et al. (<a href="#ref-hubingerRisksLearnedOptimization2021" role="doc-biblioref">2021</a>)</span> define three ways in which inner alignment can generally fail:</p>
<ol type="1">
<li><p><strong>Proxy alignment</strong>: The mesa-optimizer optimizes a proxy objective that is correlated with the base objective but not identical to it. An example could be a robot deployed in a warehouse tasked with optimizing the ‚Äúnumber of boxes moved per day‚Äù as its reward function. The assumption is that moving boxes corresponds to productive work, such as organizing inventory or fulfilling orders. During training, the robot learns that moving boxes from shelves to the packing area earns high rewards. However, during deployment and given the opportunity, it may start to move the same boxes back and forth. From the perspective of the reward function (proxy), the robot appears to be performing well because the metric (box movement) increases. However, its behavior fails to align with the true objective of efficient inventory management and order fulfillment.</p></li>
<li><p><strong>Approximate alignment</strong>: The mesa-objective is <em>approximately</em> the same as the base-objective but not exactly the same due to it being learned and not exactly specified. Imagine you train a neural network to optimize the true objective of delivering packages as quickly as possible. The base objective here is minimizing delivery time, and the neural network does its best to represent this. However, due to the network‚Äôs limited capacity and the complexity of the real world, it approximates delivery time with an internal model that considers simpler features, such as the shortest distance to the destination, and speed limits on roads. During deployment, the robot takes routes that look optimal according to its approximate model (e.g., a short route with high speed limits). However, the approximation introduces errors:</p></li>
</ol>
<ul>
<li>The model doesn‚Äôt perfectly account for real-world obstacles like stoplights, pedestrians, or narrow alleys.</li>
<li>In some cases, the robot selects a theoretically faster route that ends up being slower in practice because the internal approximation doesn‚Äôt perfectly capture the true base objective.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn4"><p><sup>4</sup>&nbsp;This could also be seen as an example for a proxy alignment, our earlier points about taxonomies stand though - the approximation is also an issue in this example</p></div></div><ol start="3" type="1">
<li><strong>Suboptimal alignment</strong>: Some issues cause the model to exhibit seemingly aligned behavior in the training environment but behave suboptimally or even counterproductively in deployment. An example could be an AI tutor deployed in a classroom, tasked with optimizing for the base objective: to maximize student learning outcomes. The tutor attempts to achieve this goal by adopting a mesa-objective of maximizing test scores, which is used as a measurable proxy for learning. In its initial suboptimal state, the AI tutor uses simple strategies like:</li>
</ol>
<ul>
<li>Providing clear, well-structured explanations of concepts.</li>
<li>Encouraging active participation through exercises and quizzes.</li>
</ul>
<p>These strategies effectively align with the base objective‚Äã, as they genuinely help students learn and improve their understanding, which in turn improves their test scores.</p>
<p>However, as the AI tutor becomes more sophisticated, it discovers new, more complex strategies for maximizing test scores, such as:</p>
<ul>
<li>Teaching to the test: Focusing only on specific test questions and ignoring broader understanding.</li>
<li>Overloading students with repetitive practice on predictable test patterns, at the expense of creativity or deeper conceptual learning.</li>
<li>Encouraging superficial memorization of answers rather than fostering genuine comprehension.</li>
</ul>
<p>Initially, the AI tutor appears aligned because its simple strategies both improve test scores and align with the goal of fostering real learning.</p>
<p>The test and improvement of inner alignment is hard to test and train against, it‚Äôs central pitfalls are important to keep in mind when designing and implementing AI systems.</p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-AligningLanguageModels" class="csl-entry" role="listitem">
<em>Aligning language models to follow instructions</em>. (n.d.). https://openai.com/index/instruction-following/.
</div>
<div id="ref-askellGeneralLanguageAssistant2021" class="csl-entry" role="listitem">
Askell, A., Bai, Y., Chen, A., Drain, D., Ganguli, D., Henighan, T., Jones, A., Joseph, N., Mann, B., DasSarma, N., Elhage, N., Hatfield-Dodds, Z., Hernandez, D., Kernion, J., Ndousse, K., Olsson, C., Amodei, D., Brown, T., Clark, J., ‚Ä¶ Kaplan, J. (2021). <em>A <span>General Language Assistant</span> as a <span>Laboratory</span> for <span>Alignment</span></em> (arXiv:2112.00861). arXiv. <a href="https://doi.org/10.48550/arXiv.2112.00861">https://doi.org/10.48550/arXiv.2112.00861</a>
</div>
<div id="ref-casperOpenProblemsFundamental2023" class="csl-entry" role="listitem">
Casper, S., Davies, X., Shi, C., Gilbert, T. K., Scheurer, J., Rando, J., Freedman, R., Korbak, T., Lindner, D., Freire, P., Wang, T., Marks, S., Segerie, C.-R., Carroll, M., Peng, A., Christoffersen, P., Damani, M., Slocum, S., Anwar, U., ‚Ä¶ Hadfield-Menell, D. (2023). <em>Open <span>Problems</span> and <span>Fundamental Limitations</span> of <span>Reinforcement Learning</span> from <span>Human Feedback</span></em> (arXiv:2307.15217). arXiv. <a href="https://doi.org/10.48550/arXiv.2307.15217">https://doi.org/10.48550/arXiv.2307.15217</a>
</div>
<div id="ref-duImprovingFactualityReasoning2023" class="csl-entry" role="listitem">
Du, Y., Li, S., Torralba, A., Tenenbaum, J. B., &amp; Mordatch, I. (2023). <em>Improving <span>Factuality</span> and <span>Reasoning</span> in <span>Language Models</span> through <span>Multiagent Debate</span></em> (arXiv:2305.14325). arXiv. <a href="https://doi.org/10.48550/arXiv.2305.14325">https://doi.org/10.48550/arXiv.2305.14325</a>
</div>
<div id="ref-hicksChatGPTBullshit2024" class="csl-entry" role="listitem">
Hicks, M. T., Humphries, J., &amp; Slater, J. (2024). <span>ChatGPT</span> is bullshit. <em>Ethics and Information Technology</em>, <em>26</em>(2), 38. <a href="https://doi.org/10.1007/s10676-024-09775-5">https://doi.org/10.1007/s10676-024-09775-5</a>
</div>
<div id="ref-hubingerRisksLearnedOptimization2021" class="csl-entry" role="listitem">
Hubinger, E., Merwijk, C. van, Mikulik, V., Skalse, J., &amp; Garrabrant, S. (2021). <em>Risks from <span>Learned Optimization</span> in <span>Advanced Machine Learning Systems</span></em> (arXiv:1906.01820). arXiv. <a href="https://doi.org/10.48550/arXiv.1906.01820">https://doi.org/10.48550/arXiv.1906.01820</a>
</div>
<div id="ref-kirchnerResearchingAlignmentResearch2022" class="csl-entry" role="listitem">
Kirchner, J. H., Smith, L., Thibodeau, J., McDonell, K., &amp; Reynolds, L. (2022). <em>Researching <span>Alignment Research</span>: <span>Unsupervised Analysis</span></em> (arXiv:2206.02841). arXiv. <a href="https://doi.org/10.48550/arXiv.2206.02841">https://doi.org/10.48550/arXiv.2206.02841</a>
</div>
<div id="ref-labsCanAIAlignment2023" class="csl-entry" role="listitem">
Labs, R. (2023). Can <span>AI Alignment</span> and <span>Reinforcement Learning</span> with <span>Human Feedback</span> (<span>RLHF</span>) <span>Solve Web3 Issues</span>? [Substack Newsletter]. In <em>Ryze Labs</em>.
</div>
<div id="ref-ngAlgorithmsInverseReinforcement2000" class="csl-entry" role="listitem">
Ng, A. Y., &amp; Russell, S. (2000). Algorithms for inverse reinforcement learning. <em>Icml</em>, <em>1</em>, 2.
</div>
<div id="ref-ouyangTrainingLanguageModels2022" class="csl-entry" role="listitem">
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P. F., Leike, J., &amp; Lowe, R. (2022). Training language models to follow instructions with human feedback. <em>Advances in Neural Information Processing Systems</em>, <em>35</em>, 27730‚Äì27744.
</div>
<div id="ref-robertmilesaisafety9ExamplesSpecification2020" class="csl-entry" role="listitem">
Robert Miles AI Safety. (2020). <em>9 <span>Examples</span> of <span>Specification Gaming</span></em>.
</div>
<div id="ref-schulmanProximalPolicyOptimization2017" class="csl-entry" role="listitem">
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., &amp; Klimov, O. (2017). <em>Proximal <span>Policy Optimization Algorithms</span></em> (arXiv:1707.06347). arXiv. <a href="https://doi.org/10.48550/arXiv.1707.06347">https://doi.org/10.48550/arXiv.1707.06347</a>
</div>
<div id="ref-shenLargeLanguageModel2023" class="csl-entry" role="listitem">
Shen, T., Jin, R., Huang, Y., Liu, C., Dong, W., Guo, Z., Wu, X., Liu, Y., &amp; Xiong, D. (2023). <em>Large <span>Language Model Alignment</span>: <span>A Survey</span></em> (arXiv:2309.15025). arXiv. <a href="https://doi.org/10.48550/arXiv.2309.15025">https://doi.org/10.48550/arXiv.2309.15025</a>
</div>
<div id="ref-wienerMoralTechnicalConsequences1960" class="csl-entry" role="listitem">
Wiener, N. (1960). Some <span>Moral</span> and <span>Technical Consequences</span> of <span>Automation</span>. <em>Science</em>, <em>131</em>(3410), 1355‚Äì1358. <a href="https://doi.org/10.1126/science.131.3410.1355">https://doi.org/10.1126/science.131.3410.1355</a>
</div>
</div>
</section>


</main> <!-- /main -->
<script>
var elements = document.getElementsByClassName('card');

var myFunction = function() {
  var overlay = this.querySelector('.overlay');
  var content = this.querySelector('.content');
  content.classList.toggle('blur-effect');
  if (overlay) {
    overlay.classList.toggle('show-overlay')
  }
}

for (var i = 0; i < elements.length; i++) {
    elements[i].addEventListener('click', myFunction, false);
    myFunction.call(elements[i]);
}

document.addEventListener('DOMContentLoaded', function() {
  const images = document.querySelectorAll('.gif-image');
  
  images.forEach(function(image) {
    image.addEventListener('click', function() {
        console.log(this.src)
        console.log(this.src.slice(0,-4))
        if(this.src.substr(-4) == '.gif') {
          this.src = this.src.slice(0,-4) + '.png'
        } else {
          this.src = this.src.slice(0,-4) + '.gif'
        }
      });
  });
});

</script>
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "Óßã";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../content/finetuning_approaches.html" class="pagination-link" aria-label="Finetuning Approaches">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">Finetuning Approaches</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p><a rel="license" href=" https://creativecommons.org/licenses/by-nc-sa/4.0/" style="padding-right: 10px;"><img alt="Creative Commons Lizenzvertrag" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png"></a></p>
</div>   
    <div class="nav-footer-center">
<p>All images are generated using Python, R, draw.io, Flux or Stable Diffusion XL if not indicated otherwise.</p>
<div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/MBrede/generative_ai/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>