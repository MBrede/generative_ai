---
toc-title: '![](../imgs/prompting.jpg){width=240px}<br> <h3>Prompting</h3>'
---

# Prompting

Prompting describes the utilization of the ability of language models to use zero or few-shot instrutions to perform a task. This ability, which we briefly touched on when we were discussing the history of language models (i.e., the paper by @radfordLanguageModelsAre2019), is one of the most important aspects of modern large language models.

Prompting can be used for various tasks such as text generation, summarization, question answering, and many more. 

## Instruct-tuned models

Instruct-tuned models are trained on a dataset (for an example, see @fig-instructData) that consists of instructions and their corresponding outputs. This is different from the pretraining phase of language models where they were trained on large amounts of text data without any specific task in mind. The goal of instruct-tuning is to make the model better at following instructions and generating more accurate and relevant outputs.

:::{#fig-instructData}
![](../imgs/instruct_dataset.png)

An example for a dataset that can be used for instruct-finetuning. This dataset can be found on [huggingface](https://huggingface.co/datasets/rajpurkar/squad)
:::


::: {.callout-note}
## üìù Task
Test the difference between instruct and non-instruct-models.

Do this by trying to get a gpt2-version (i.e., ["QuantFactory/gpt2-xl-GGUF"](https://model.lmstudio.ai/download/QuantFactory/gpt2-xl-GGUF)) and a small Llama 3.2 Instruct-Model (i.e., ["hugging-quants/Llama-3.2-1B-Instruct-Q8_0-GGUF"](https://model.lmstudio.ai/download/hugging-quants/Llama-3.2-1B-Instruct-Q8_0-GGUF) to write a small poem about the inception of the field of language modelling.

Use LM-Studio to test this.
:::


:::{class="card"}
:::{class="content"}

::: {#fig-poems layout-ncol=2}

![A poem written by Llama 3.2 1B - a model with Instruct-Finetuning](../imgs/llama1bpoem.png){#fig-llamapoem}

![A "poem" written by GPT2 - a model without Instruct-Finetuning](../imgs/gpt2poem.png){#fig-gpt2poem}

A poem and a "poem"
:::

:::
:::{class="overlay"}
Show answer
:::
:::


## Prompting strategies

The results of a prompted call to a LM is highly dependent on the exact wording of the prompt. This is especially true for more complex tasks, where the model needs to perform multiple steps in order to solve the task. It is not for naught that the field of "prompt engineering" has emerged.
There is a veritable plethora of resources available online that discuss different strategies for prompting LMs. It has to be said though, that the strategies that work and don't work can vary greatly between models and tasks.
A bit of general advice that holds true for nearly all models though, is to 

a) define the task in as many small steps as possible
b) to be as literal and descriptive as possible and 
c) to provide examples if possible.

Since the quality of results is so highly dependent on the chosen model, it is good practice to test candidate strategies against each other and therefore to define a target on which the quality of results can be evaluated.
One example for such a target could be a benchmark dataset that contains multiple examples of the task at hand.

<!--
Too many labels. Reduce the label count for the task -->
::: {.callout-note}
## üìù Task 

**1.** Test the above-mentioned prompting strategies on the [MTOP Intent Dataset](https://huggingface.co/datasets/mteb/mtop_intent/viewer/de) and evaluate the results against each other. The dataset contains instructions and labels indicating on which task the instruction was intended to prompt. Use a python script to call one of the following three models in LM-Studio for this:

1. [Phi 3.1 mini](https://model.lmstudio.ai/download/lmstudio-community/Phi-3.1-mini-128k-instruct-GGUF)
2. [Gemma 2 2B](https://model.lmstudio.ai/download/lmstudio-community/gemma-2-2b-it-GGUF)
3. [Llama 3.2 1B](https://model.lmstudio.ai/download/hugging-quants/Llama-3.2-1B-Instruct-Q8_0-GGUF)

Use the [F1-score](https://en.wikipedia.org/wiki/F-score) implemented in [scikit learn](https://scikit-learn.org/1.5/modules/generated/sklearn.metrics.f1_score.html) to evaluate your results.

**2.** You do sometimes read very specific tips on how to improve your results. Here are three, that you can find from time to time:

* Do promise rewards (i.e., monetary tips) instead of threatening punishments
* Do formulate using affirmation ("*Do the task*") instead of negating behaviours to be avoided ("*Don't do this mistake*")
* Let the model reason about the problem before giving an answer
    
Check these strategies on whether they improve your results. If your first instruction already results in near-perfect classification, brainstorm a difficult task that you can validate qualitatively. Let the model write a recipe or describe Kiel for example.

**3.** Present your results

**3.** Upload your code to moodle
:::


## Generation of synthetic texts

As we discussed before, small models can perform on an acceptable level, if they are finetuned appropriately.

A good way to do this is to use a larger model to generate synthetic data that you then use for training the smaller model. This approach has been used successfully in many applications, for example for improving graph-database queries [@zhongSyntheT2CGeneratingSynthetic2024], for improving dataset search [@silvaImprovingDenseRetrieval2024] or the generation of spreadsheet-formulas [@singhEmpiricalStudyValidating2024].

Since even the largest LLMs are not perfect in general and might be even worse on some specific niche tasks, evidence suggests that a validation strategy for data generated in this way is beneficial [@singhEmpiricalStudyValidating2024; @kumarSelectiveFinetuningLLMlabeled2024].

Strategies to validate the synthetic data include:

* Using a human annotator to label part of the data to test the models output
* Forcing the model to answer in a structured way that is automatically testable (e.g., by using JSON)
* Forcing the model to return 2 or more answers and checking for consistency
* Combining the two approaches above (i.e., forcing the model to return multiple structured outputs (JSON, XML, YAML, ...) and checking for consistency)
* Using a second LLM/different prompt to rate the answers

::: {.callout-note}
## üìù Task 

Using your script for batch-testing different prompts, generate synthetic data for a emotion detection task based on [Paul Ekman's](https://en.wikipedia.org/wiki/Paul_Ekman) six basic emotions: anger, disgust, fear, happiness, sadness and surprise^[Though this nomenclature has fallen a bit out of fashion].

The generated data should consist of a sentence and the emotion that is expressed in it.
Start by generating two examples for each emotion. Validate these results and adapt them if necessary.
Then use these examples to generate 100 samples for each emotion.

Use one of the above mentioned (non-manual) strategies to validate the data you generated.

Upload your results to Moodle.
:::


## Further Readings

* [This prompting-guide](https://www.promptingguide.ai/) has some nice general advice
* [OpenAI](https://platform.openai.com/docs/guides/prompt-engineering) has its own set of tipps
* [deepset](https://docs.cloud.deepset.ai/docs/prompt-engineering-guidelines), the company behind Haystack, has a nice guide as well
* [This blog-article](https://heidloff.net/article/fine-tune-small-llm-with-big-llm/), again written by Heidloff [@heidloffFinetuningSmallLLMs2023]