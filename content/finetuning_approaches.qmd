---
toc-title: '![](../imgs/cover.jpg){width=240px}<br> <h3>Finetuning Approaches</h3>'
---

# Finetuning Approaches

*Finetuning* in terms of generative models means the general concept taking a pre-trained, "foundational" model and updating its parameters using new data.
This data is usually much smaller than the data used to train the original model.
The goal is to adapt the model to the new data while preserving as much of the knowledge it has already learned from the original training data.
We have already seen an example of a finetuning approach when we were talking about instruct-tuned models @sec-instruct.
These models are based on plain MLM-trained language models, that are then trained on new data that is presented in a Instruct - Response format.
The result of this specific example of finetuning was a model that, instead of just completing a text, answered in the format present in the finetuning data.

Though the central concept of finetuning is always the same, i.e., updating the parameters of a pre-trained model using new data, there are many different ways to do this.
The following sections will give an overview of some of the most common approaches.

## Full Finetuning

Full finetuning is the simplest approach to finetuning. As the name says, it is based on completely updating the parameters of the pre-trained model using new data.
This means that all weights of the model are updated during training using regular gradient descent or a variant thereof.
The main advantage of this approach is that it is very simple and easy to implement. Complete (few-shot) fine-tuning has also shown to perform better in the domain of finetuning and in Out-of-domain tasks when compared to Few-Shot-Prompt-approaches @mosbachFewshotFinetuningVs2023.
However, it also has some disadvantages.
Firstly, it can be computationally expensive as it requires training all parameters of the model. @tip-fineTuningResources illustrates the relationship between necessary resources for finetuning compared to inference.

:::{#tip-fineTuningResources .callout-tip collapse="true" appearance="minimal"}

{{< include resources_full_finetuning.md >}}

:::

Secondly, it can lead to catastrophic forgetting, i.e., the model forgets what it has learned during pre-training when adapting to new data [@luoEmpiricalStudyCatastrophic2024].

## Parameter-Efficient Finetuning (PEFT)

Another approach to finetuning is to not update all a models parameters but to (partially) freeze them and only update a small subset of the parameters or to train an adaptor module that can be added to the model.
This approach is called parameter-efficient fine-tuning (PEFT).
The main advantage of PEFT is that it is much more computationally efficient than full finetuning as it only requires updating a small subset of the parameters.
We will look at three different approaches to PEFT:

<!-- 1. Prompt-based Finetuning (Prefix-tuning, Prompt tuning and Multitask Prompt Tuning) -->

1. Prompt-based Finetuning (Prefix-tuning and Prompt tuning)

2. Adapter-based finetuning (Low-Rank Adaptation and its relatives)

3. IA3 (Infused Adapter by Inhibiting and Amplifying Inner Activations)

### Prompt-based Finetuning

Prompt-based finetuning is a family of methods that use so called "soft-prompts" to guide a models generation. The general concept is pretty close to prompting as we discussed it in @sec-prompting. The main difference is that instead of engineering a prompt constructed from discrete tokens that results in opportune results, we let standard optimization procedures find a continuos embedding-vector in a pre-trained LMs embedding-space.
Prefix-Tuning, Prompt Tuning and P-tuning are three different approaches to prompt-based finetuning - all utilizing some implementation of this soft-prompt concept.

#### Prefix tuning

Prefix-Tuning [@liPrefixTuningOptimizingContinuous2021] is a method of adapting a language model to a specific down-stream task by adding a continuous prefix vector to the input embeddings. This is done by learning a continuos matrix with a set amount of columns (i.e., tokens) and the frozen models embeddings-dimensionality^[Since directly learning the prefix-weights proved to result in unstable performance, the authors did not directly train prefix-vectors but a MLP scaling up from a smaller 
dimensionality to the embedding size. Since the rest of the proxy-model is discarded after training though, the method can be treated as the same principle.] that is prepended to the input of each transformer layer (i.e., the encoder and the decoder-stack). The principle is illustrated in @fig-prefixTuning.

![Illustration of Prefix-tuning. A continuous prefix vector is learned and concatenated to the input embeddings before they are fed into the transformer layers. From @liPrefixTuningOptimizingContinuous2021](../imgs/prefix_tuning.png){#fig-prefixTuning .enlarge-onhover}

This vector can then be used to guide the model during inference.
The main advantages of this method are 

a) a small number of parameters that need to be learned and
b) the ability to quickly adapt to different tasks by simply switching out the prefix vector.

Since the learned prefix-weights have to be prepended to each input though, one has to have access to the models internal representation during inference (at least for encoder-decoder-stacks). This is not always possible, especially when using black-box models like large language models that are hosted on a remote server.

#### Prompt-Tuning

Prompt-tuning [@lesterPowerScaleParameterEfficient2021] is a method that is conceptually very similar to prefix-tuning, but avoids the need for accessing the internal representation of the model during inference by using what the authors call "soft prompts". 
Again, instead of prompting using discrete tokens, continuous "special tokens" are learned that are concatenated to the input embeddings. 
The main contribution of Prompt-Tuning over Prefix-Tuning is a) that they showed that inputting the soft-prompts to the encoder alone suffices and more importantly b) that the performance of models fine-tuned in this manner is comparable to full finetuning, at least for larger LLMs (@fig-promptTuning).

![Results of Prompt-tuning compared to prompt-engineering and complete finetuning, taken from @lesterPowerScaleParameterEfficient2021](../imgs/prompt_tuning.png){#fig-promptTuning .enlarge-onhover}

<!-- ### Multitask prompt tuning
 -->

### Adapter-based finetuning

Instead of focusing on the embeddings and thus the input of the language models, LoRA and its relatives focus on adapting the output of the attention and feed-forward layers of a transformer.
The family of Low-Rank Adaptation (LoRA) methods [@huLoRALowRankAdaptation2021] we will discuss here is a group of parameter-efficient fine-tuning techniques that adapt the models output by injecting trainable rank decomposition matrices into a transformers layer, greatly reducing the amount of parameters that need to be learned.

#### LoRA (Low-Rank Adaptation)


#### QLoRA (Quantized Low-Rank Adaptation)


#### X-LoRA (Mixture of Experts with LoRA)


### Unsloth

Unsloth [@unsloth] is a python-module that implements LoRA-finetuning in a very efficient way that further reduces raining resource requirements.
This is mostly done by a far more efficient Gradient Descent algorithm that is specifically optimized for LoRA finetuning [@IntroducingUnsloth].

They additionally introduced dynamic quantization to their models, which allows them to further reduce the memory footprint without losing too much performance.

## LLM-Alignment



## Further Readings

