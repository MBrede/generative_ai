---
toc-title: '![](../imgs/llm-pipeline.jpg){width=240px}<br> <h3>LLM pipelines</h3>'
---

# LLM pipelines {#sec-evalpipelines}

<!-- in @sec-agents we bla  -->

## Motivation and learning objectives

In session 5 we focused on __agentic decision-making__ and the EDA agent as a motivating example. In this session, we will:

- distinguish more clearly between __agents__ and __LLM pipelines__,
- understand the concept of __LLM as a judge__ and why evaluation is a bottleneck in generative AI systems,
- design and discuss __evaluation pipelines__ for our EDA agent and related use cases, and
- connect these ideas to your own project work.

After this session you should be able to:

- design a simple pipeline of LLM calls and tools,
- use an LLM to evaluate model outputs along explicit criteria, and
- reflect critically on the strengths and weaknesses of LLM-based evaluation.


## Pipelines: from input to output

As discussed, an agent decides dynamically which tools to use. A pipeline, in contrast, follows a pre-defined sequence of steps:

1. Receive input.
2. Transform it through a series of modules (some LLM calls, some classic code).
3. Produce an output ‚Äì optionally with internal loops, but without LLM-driven control flow.

Pipelines are attractive because they are:

- easier to __test and debug__,
- often more __robust__,
- and usually more __efficient__ than agentic setups when the workflow is well understood.

### Example: simple EDA pipeline (no agent)

A non-agentic pipeline for our EDA use case could be:

1. __Preprocessing__: load the CSV, detect column types, basic validation.
2. __Analytics__: compute standard statistics and sample plots using Python code.
3. __Report generation__: call an LLM once to turn the statistics and plots into a narrative EDA report.

No tool selection is delegated to the LLM; it only writes the report. This is already powerful ‚Äì but the __quality__ of the report still varies, and we need a way to __evaluate__ it.


## LLM as a judge {#sec-llmJudge}

The core idea of LLM as a judge is simple:

__Use a (possibly different) LLM to evaluate the quality of outputs produced by an LLM-based system.__

__Why is this useful?__

Consider a small experiment:

::: {.callout-note}
## üìù Task

1. Open a notebook and connect it with a local LLM using LM Studio (or other).
2. Ask it to generate __one__ story containing fried eggs on a sunrise. 
3. Informally evaluate the story: What works well? What doesn‚Äôt? How did you decid
4. Now ask the model to generate __50‚Äì100__ such stories (use a higher temperature).
5. Try to evaluate all of them manually...

Reflect: How long does it take? How consistent are your own judgements?

:::

Typical pattern:

1. We generate large amounts of text, code, or structured output.
2. We suspect that quality is variable.
3. We need a way to __assess__ that quality.
4. Reading and judging everything manually is too slow and doesn‚Äôt scale. (We don't have time for this.)

Instead, we can use an LLM in an evaluation role:

- It receives the output to be evaluated (e.g. an EDA report).
- It receives __criteria__ (e.g. correctness, completeness, clarity).
- It returns scores, labels, or structured feedback.

The evaluation can then be used to:

- accept or reject outputs,
- select the best out of several candidates,
- give automated feedback for improvement,
- or serve as a signal for fine-tuning or reinforcement learning (e.g. Constitutional AI [@ConstitutionalAIHarmlessness]).

This approach is called LLM as a judge.

### Benefits and drawbacks

__Benefits:__

- Scales to large numbers of outputs.
- Can approximate nuanced, qualitative judgements when well-prompted.
- Easy to combine with existing systems (few additional components).
- Flexible: different prompts and criteria for different tasks.

__Drawbacks:__

- __Cost:__ multiple LLM calls per sample.
- __Bias:__ models might prefer their own style or longer answers.
- __Subjectivity:__ evaluation depends on the prompt and the judge model.
- __Mismatch with humans:__ high scores do not always mean humans agree.

Because of these limitations, LLM-based judgements should be:

- designed carefully (clear rubrics),
- and, ideally, __calibrated__ against human ratings.


## Evaluation pipelines: general pattern

Many applications follow a similar structure:

1. __Generator:__ produces a candidate output (text, code, EDA report, answer, ‚Ä¶).
2. __Judge:__ evaluates the output (via an LLM with evaluation prompt).
3. __Editor (optional):__ improves the output based on the feedback.
4. __Loop (optional):__ repeat 2 and 3 until a stopping criterion is met.

This pattern applies to:

- EDA reports,
- answers in Q&A systems,
- code generation,
- many more


## Example: Dataset hypothesis creation pipeline

We will now modify our EDA agent/pipeline from earlier to illustrate this. We will build a pipeline that will generate a __hypothesis__ based on superficial knowledge of the dataset. 

::: {.callout-note}
## ‚ÑπÔ∏è Info 
A __hypothesis__ is a testable prediction or proposed explanation for a phenomenon, based on limited evidence that serves as a starting point for further investigation. It typically takes the form of an "if-then" statement that can be supported or refuted through experimentation and observation. A good scientific hypothesis must be __falsifiable__ ‚Äî meaning there must be a possible way to prove it wrong through empirical evidence.
:::

For now, we will not try to falsify (or verify) the hypothesis, just checking, if the generated output makes a good hypothesis. Our workflow looks thusly:

1. __Hypothesis generator__
    - Receives the dataset and additional metadata about the data
    - Produces a hypothesis
2. __Judge-LLM__
    - Receives the hypothesis + a rubric, e.g. with criteria:
        - Is this a prediction?
        - Does it explain something?
        - Is it falsifiable?
    - Returns:
        - scores (e.g. 1‚Äì5) per criterion, and
        - short textual feedback for each.
3. __Optional editor step__
    - Another LLM call (possibly with a different model) revises the hypothesis
4. __Stopping condition__
    - For example: stop after 2‚Äì3 improvement iterations or if all scores exceed a threshold.


### Generator

Let's have a closer look at the generator:

```{mermaid}

flowchart LR
data["`dataset,
metadata,
columns,
...`"]
gen(Generator)
hyp[Hypothesis]
data --> gen
gen --> hyp
```


A system prompt for the generator could be:

    You are a research asssistant tasked with generating a first hypothesis on a given dataset. 
    You will receive a brief summary of the dataset in question. this might include metadata, example rows, a list of columns etc. 
    Your Task is to generate a hypothesis based on this superficial knowledge, that will be testet at a later stage. 

When calling the generator, you should give the relevant information in the user prompt:

    Metadata:
    {dataset_metadata}

    columns: 
    {dataset.columns}

    example rows:
    {dataset.head()}



::: {.callout-note}
## üìù Task

Your turn!

In your notebook, implement a hypothesis generator. 

1. Get a dataset. You can use the [the titanic dataset from kaggle](https://www.kaggle.com/competitions/titanic/data) again or something else. 
2. Compile some relevant information e.g. ``df.head()``, ``df.describe()`` etc. along other information. The titanic dataset has a very good description at the website. 
3. Send a good system prompt and the relevant information to an LLM. You may want to increase the model's temperature. 
4. Evaluate the results.
:::


### Reviewer (LLM as a judge)


```{mermaid}

flowchart LR

hyp[Hypothesis]
rev(Reviewer)
feedback[Feedback]
hyp --> rev
rev --> feedback
```

The reviewer is a judge-LLM with a rubric for a good hypothesis:

    You are a research assistant. Your task is to review a hypothesis based on these rules:
    1. Is this a prediction?
    2. Does it explain something?
    3. Is it falsifiable?
    Provide concise feedback on how to improve the hypothesis.

Here, LLM-as-judge is used to evaluate and guide improvements.


::: {.callout-note}
## üìù Task

Let's build us a very judgemental robot!

1. In the same notebook, initialize a reviewer as well. 
2. Let the reviewer review the hypothesis generated by the generator. 
3. Adjust the rubric until the reviewer finds stuff to improve. 
:::

### Editor

```{mermaid}

flowchart LR
data["`dataset,
metadata,
columns,
...`"]

hyp[Hypothesis]
feedback[Feedback]
editor(Editor)
new_hyp[Hypothesis]
data --> editor
hyp --> editor
feedback --> editor
editor --> new_hyp
```

The editor uses the original input + hypothesis + feedback to produce a revised hypothesis. Its prompt encodes this behaviour explicitly (incorporate feedback, keep focus, improve wording, etc.).

::: {.callout-note}
## üìù Task

Time to improve!

1. In the same notebook, implement the editor as well. Make a new LLM call/prompt or adapt your original generator. 
2. Let the editor generate a new hypothesis or improve an existing one based on the feedback from the reviewer.
3. Get the editor to actually generate something that is different from the generators version!
:::


### Full pipeline

We now basically have a working LLM-based pipeline. 

```{mermaid}

flowchart LR
data["`dataset,
metadata,
columns,
...`"]
gen(Generator)
hyp[Hypothesis]
rev(Reviewer)
feedback[Feedback]
editor(Editor)
new_hyp[Hypothesis]
data --> gen
gen --> hyp
hyp --> rev
rev --> feedback
data --> editor
hyp --> editor
feedback --> editor
editor --> new_hyp
```


A good next step would be to add a loop around the review process, so that it only stops if the reviewer is happy. This could look like this:

```python
hypothesis = generator.generate(dataset_description)

for _ in range(max_iter):
    review = reviewer.review(hypothesis)
    if review_is_positive_enough(review):
        break
    hypothesis = editor.edit(dataset_description, hypothesis, review)

return hypothesis
```

```{mermaid}

flowchart LR
data["`dataset,
metadata,
columns,
...`"]
gen(Generator)
hyp[Hypothesis]
rev(Reviewer)
feedback[Feedback]
editor(Editor)
if{good enough?}
stop((stop))
data --> gen
gen --> hyp
hyp --> rev
rev --> feedback
data --> editor
hyp --> editor
feedback --> if
if -- no --> editor
if -- yes --> stop
editor --> hyp
```


The missing piece is review_is_positive_enough, where you might:

- search for a specific ‚ÄúOK‚Äù marker in the review,
- ask the reviewer to output a score and threshold it,
- or run an additional small LLM classification step.

Alternatively, you can just generate a lot of hypotheses and choose the best one (or the best _n_). 

Exactly the same pattern can be transferred to __your own project outputs__.

::: {.callout-note}
## üìù Task

Build it!

1. Think of a way to stop the review process once the hypothesis is good enough. 
2. Implement the loop. 

This is the minimum goal for today, so when you are done, you can upload your notebook to moodle. Alternatively, go on and implement the agent (see below) as well!

:::

### From pipeline to orchestrated agents

The example above is still essentially a pipeline: the order of steps is fixed (Generator ‚Üí Reviewer ‚Üí Editor ‚Ä¶). We could, however, introduce an orchestrator agent that decides which component to call next:

    You are tasked with creating high-quality scientific hypothesis for a given dataset. Use the tools at your disposal to ensure good quality. 
    Available tools:
    ‚Äì Generator ‚Äì creates hypotheses
    ‚Äì Reviewer ‚Äì evaluates hypothesis quality
    ‚Äì Editor ‚Äì improves hypothesis based on feedback
    Decision guidelines:
    ‚Äì Use Generator to create hypothesis.
    ‚Äì Use Reviewer to generate feedback.
    ‚Äì Use Editor to improve hypothesis.
    ‚Äì Choose END when the hypothesis is good enough.
    Output only the next agent to run ("Generator", "Reviewer", "Editor", or "END").

The agent is now just a loop around this orchestrator: 

```python
# pseudocode
initialize: 
    tools
    history

while true:
    response = orchestrator(history)
    if response == "END"
    break
    if response == "generator" 
        history += generator.generate()
    # same for other tools

```

![Image credit https://x.com/adamdotdev (from [the Tiny Agents blogpost](https://huggingface.co/blog/tiny-agents))](../imgs/agent_loop_tinyagents.jpg)

This is essentially the agentic version of the same pipeline. The pros and cons are analogous to those discussed in session @sec-agents:

- more flexibility,
- but also more complexity and less predictability.

For your projects, you should decide consciously whether you need this flexibility ‚Äì or whether a simple pipeline with a clear evaluation step is sufficient.


::: {.callout-note}
## üìù Task


1. In the same notebook, initialize the orchestrator agent as well.
2. Implement the workflow shown above in real code. 
3. Watch happily as it all works without any issues whatsoever. 
4. Upload to Moodle. 
:::


## Project work

Time to work on your projects!

::: {.callout-note}
## üìù Task

1. Discuss your project. 
2. Set up the repository
3. Plan the project
4. Start collecting data or finding a data set
5. Start implementing

Happy coding!


:::



## Further Readings

- [the Tiny Agents blogpost](https://huggingface.co/blog/tiny-agents) really helps understanding agents. 
- [Here](https://www.youtube.com/watch?v=ewLMYLCWvcI) is a video describing other multi-agent systems, including an [agent hospital](https://arxiv.org/abs/2405.02957) and a [multi-agent translator](https://arxiv.org/html/2405.11804v1)


## References