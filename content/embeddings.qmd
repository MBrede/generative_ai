---
toc-title: '![](../imgs/rag.webp){width=240px}<br> <h3>Embedding-based agent-systems</h3>'
---

# Embedding-based agent-systems

All agents we discussed until here are using tools that allow them to use their generated inputs in some way.
In most of the task we want to utilize agents, we do not only want to generate text but to also inform the generation based on some kind of existing knowledge base.
Examples for these kinds of usecases include:

- Answering questions about a specific topic (e.g., a company or product)
- Summarizing a document
- Generating a report based on data

Though most modern LLMs are increasingly capable in answering basic knowledge-questions, the more comples a topic or the more relevant the factual basis of an answer is, the more it is important to base generated answers on actual data.

## Semantic embeddings and vector stores

To empower an agent too look up information during its thought-process, one has to build a tool that allows an agent to use natural language to retrieve information necessary for a task.
The fundamental principle to do this are so-called *semantic embeddings*.
These are pretty close to the concept we introduced when talking about the foundations of LLMs (see [here](getting_started_with_llms)) and can be understood as a way to map textual data into a vector space.
The main idea is that semantically similar texts should have similar embeddings, i.e., they are close in the vector space. Close in this context is meant as having a reasonibly small distance between them. The go-to standard to measure this distance is the **cosine similarity**, which has proven usefull enough to be the standard for a range of semantic retrieval implementations (i.e., they are used in [OpenAI tutorials](https://cookbook.openai.com/examples/recommendation_using_embeddings) and in [Azure embedding-applications](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/understand-embeddings)). The cosine similarity is defined as:

$$
\text{cosine\_similarity}(u, v) = \frac{u \cdot v}{\|u\| \|v\|} = \frac{\sum_{i=1}^{n} u_i v_i}{\sqrt{\sum_{i=1}^{n} u_i^2} \sqrt{\sum_{i=1}^{n} v_i^2}}
$$
The rationale here is that sequences with semantically similar contents should point to similar directions in the high dimensional vector space. See @fig-cosineSimilarity for an illustration of this and other common similarity concepts seen in semantic retrieval.


```{r}
#| label: fig-cosineSimilarity
#| fig-cap: Illustration of common similarity metrics in semantic search.
#| fig-subcap: 
#|    - Illustration of "semantic embeddings" of different word.
#|    - "Illustration of 4 common similarity concepts seen in semantic retrieval: cosine, euclidean, dot product and manhattan. dot product and cosine are taking the direction of the vector into account, while the cosine ignores the length of the vectors and the dot product does not. Manhattan and euclidean are both measuring the distance between two points in a vector space, but they do it differently. Euclidean is the straight line between two points, while manhattan is the sum of the absolute differences between the coordinates of the two points."
#| fig-alt: 
#| - Illustration of "semantic embeddings" of different word.
#|  The words "good", "nice" and "orange" are all mapped to a vector space.
#| - "Illustration of similarities between these words. 4 common similarity 
#| concepts seen in semantic retrieval: cosine, euclidean, dot product and manhattan."
#| echo: false
#| message: false
#| warning: false
#| fig-width: 8
#| fig-align: center
#| classes: .enlarge-onhover

library(tidyverse)

mat2df <- function(mat){
    tibble(row = rep(seq_len(nrow(mat)), times = ncol(mat)),
           col = rep(seq_len(ncol(mat)), each = nrow(mat)),
           value = as.vector(mat))
}

vectors  <- tibble(vector = letters[1:5],
                   words = c('good', 'nice', 'orange', 'bad', 'blue'),
                  x_end = c(.5, .23, -0.7,-0.25,.5),
                  y_end = c(.5, .26, .5, -0.24, -0.5),
                  y = 0,
                  x = 0)

Matrix <- as.matrix(vectors[3:4])
sim <- Matrix / sqrt(rowSums(Matrix * Matrix))
co_sim <- mat2df(sim %*% t(sim))

euclidean_sim <- mat2df(as.matrix(1 / (1 + dist(Matrix, method = 'euclidean'))))

manhattan_sim <- mat2df(as.matrix(1 / (1 + dist(Matrix, method = 'manhattan'))))

dot_product_sim <- mat2df(Matrix %*% t(Matrix))

similarities <- bind_rows(
    co_sim,
    euclidean_sim,
    manhattan_sim,
    dot_product_sim) %>%
    mutate(method = rep(c('cosine', 'euclidean', 'manhattan', 'dot product'),
           each = nrow(co_sim)),
           col = vectors$words[col],
           row = vectors$words[row])

vectors %>%
    ggplot(aes(color = words)) +
        geom_segment(aes(x=x,y=y,xend=x_end, yend = y_end), arrow = arrow(length = unit(.4, 'cm')), linewidth = 1) +
        scale_color_manual(values = c('good'='darkgreen', 'nice' = 'green', 'orange' = 'orange', 'bad' = 'red', 'blue' = 'darkblue'))+
        theme_minimal()

similarities %>% 
mutate(row = factor(row),
       col = factor(col, levels = rev(levels(row))))  %>% 
group_by(method)  %>% 
mutate(value = (value - min(value))/(max(value)-min(value))) %>%
    ggplot(aes(x = col, y = row)) +
    geom_tile(aes(fill = value)) +
    facet_wrap(~method) +
    labs(x = '', y = '') +
    theme_minimal() +
    theme(legend.position = 'none') +
    scale_fill_distiller(palette = "Spectral", direction=-1)

```

As always, there is not the one solution to all problems though and the applicability of cosine similarity might not be optimal for your usecase [@steckCosineSimilarityEmbeddingsReally2024; @goyalComparativeAnalysisDifferent2022].

Though one could use any kind of (L)LM to calculate embeddings for this case^[And there are approaches to use LLMs to solve this taks i.e., @jiangScalingSentenceEmbeddings2023a], it is advisable to use models specifically trained for this purpose. 
@reimersSentenceBERTSentenceEmbeddings2019a proposed *Sentence-BERT* which is a simple but effective approach to calculate semantic embeddings. SBERT and similar approaches are based on a (L)LM that was trained to predict missing words as we discussed before, resulting in a general representation of natural language. In the case of the original paper, they used (among others) the BERT model @devlinBERTPretrainingDeep2019a mentioned before.

The authors then use this to embed a pair of sentences into one embedding-vector each^[The original BERT-paper did this by adding a pooling layer before the task-header that extracted and weighed the context-dependend embedding of the first token. The SBERT paper tried different pooling-strategies and used a mean over each embedding dimension of the sequence.], for which some measure of semantic similarity is known. 
An example for a dataset containing such sentences is the [Stanford Natural Language Inferenc(SNLI) corpus @bowmanLargeAnnotatedCorpus2015](https://aclanthology.org/D15-1075/) which labels 550k pairs of sentences as either *entailment*, *contradiction* or *neutral*.
@reimersSentenceBERTSentenceEmbeddings2019a then concated the both senteces embeddings and their element-wise difference into a single vector which is fed to a multiclass classifier, indicating in which category the sentences relationship falls. At inference, this classification head was removed and replaced as the cosine similarity as discussed above.
The resulting network is highly effective in calculating semantic similarities between sentences.

A look at the [sbert-website](https://www.sbert.net/docs/sentence_transformer/loss_overview.html) shows that the module has somewhat grown and now does supply a series of learning paradigms that can be used to efficiently tune a model for your specific usecase^[And this does not have to be expensive. @tunstallEfficientFewShotLearning2022 have shown a highly efficient contrastive learning paradigm that limts the amount of necessary labels for a ridiuculously small amount of labels.]. 
As the library has grown, so has the sheer amount of pretrained embedding-models in some way based on this architecture that are hosted on huggingface. The [MTEB-Leaderboard](https://huggingface.co/spaces/mteb/leaderboard) is a good strat to search for a model for your application.
One utilization of this model-family, which has already been implicitly used in this script, is their very efficient ability to semantically search for documents.
If a model is very good at finding similar sentences, it can also be very good to find documents that are very similar to a question.

Look at the example illustrated in @fig-docRetrieval. The question "why is the sky blue" embedded with the same model as our 5 documents stating some facts.

![](){#fig-docRetrieval}

We can then calculate the cosine-similarity between these embeddings and return the document, that has the highest similarity to our question.


:::{.callout-note}

### üìù Task

Install the sentence-transformer package and download [the climate_fever-dataset](https://huggingface.co/datasets/tdiggelm/climate_fever).

Choose one model from the [MTEB-Leaderboard](https://huggingface.co/spaces/mteb/leaderboard) that you deem adequatly sized and appropriate for the task

Test the different metrics for the first twenty claims of the dataset and a question you formulate.

Use the similarity-implementations from [sklearn.metrics.pairwise](https://scikit-learn.org/dev/api/sklearn.metrics.html#module-sklearn.metrics.pairwise).

:::

This approach of using a model to embed documents and questions into a vector space is the basis for the so-called *Retrieval augmented generation*.

## Retrieval augmented generation

Retrieval augmented generation (RAG) is a framework that does pretty much do what it says on the tin.
You use a retrieval model to find documents that are similar to your question and then either return these documents our feed them into a generative model, which then generates an answer based on these documents.
This process can additionally be wrapped as a tool to be used by an agent, so that your existing agent can now also use external knowledge sources to answer questions.

Retrieval does not have to be semantics-based in this context - all kinds of data sources and databases can be made accessible for a LLM - we will focus on a purely embbedding based approach here though.

Although the small example in the last task was working, it is not really scalable. It was fine for a limited set of examples, if you want to realistically make a whole knowledge base searchable, you need to use an appropriate database system.

### Vector databases

A vector database is a database that stores vectors and allows for efficient similarity searches.
As can be seen in the [db-engines ranking](https://db-engines.com/en/ranking) there has been a surge of interest in this area recently, with many new players entering the market.
From the plethora of vector databases, these three are examples that virtue a honorary mention:

1. [Chroma](https://www.trychroma.com/) - a in-memory database for small applications that is especially easy to get to run. 

2. [Elasticsearch](https://www.elastic.co/guide/en/elasticsearch/reference/current/bring-your-own-vectors.html) - a well established database that is the go to system for open source search engines and has recently (and kind of naturally) also branched out into vector databases.

3. [Qdrant](https://qdrant.tech/) - the product of a Berlin-based startup that focusses on stability and scalability. It can also run in memory, but does natively support hard drive storage.

The best way to use qdrant is to use [docker](https://qdrant.tech/documentation/quickstart/) to run it and the python sdk to interact with it.

::: {.callout-note}
## üìù Task

Install qdrant and the python sdk.

Create a collection for the claims and one for the evidence in [the climate_fever-dataset](https://huggingface.co/datasets/tdiggelm/climate_fever).
Add the first 200 entries to each of these collections.

Test the similarity search on a question you formulate.

:::

### RAG

The last step to make this into a RAG pipeline is to use a generative model to answer the question based on the retrieved documents.

Most agent frameworks provide integrations for a variety of vector databases. 

In terms of haystack, there are not just one but two tutorials on how to get qdrant to integrate into your pipeline, one from [qdrant](https://qdrant.tech/documentation/frameworks/haystack/) for general integration and one from [haystack](https://haystack.deepset.ai/cookbook/sparse_embedding_retrieval) for sparse vectors (though the procedure shown should also work for our dense case).

::: {.callout-note}
## üìù Task

Build a haystack pipeline that allows you to chat with the climate_fever evidence.

[This tutorial](https://haystack.deepset.ai/cookbook/conversational_rag_using_memory) might be helpful in approaching that task.
:::

### Document chunking

The examples we looked at until now were all working with short text-snippets that comforably fit into the context window of a LLM.
If you think about usual usecases for RAG-systems, this is not the most common case though.
Usually, you will have a base of documents that can span multiple 1000's of tokens and you want to be able to answer questions about these documents.
Furthermore, you do not only want to know which document might be relevant, but ideally also which part of the document matches your question best.

This is where the process of doctument chunking or document splitting comes into play.
There is a series of possible approaches to split a document, the most common, so called **naive chunking** method, is to use a structural element of the document though. This means that you parse the documents into sentences, paragraphs or pages and then use these as chunks that you individually embed and store in your vector database.
To prevent loss of relevant context when splitting a document into chunks, it is additionally common to add some **overlap** between the chunks. This tries to solve the lost context problem, does however create reduncencies in the data.

An alternative approach is to use **semantic chunking**. This means that you split a document into chunks based on their meaning. Jina.ai explained in a blogpost [@LateChunkingLongContext2024] their so called "late chunking" method. which iteratively runs the whole document through the attention head of the transformer to gain embeddings per token, and then averages these embeddings per naive chunk. This way, the chunks are still structure based but contain semantic information about the whole context.
Haystack does not implement this feature yet, though [it is planned](https://github.com/deepset-ai/haystack/issues/8111).

Another approach to semantic chunking is descirbed on the doc-pages of [LlamaIndex](https://docs.llamaindex.ai/en/stable/examples/node_parsers/semantic_chunking/). In their approach to semantic chunking, an adaptive splitting-rule is used, that splits the documents based on semantic similarity of sentences. This means that sentences that are semantically similar are grouped together into chunks.

::: {.callout-note}
## üìù Task

Implement a document chunking strategy for a book of your choice from the [project_gutenberg](https://huggingface.co/datasets/manu/project_gutenberg) dataset.

You can use any approach you like, but you should explain your choice and why it is appropriate for this dataset.
:::

### Query Expansion

Until now, we have based our retrieval on the assumption, that the question the user formulates is a good representation of their information need.
This is not always the case though.
Often, users do not know what they are looking for or they use synonyms or paraphrases that are not present in the documents.
If the question is not formulated well, or if it is too specific, the system might not be able to find relevant documents.
To improve the quality of the questions, we can use **query expansion**. This means that we take the original question and expand it with additional information to make it more specific and to increase the chances of finding relevant documents.
This can be done in multiple ways, one common approach is to use a generative model to generate multiple queries based on the original question.
Another approach is to use a keyword extraction algorithm to extract keywords from the question and then use these keywords to expand the query.

Haystack provides an implementation of query expansion using [a custom pipeline component](https://haystack.deepset.ai/cookbook/query-expansion). 

::: {.callout-note}
## üìù Task

Implement query expansion for the climate_fever dataset using Haystack.

Experiment with different prompts and temperatures.
:::

## Further Readings

* [This blogpost](https://www.deepset.ai/blog/llms-retrieval-augmentation) by DeepSet gives a good overview of the concept of RAG

## References