[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Generative AI",
    "section": "",
    "text": "Introduction\nThis introductory script on the topic of ‚ÄúGenerative AI‚Äù was created for the elective module ‚ÄúGenerative AI‚Äù as given to the master students of the master programme ‚ÄúData Science‚Äù at the University of Applied Sciences Kiel and was built using quarto.\nThe intention of this script is not to be a complete guide to all things generative AI, but to give an overview of the topics and applications of these emerging techniques.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Generative AI",
    "section": "Schedule:",
    "text": "Schedule:\n\nCourse schedule\n\n\n\n\n\n\n\n\n\nNumber:\nCW:\nDate:\nTitle:\nTopics:\n\n\n\n\n1\n46\n12.11.\nGetting started with (L)LMs\nLanguage Model Basics\n\n\n\n\n\n\nChoosing open source models\n\n\n\n\n\n\nBasics of using open source models (Huggingface, Ollama, LLM-Studio, Llama.cpp, ‚Ä¶)\n\n\n2\n46\n13.11.\nPrompting\nPrompting strategies\n\n\n\n\n\n\nGeneration of synthetic texts\n\n\n3\n47\n19.11.\nAgent basics\nFundamentals of agents and train-of-thought prompting\n\n\n\n\n\n\nExamples of agent-frameworks (Llamaindex, LangChain & Haystack)\n\n\n4\n47\n20.11.\nEmbedding-based agent-systems\nSemantic embeddings and vector stores\n\n\n\n\n\n\nRetrieval augmented and interleaved generation\n\n\n5\n48\n26.11.\nFunction Calling\nCode generation and function calling\n\n\n\n\n\n\nData analysis\n\n\n6\n48\n27.11.\nAgent interaction\nConstitutional AI Tuning\n\n\n\n\n\n\nPreventing prompt injections\n\n\n7\n49\n3.12.\nAI image generation I\nAI image generator basics\n\n\n\n\n\n\nBasics of using Open Source AI image generation models\n\n\n\n\n\n\nGenerative Adversarial Networks (GANs)\n\n\n8\n49\n4.12.\nAI image generation II\nMultimodal embeddings\n\n\n\n\n\n\nVariational Autoencoders / Diffusion Models\n\n\n9\n50\n10.12.\nAugmentation of image datasets\n(Generative) approaches for image dataset augmentation\n\n\n10\n50\n11.12.\nFinetuning Basics\nBasics of Finetuning strategies\n\n\n\n\n\n\nAlignment and Finetuning of (L)LMs\n\n\n11\n51\n17.12.\nRank adaptation\nFundamentals of High and Low-Rank Adaptation of Language and Diffusion Models\n\n\n\n\n\n\n(Q)LoRA fine-tuning using Unsloth\n\n\n12\n51\n18.12.\nProject presentations",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "content/orga.html",
    "href": "content/orga.html",
    "title": "Organizational Details",
    "section": "",
    "text": "Planned Class Structure\nEach class meeting will follow this structure:\nStudents will be divided into teams of three at the start of the course, with projects culminating in a final presentation to the class. The project grade will count towards your final course grade.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Organizational Details</span>"
    ]
  },
  {
    "objectID": "content/orga.html#planned-class-structure",
    "href": "content/orga.html#planned-class-structure",
    "title": "Organizational Details",
    "section": "",
    "text": "Instructional Session: We‚Äôll introduce new concepts and techniques.\nPractice Exercise: Students will apply these concepts through an exercise.\nProject Worktime: Students will work on their team projects.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Organizational Details</span>"
    ]
  },
  {
    "objectID": "content/orga.html#project-details",
    "href": "content/orga.html#project-details",
    "title": "Organizational Details",
    "section": "Project Details",
    "text": "Project Details\nProjects should allow students to apply what they‚Äôve learned throughout the course. They must implement an LLM-based system that includes at least two of the following features:\n\nRetrieval Augmentation/RAG\nData Analysis\nMultiple Agents\nFine-tuning on Synthetic Data\n\nThe project should also include function-calling-based interface to an AI image generator.\nStudents are free to choose their project topic, as long as it fits within the course scope and is approved by the instructor. All projects must be implemented in Python.\nExample Project Ideas:\n\nLLM Tourist Guide: Uses TA.SH data to provide travel tips and enhances them with generated images.\nQuarto Data Presentation Pipeline: Builds and illustrates a Quarto presentation based on a given open dataset.\nSynthetic Author: Generates commit-messages based on commit history/diff. It could also suggest GitHub issues illustrated with AI-generated images.\nAI Storyteller: Creates illustrated short stories for children based on historical events.\nAI Webdesigner A tool that creates and illustrates a webpage based on a Amazon product page.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Organizational Details</span>"
    ]
  },
  {
    "objectID": "content/getting_started_with_llms.html",
    "href": "content/getting_started_with_llms.html",
    "title": "Getting started with (L)LMs",
    "section": "",
    "text": "Language Model Basics\nThis chapter provides a brief introduction to the history and function of modern language models, focusing on their practical use in text generation tasks. It will then give a short introduction on how to utilize pretrained language models for your own applications.\nLanguage models have diverse applications, including speech recognition, machine translation, text generation, and question answering. While we‚Äôll concentrate on text generation for this course, understanding the general concept of language models is crucial. Given language‚Äôs inherent complexity and ambiguity, a fundamental challenge in NLP is creating structured representations that can be employed downstream. This section will first explore the evolution of these representations before introducing the transformer architecture, which forms the foundation of most modern language models.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Getting started with (L)LMs</span>"
    ]
  },
  {
    "objectID": "content/getting_started_with_llms.html#language-model-basics",
    "href": "content/getting_started_with_llms.html#language-model-basics",
    "title": "Getting started with (L)LMs",
    "section": "",
    "text": "A short history of natural language processing\n\n\n\n\n\n\nFig¬†2.1: BOW-representation of sentences.\n\n\n\nThe Bag Of Words (BOW) method represents text data by counting the frequency of each word in a given document or corpus. It treats all words as independent and ignores their order, making it suitable for tasks like text classification, for which it was traditionally the gold-standard. However, BOW has limitations when it comes to capturing semantic relationships between words and gets utterly useless if confronted with words not represented in the corpus. Additionally, it does not take into account the order of words in a sentence, which can be crucial for understanding its meaning. For example, the sentences ‚ÄúThe cat is on the mat‚Äù and ‚ÄúThe mat is on the cat‚Äù have different meanings despite having the same set of words.\n\n\n\n\n\n\nFig¬†2.2: CBOW-representation of corpus.\n\n\n\nThe Continuous Bag Of Words (CBOW) method extends traditional BOW by representing words as dense vectors in a continuous space. CBOW predicts a target word based on its context, learning meaningful word representations from large amounts of text data.\n\n\n\n\n\n\nFig¬†2.3: FastText1 Model using CBOW-Method to predict missing word.\n\n1¬†Well, kind of. One of the major advantages of fasttext was the introduction of subword information which were left out of this illustration to save on space. This meant that uncommon words that were either absent or far and few between in the training corpus could be represented by common syllables. The display like it is here is far closer to fasttext‚Äôs spiritual successor word2vec (Mikolov et al., 2013).\n\nfastText (Bojanowski et al., 2017), an open-source library developed by Facebook, builds upon the CBOW method and introduces significant improvements. It incorporates subword information and employs hierarchical softmax for efficient training on large-scale datasets. Even with limited data, fastText can learn meaningful word representations. fastText and its predecessor Word2Vec are considered precursors to modern language models due to their introduction of Embeddings, which laid the foundation for many modern NLP methods.\n\n\n\n\n\n\nFig¬†2.4: Model using CBOW-Method to predict missing word.\n\n\n\nLanguage Model Embeddings are learned by predicting the next word in a sequence. These embeddings capture semantic and syntactic relationships between words, enabling them to understand context effectively. Since these embeddings represent the conditional probability distribution that language models learn to comprehend natural language, they can be reused by other models for tasks such as text classification or text retrieval. But more on this later.\nStill, these models did not really solve the inherent issue of the order of words in a sentence. The input of models of this generation still used a dummyfied version of the corpus to represent context, which loses a lot of information.\n\n\n\n\n\n\nFig¬†2.5: Illustration of a simple RNN-model, (exaggeratingly) illustrating the issue of vanishing gradients\n\n\n\nTraditionally, this was approached by feeding these embeddings into Recurrent Neural Networks (RNNs). These models could learn to keep track of sequential dependencies in text data and improve the understanding of context. However, RNNs suffered from their architecture‚Äôs inherent inability to retain information over long sequences. Simple RNN- cells2 iterate through a sequence and use both their last output and the next sequence element as input to predict the next output. This makes it hard for them to learn long-term dependencies, since they have to compress all information into one vector. This problem is known as the vanishing gradient problem and is schematically illustrated in Figure¬†2.5.\n\n2¬†And pretty much all of the more complex variantsLong Short-Term Memory (LSTM) networks addressed this issue by introducing a mechanism called ‚Äúgates‚Äù that allowed information to flow through the network selectively and more efficiently, but were, as the RNNs before, notoriuosly slow in training since only one word could be processed at a time. Additionally, a single LSTM is still only able to process the input sequence from left to right, which is not ideal for inputs that contain ambiguos words that need context after them to fully understand their meaning. Take the following part of a sentence:\n\nThe plant was growing\n\nThe word plant get‚Äôs wildly differing meanings, depending on how the sentence continues:\n\nThe plant was growing rapidly in the sunny corner of the garden.\n\n\nThe plant was growing to accommodate more machinery for production.\n\nA model that only processes the input sequence from left to right would just not be able to understand the meaning of ‚Äúplant‚Äù in this context.\nThe ELMo model (Peters et al., 2018), which stands for Embeddings from Language Models, is an extension of LSTMs that improved contextual word representations. ELMo uses bidirectional LSTM layers to capture both past and future context, enabling it to understand the meaning of words in their surrounding context. This resulted in ELMo outperforming other models of its era on a variety of natural language processing tasks. Still as each of the LSTM-Layer were only able to process one part of the sequence at a time, it was still unfortunately slow in training and inference. Its performance additionally decreased with the length of the input sequence since LSTM-cells have a better information retention than RNNs but are still not able to keep track of dependencies over long sequences.\n\n\nAttention is all you need\nIn their seminal paper ‚ÄúAttention is all you need‚Äù, Vaswani et al. (2023) described the transformer architecture.\nAs the paper‚Äôs title neatly suggests, the major breakthrough presented in this paper was the introduction of the so-called self-attention mechanism. This mechanism allows the model to ‚Äúfocus‚Äù on different parts of the input to a) determine the appropriate context for each word and b) to improve its performance on differing tasks by allowing the model to filter unnecessary information.\n\nSelf-Attention Mechanism\nThe self-attention mechanism relies on three components: Query (Q), Key (K), and Value (V), inspired by concepts in information retrieval. Imagine you search for a specific term in a library (query), match it against the catalog (key), and retrieve relevant books (value).\nIn practice, for each word in a sentence, the model calculates: 1. Relevance Scores: Compare each Query vector (Q) with every Key vector (K) in the sequence using the dot product. These scores measure how much focus one word should have on another. 2. Attention Weights: Normalize the scores using a softmax function to ensure they sum to 1, distributing focus proportionally across all words. 3. Weighted Sum: Multiply each Value vector (V) by its corresponding attention weight to compute the final representation.\nFor example, in the sentence, ‚ÄúThe cat sat on the mat,‚Äù the model might assign more attention to ‚Äúcat‚Äù when analyzing ‚Äúsat,‚Äù capturing their relationship.\n\n\nCalculating Attention\nFor a sequence of words, the attention scores are computed as: [ (Q, K, V) = ()V ] where: - (Q) represents the query matrix. - (K) is the key matrix. - (V) is the value matrix. - (d_k) is the dimensionality of the key vectors, ensuring scale invariance.\nLet‚Äôs illustrate this with a practical example.\nWe look at a retrieval task in which we query in a domain that has 5 attributes describing the items in it. The aforementioned ‚Äúlookup‚Äù is then implemented by calculating the dot product between the query and the transposed keys resulting in a vector of weights for each input-aspect.\nAs a simplification, we assume that all aspects can be described in binary terms. A hypothetical 1x5 query matrix (Q) represents the aspects we are querying in a 5-dimensional space, while a transposed 1x5 key matrix (K) represents the aspects of the search space. The dot product between these matrices results in a scalar that reflects the alignment or similarity between the query and the key, effectively indicating how many aspects of the query align with the search space.\n\n\n\n\n\n\n\n\n\nIf we now add a series of items we want to query for to our matrix \\(K\\), the result will be a vector representing the amount of matches, each item has with our query:\n\n\n\n\n\n\n\n\n\nThe result is a vector of attention scores that sum up to 1 for each word in the input sequence. This principle does obviously also work for more than one query by adding more rows to our Query matrix \\(Q\\). This does result in a matrix, in which each row indicates the amount of matching keys for each query:\n\n\n\n\n\n\n\n\n\nInstead of binary indicators, the \\(Q\\) and \\(K\\) matrices in the attention mechanism are filled with floats. This does still result in the same kind of matched-key-result, although the results are now more like degrees of relevance instead of absolute matches:\n\\[\nQ \\times K^T =\n\\]\n\n\n\n\n\n\n\n\n\nAs you can already see in this small example, the values of individual cells can get relatively high compared to the rest of the matrix. As you remember - we want to use this product to rank our values. If these numbers are too large, it might lead to numerical instability or incorrect results. To address this issue, we will scale down the dot-product by dividing it with \\(\\sqrt{d_n}\\), where \\(d_n\\) is the dimension of the aspect space (in our case 5).\n\\[\n\\frac{Q \\times K^T}{\\sqrt{d_n}} =\n\\]\n\n\n\n\n\n\n\n\n\nSince we want to use this matrix for filtering our dataset, we would prefer the weights to sum up to one. To achieve that, we will apply a softmax function on each row of the matrix (remember that the rows currently represent the key-weighted aspects for each query). The resulting matrix with scaled weights for each aspect is then multiplied with the value-matrix that contains one datapoint in each row, described by 5 aspects along the columns.\n\\[\nsoftmax(\\frac{Q \\times K^T}{\\sqrt{d_n}}) \\times V =\n\\]\n\n\n\n\n\n\n\n\n\nThe result is now an attention matrix in the sense that it tells us the importance of each value‚Äôs aspect for our query. In the specific example, the forth value seems to be the most important aspect for our third query. The crucial advantage is, that all aspects of all queries can be simultaneously compared with all aspects of all values without the necessity of sequential processing.\nThough this general idea of weighting aspects in the sense of self-attention3 to process a sequence without disadvantages of the distances of the items was used before (Bahdanau, 2014), the major contribution of the paper was the complete reliance of this mechanism without the need of LSTM/RNN parts. That their suggested architecture works is in part due to the utilisation of multiple self-attention layers, each learning its own weights for \\(Q\\), \\(K\\) and \\(V\\). This allows the model to learn more complex patterns and dependencies between words in a sentence. You can think of it as allowing the model to focus on different parts of the input sequence at different stages of processing. The outputs of the multiple heads are then concatenated and linearly transformed into the final output representation using a series of fully connected feed-forward layers.\n3¬†self in the sense of the model weighting its own embeddings, queries, keys and valuesThis small example is already pretty close to the general attention-mechanism described by Vaswani et al. (2023) (see also Figure¬†2.6), though the actual language model learns its own weights for \\(Q\\), \\(K\\) and \\(V\\).\n\n\n\n\n\n\nFig¬†2.6: Multi-headed attention as depicted in Vaswani et al. (2023)\n\n\n\nInstead of 5x5 matrices, the attenion mechanism as described in the paper implements \\(d_n \\times d_c\\)4 matrices, where \\(d_n\\) is the dimension of the embedding space5 and \\(d_c\\) is the size of the context window. In the original paper, Vaswani et al. (2023) implement the context-window as the same size as the embedding space (i.e., \\(d_n = d_c\\)). In Figure¬†2.7 you can see a brilliant illustration of the multiheaded-attention mechanism at work.\n4¬†\\(\\frac{d_n}{h} \\times \\frac{d_c}{h}\\) actually, the paper used feed-forward layers to reduce the dimensionality of each attention header to reduce th computational cost.5¬†I.e., the dimensionality used to represent each word‚Äôs meaning. In the previous toy-example illustrating the concept of embeddings, this would be the width of the hidden layer (8). In the case of transformers, this is usually 512 or 1024. These embeddings are learned during training and are a simple transformation of the one-hot vectors returned by the models tokenizer.\n\n\n\n\n\nFig¬†2.7: Illustration of the multi-headed attention mechanism. Taken from Hussain et al. (2024)\n\n\n\nThe implementation of the multi-headed attention mechanism allowed to solve all major issues of the language modelling approaches of the previous generation6. It firstly allows the input of a whole text-sequence at once, rendering the training and inference far speedier then the recursive approaches. Furthermore, the multi-head attention mechanism allows the model to focus on different parts of the input sequence simultaneously, enabling it to capture more complex relationships between words and improve its understanding of context without losing information about long-term dependencies. This mechanism also implicitly solves the bidirectionality-issue since each word can be taken into account when processsing every other word in the sequence.\n6¬†Well, kind of. Transformers are far superior language models due to their ability to parallely process long sequence without issues with stretched context - these advantages come at a price though. GPT-3s training is estimated to have emitted around 502 metric tons of carbon (AIAAIC - ChatGPT training emits 502 metric tons of carbon, n.d.). The computational cost of the architecture as described here does additionally scale quadratically with context window size.The description until now omitted one final but key detail - we only spoke about the weight matrices \\(Q\\), \\(K\\) and \\(V\\). Each of these weight matrices are actually the product of the learned weights and the input vectors. In other words, each of the three matrices is calculated as follows:\n\\[\n\\begin{align}\n    Q &= XW_Q \\\\\n    K &= XW_k \\\\\n    V &= XW_v\n\\end{align}\n\\]\nwhere \\(W_{Q, k, v}\\) are the learned weight matrices and \\(X\\) is the input matrix. This input matrix consists of a) the learned embeddings of the tokenized input-parts and b) the added, so called positional encoding.7\n7¬†While we are talking about omitted details, the whole architecture implements its layers as residual layers. This means that the output of each layer is added to the input of the layer before it is passed on to the next layer. But this detail is irelevant for our understanding of the central mechanism.The positional encoding is a vector that encodes the position of each token in the input sequence. It is added to the embedding of each token to provide the model with information about the order of the tokens in the sequence. The positional encoding is calculated as follows:\n\\[\n\\begin{array}{lcl}\nPE_{(pos, 2i)} &=& sin(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}) \\\\\nPE_{(pos, 2i+1)} &=& cos(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})\n\\end{array}\n\\]\nWhere \\(i\\) is the dimension and \\(pos\\) is the position. Those 2 formulas are not the most intuitive, what they do is to add a unique offset to each embedding though, that allows the model to infer and weigh the token‚Äôs positions in the matrix on it‚Äôs own. Figure¬†2.8 illustrates the pattern this specific combination of sin and cos creates for each sequence-position and embedding-dimension.\n\n\n\n\n\n\n\n\nFig¬†2.8: The positional encoding for 50 dimensions and 512 embedding-dimensions. The x-axis represents the position and the y-axis represents the dimension. The color represents the value of the encoding.\n\n\n\n\n\nThese parts alltogether are all building-blocks of the basic transformer architecture. As you can see in Figure¬†2.9, all parts depicted by Vaswani et al. (2023) are parts we have discussed until now.\n\n\n\n\n\n\nFig¬†2.9: The transformer architecture as depicted in Vaswani et al. (2023)\n\n\n\nThe Encoder half uses the embedding -&gt; encoding -&gt; multi-headed-attention -&gt; feed-forward structure to create a semantic representation of the sequence. The Decoder half uses the same structure, but with an additional masked multi-head attention layer to prevent the model from looking at future tokens. This is necessary because we want to generate a sequence token by token.\nFigure¬†2.10, taken from Kaplan et al. (2020), shows the test performance of Transformer models compared to LSTM-based models as a function of model size and context length. Transformers outperform LSTMs with increasing context length.\n\n\n\n\n\n\nFig¬†2.10: Comparison of Transformer- and LSTM-performance based on Model size and context length. Taken from Kaplan et al. (2020)\n\n\n\nFurthermore, Kaplan et al. (2020) and Hoffmann et al. (2022) after them postulated performace power-laws (see also Figure¬†2.11) that suggest that the performance of a Transformer directly scales with the models size and data availability. Though the task of prediction of natural language poses a non-zero limit to the performance, it is suggested that this limit is not reached for any of the currently available models.8\n8¬†Incidentally, we might run out of data to train on before reaching that limit (Villalobos et al., 2024).\n\n\n\n\n\nFig¬†2.11: Performance power law for transformer models. Taken from Kaplan et al. (2020)\n\n\n\nThe advances made through leveraging transformer-based architectures for language modelling led to a family of general-purpose language models. Unlike the approaches before, these models were not trained for a specific task but rather on a general text base with the intention of allowing specific fine-tuning to adapt to a task. Classic examples of these early general-purpose natural language generating Transformer models are the Generative Pre-trained Transformer (the predecessor of ChatGPT you all know), first described in Radford et al. (2018), and the ‚ÄúBidirectional Encoder Representations from Transformers‚Äù (BERT) architecture and training procedure, described by Devlin et al. (2019).\nThis general-purpose architecture is the base of modern LLMs as we know them today and most applications we will discuss in this course.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Getting started with (L)LMs</span>"
    ]
  },
  {
    "objectID": "content/getting_started_with_llms.html#choosing-open-source-models",
    "href": "content/getting_started_with_llms.html#choosing-open-source-models",
    "title": "Getting started with (L)LMs",
    "section": "Choosing open source models",
    "text": "Choosing open source models\nThe 2023 release of ChatGPT by OpenAI has sparked a lot of interest in large language models (LLMs) and their capabilities. This has also led to an increase in the number of available open-source LLMs. The selection of a model for your application is always a trade-off between performance, size, and computational requirements.\nAlthough Kaplan et al. (2020) showed a relationship between performance and model-size, the resources available will most probably limit you to smaller models. Additionally, a lot of tasks can be solved by smaller models if they are appropriately fine-tuned (Hsieh et al., 2023).\nA good idea when choosing an open source model is to start small and test whether the performace is sufficient for your use case. If not, you can always try a larger model later on.\nAdditionally, it is good practice to check the license of the model you want to use. Some models are only available under a non-commercial license, which means that you cannot use them for commercial purposes.\nThirdly, you should make sure that the model you choose is appropriate for your use case. For example, if you want to use a model for text generation, you should make sure that it was trained on a dataset that is similar to the data you will be using. If you want to use a model for translation, you should make sure that it was trained on a dataset that includes the languages you are interested in. A lot of usecases do already have benchmark datasets that can be used to pit models against each other and evaluate there appropriateness for a given use case based on a few key metrics.\nA good starting point for getting an overview about such metrics and benchmarks is Hugging Face. This platform has long cemented itself as the go-to place for getting access to open source models, but also provides a lot of resources for evaluating and comparing them. This page provides an overview of benchmarks, leaderboards and comparisons for a variety of tasks.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Getting started with (L)LMs</span>"
    ]
  },
  {
    "objectID": "content/getting_started_with_llms.html#basics-of-using-open-source-models-huggingface-ollama-llm-studio-llama.cpp",
    "href": "content/getting_started_with_llms.html#basics-of-using-open-source-models-huggingface-ollama-llm-studio-llama.cpp",
    "title": "Getting started with (L)LMs",
    "section": "Basics of using open source models (Huggingface, Ollama, LLM-Studio, Llama.cpp, ‚Ä¶)",
    "text": "Basics of using open source models (Huggingface, Ollama, LLM-Studio, Llama.cpp, ‚Ä¶)\nNow it is your turn! In your project-groups, you will each have to build a small ‚ÄúHello World‚Äù-style application that uses an open source model.\n\nChoose a small model using the sources we discussed before.\nEach group is to use one of the following frameworks\n\nHuggingface\nOllama\nLM-Studio from python\nLlama.cpp to load and use the model in your application.\n\nPresent your results and your experiences with the frameworks in a short presentation.\nSubmit your code and report on moodle.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Getting started with (L)LMs</span>"
    ]
  },
  {
    "objectID": "content/getting_started_with_llms.html#further-readings",
    "href": "content/getting_started_with_llms.html#further-readings",
    "title": "Getting started with (L)LMs",
    "section": "Further Readings",
    "text": "Further Readings\n\nThis quite high-level blog-article about foundational models by Heidloff (2023)\nThe Attention is all you need-paper (Vaswani et al., 2023) and the brilliant video discussing it by Umar Jamil (Umar Jamil, 2023)\nThis very good answer on stack exchange that explains the attention-concept ((https://stats.stackexchange.com/users/95569/dontloo), n.d.)",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Getting started with (L)LMs</span>"
    ]
  },
  {
    "objectID": "content/getting_started_with_llms.html#references",
    "href": "content/getting_started_with_llms.html#references",
    "title": "Getting started with (L)LMs",
    "section": "References",
    "text": "References\n\n\n\n\nAIAAIC - ChatGPT training emits 502 metric tons of carbon. (n.d.). https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chatgpt-training-emits-502-metric-tons-of-carbon.\n\n\nBahdanau, D. (2014). Neural machine translation by jointly learning to align and translate. arXiv Preprint arXiv:1409.0473. https://arxiv.org/abs/1409.0473\n\n\nBojanowski, P., Grave, E., Joulin, A., & Mikolov, T. (2017). Enriching Word Vectors with Subword Information (arXiv:1607.04606). arXiv. https://doi.org/10.48550/arXiv.1607.04606\n\n\nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (arXiv:1810.04805). arXiv. https://doi.org/10.48550/arXiv.1810.04805\n\n\nHeidloff, N. (2023). Foundation Models, Transformers, BERT and GPT. In Niklas Heidloff. https://heidloff.net/article/foundation-models-transformers-bert-and-gpt/.\n\n\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. de L., Hendricks, L. A., Welbl, J., Clark, A., Hennigan, T., Noland, E., Millican, K., Driessche, G. van den, Damoc, B., Guy, A., Osindero, S., Simonyan, K., Elsen, E., ‚Ä¶ Sifre, L. (2022). Training Compute-Optimal Large Language Models (arXiv:2203.15556). arXiv. https://doi.org/10.48550/arXiv.2203.15556\n\n\nHsieh, C.-Y., Li, C.-L., Yeh, C.-K., Nakhost, H., Fujii, Y., Ratner, A., Krishna, R., Lee, C.-Y., & Pfister, T. (2023). Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes (arXiv:2305.02301). arXiv. https://doi.org/10.48550/arXiv.2305.02301\n\n\n(https://stats.stackexchange.com/users/95569/dontloo), dontloo. (n.d.). What exactly are keys, queries, and values in attention mechanisms? Cross Validated.\n\n\nHussain, Z., Binz, M., Mata, R., & Wulff, D. U. (2024). A tutorial on open-source large language models for behavioral science. Behavior Research Methods, 56(8), 8214‚Äì8237. https://doi.org/10.3758/s13428-024-02455-8\n\n\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., & Amodei, D. (2020). Scaling Laws for Neural Language Models (arXiv:2001.08361). arXiv. https://doi.org/10.48550/arXiv.2001.08361\n\n\nMikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space (arXiv:1301.3781). arXiv. https://doi.org/10.48550/arXiv.1301.3781\n\n\nPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., & Zettlemoyer, L. (2018). Deep contextualized word representations (arXiv:1802.05365). arXiv. https://doi.org/10.48550/arXiv.1802.05365\n\n\nRadford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). Improving language understanding with unsupervised learning.\n\n\nUmar Jamil. (2023). Attention is all you need (Transformer) - Model explanation (including math), Inference and Training.\n\n\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2023). Attention Is All You Need (arXiv:1706.03762). arXiv. https://doi.org/10.48550/arXiv.1706.03762\n\n\nVillalobos, P., Ho, A., Sevilla, J., Besiroglu, T., Heim, L., & Hobbhahn, M. (2024, June). Position: Will we run out of data? Limits of LLM scaling based on human-generated data. Forty-First International Conference on Machine Learning.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Getting started with (L)LMs</span>"
    ]
  },
  {
    "objectID": "content/prompting.html",
    "href": "content/prompting.html",
    "title": "Prompting",
    "section": "",
    "text": "Instruct-tuned models\nPrompting describes the utilization of the ability of language models to use zero or few-shot instrutions to perform a task. This ability, which we briefly touched on when we were discussing the history of language models (i.e., the paper by Radford et al. (2019)), is one of the most important aspects of modern large language models.\nPrompting can be used for various tasks such as text generation, summarization, question answering, and many more.\nInstruct-tuned models are trained on a dataset (for an example, see Figure¬†3.1) that consists of instructions and their corresponding outputs. This is different from the pretraining phase of language models where they were trained on large amounts of text data without any specific task in mind. The goal of instruct-tuning is to make the model better at following instructions and generating more accurate and relevant outputs.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Prompting</span>"
    ]
  },
  {
    "objectID": "content/prompting.html#instruct-tuned-models",
    "href": "content/prompting.html#instruct-tuned-models",
    "title": "Prompting",
    "section": "",
    "text": "Fig¬†3.1: An example for a dataset that can be used for instruct-finetuning. This dataset can be found on huggingface\n\n\n\n\n\n\n\n\n\nüìù Task\n\n\n\nTest the difference between instruct and non-instruct-models by trying to get a gpt2-version (i.e., ‚ÄúQuantFactory/gpt2-xl-GGUF‚Äù) and a small Llama 3.2 Instruct-Model (i.e., ‚Äúhugging-quants/Llama-3.2-1B-Instruct-Q8_0-GGUF‚Äù to write a small poem about the inception of the field of language modelling. Use LM-Studio to test this.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) A poem written by Llama 3.2 1B - a model with Instruct-Finetuning\n\n\n\n\n\n\n\n\n\n\n\n(b) A ‚Äúpoem‚Äù written by GPT2 - a model without Instruct-Finetuning\n\n\n\n\n\n\n\nFig¬†3.2: A poem and a ‚Äúpoem‚Äù\n\n\n\n\n\nShow answer",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Prompting</span>"
    ]
  },
  {
    "objectID": "content/prompting.html#prompting-strategies",
    "href": "content/prompting.html#prompting-strategies",
    "title": "Prompting",
    "section": "Prompting strategies",
    "text": "Prompting strategies\nThe results of a prompted call to a LM is highly dependent on the exact wording of the prompt. This is especially true for more complex tasks, where the model needs to perform multiple steps in order to solve the task. It is not for naught that the field of ‚Äúprompt engineering‚Äù has emerged. There is a veritable plethora of resources available online that discuss different strategies for prompting LMs. It has to be said though, that the strategies that work and don‚Äôt work can vary greatly between models and tasks. A bit of general advice that holds true for nearly all models though, is to\n\ndefine the task in as many small steps as possible\nto be as literal and descriptive as possible and\nto provide examples if possible.\n\nSince the quality of results is so highly dependent on the chosen model, it is good practice to test candidate strategies against each other and therefore to define a target on which the quality of results can be evaluated. One example for such a target could be a benchmark dataset that contains multiple examples of the task at hand.\n\n\n\n\n\n\nNote\n\n\n\n## üìù Task 1. Test the above-mentioned prompting strategies on the MTOP Intent Dataset and evaluate the results against each other. The dataset contains instructions and labels indicating on which task the instruction was intended to prompt. Use a python script to call one of the following three models in LM-Studio for this:\n\nPhi 3.1 mini\nGemma 2 2B\nLlama 3.2 1B\n\nUse the F1-score implemented in scikit learn to evaluate your results.\n2. You do sometimes read very specific tips on how to improve your results. Here are three, that you can find from time to time:\n\nDo promise rewards (i.e., monetary tips) instead of threatening punishments\nDo formulate using affirmation (‚ÄúDo the task‚Äù) instead of negating behaviours to be avoided (‚ÄúDon‚Äôt do this mistake‚Äù)\nLet the model reason about the problem before giving an answer\n\nCheck these strategies on whether they improve your results. If your first instruction already results in near-perfect classification, brainstorm a difficult task that you can validate qualitatively. Let the model write a recipe or describe Kiel for example.\n3. Present your results\n3. Upload your code to moodle",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Prompting</span>"
    ]
  },
  {
    "objectID": "content/prompting.html#generation-of-synthetic-texts",
    "href": "content/prompting.html#generation-of-synthetic-texts",
    "title": "Prompting",
    "section": "Generation of synthetic texts",
    "text": "Generation of synthetic texts\nAs we discussed before, small models can perform on an acceptable level, if they are finetuned appropriately.\nA good way to do this is to use a larger model to generate synthetic data that you then use for training the smaller model. This approach has been used successfully in many applications, for example for improving graph-database queries (Zhong et al., 2024), for improving dataset search (Silva & Barbosa, 2024) or the generation of spreadsheet-formulas (Singh et al., 2024).\nSince even the largest LLMs are not perfect in general and might be even worse on some specific niche tasks, evidence suggests that a validation strategy for data generated in this way is beneficial (Kumar et al., 2024; Singh et al., 2024).\nStrategies to validate the synthetic data include:\n\nUsing a human annotator to label part of the data to test the models output\nForcing the model to answer in a structured way that is automatically testable (e.g., by using JSON)\nForcing the model to return 2 or more answers and checking for consistency\nCombining the two approaches above (i.e., forcing the model to return multiple structured outputs (JSON, XML, YAML, ‚Ä¶) and checking for consistency)\nUsing a second LLM/different prompt to rate the answers\n\n\n\n\n\n\n\nüìù Task\n\n\n\nUsing your script for batch-testing different prompts, generate synthetic data for a emotion detection task based on Paul Ekman‚Äôs six basic emotions: anger, disgust, fear, happiness, sadness and surprise1.\nThe generated data should consist of a sentence and the emotion that is expressed in it. Start by generating two examples for each emotion. Validate these results and adapt them if necessary. Then use these examples to generate 100 samples for each emotion.\nUse one of the above mentioned (non-manual) strategies to validate the data you generated.\n\n\n1¬†Though this nomenclature has fallen a bit out of fashion",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Prompting</span>"
    ]
  },
  {
    "objectID": "content/prompting.html#further-readings",
    "href": "content/prompting.html#further-readings",
    "title": "Prompting",
    "section": "Further Readings",
    "text": "Further Readings\n\nThis prompting-guide has some nice general advice\nOpenAI has its own set of tipps\ndeepset, the company behind Haystack, has a nice guide as well\nThis blog-article, again written by Heidloff (Heidloff, 2023)\n\n\n\n\n\nHeidloff, N. (2023). Fine-tuning small LLMs with Output from large LLMs. In Niklas Heidloff. https://heidloff.net/article/fine-tune-small-llm-with-big-llm/.\n\n\nKumar, B., Amar, J., Yang, E., Li, N., & Jia, Y. (2024). Selective Fine-tuning on LLM-labeled Data May Reduce Reliance on Human Annotation: A Case Study Using Schedule-of-Event Table Detection (arXiv:2405.06093). arXiv. https://doi.org/10.48550/arXiv.2405.06093\n\n\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language models are unsupervised multitask learners. OpenAI Blog, 1(8), 9.\n\n\nSilva, L., & Barbosa, L. (2024). Improving dense retrieval models with LLM augmented data for dataset search. Knowledge-Based Systems, 294, 111740. https://doi.org/10.1016/j.knosys.2024.111740\n\n\nSingh, U., Cambronero, J., Gulwani, S., Kanade, A., Khatry, A., Le, V., Singh, M., & Verbruggen, G. (2024). An Empirical Study of Validating Synthetic Data for Formula Generation (arXiv:2407.10657). arXiv. https://doi.org/10.48550/arXiv.2407.10657\n\n\nZhong, Z., Zhong, L., Sun, Z., Jin, Q., Qin, Z., & Zhang, X. (2024). SyntheT2C: Generating Synthetic Data for Fine-Tuning Large Language Models on the Text2Cypher Task (arXiv:2406.10710). arXiv. https://doi.org/10.48550/arXiv.2406.10710",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Prompting</span>"
    ]
  },
  {
    "objectID": "content/agent_basics.html",
    "href": "content/agent_basics.html",
    "title": "Agent basics",
    "section": "",
    "text": "Fundamentals of agents and train-of-thought prompting",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Agent basics</span>"
    ]
  },
  {
    "objectID": "content/agent_basics.html#examples-of-agent-frameworks-llamaindex-langchain-haystack",
    "href": "content/agent_basics.html#examples-of-agent-frameworks-llamaindex-langchain-haystack",
    "title": "Agent basics",
    "section": "Examples of agent-frameworks (Llamaindex, LangChain & Haystack)",
    "text": "Examples of agent-frameworks (Llamaindex, LangChain & Haystack)",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Agent basics</span>"
    ]
  },
  {
    "objectID": "content/agent_basics.html#further-readings",
    "href": "content/agent_basics.html#further-readings",
    "title": "Agent basics",
    "section": "Further Readings",
    "text": "Further Readings",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Agent basics</span>"
    ]
  },
  {
    "objectID": "content/embeddings.html",
    "href": "content/embeddings.html",
    "title": "Embedding-based agent-systems",
    "section": "",
    "text": "Semantic embeddings and vector stores",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Embedding-based agent-systems</span>"
    ]
  },
  {
    "objectID": "content/embeddings.html#retrieval-augmented-and-interleaved-generation",
    "href": "content/embeddings.html#retrieval-augmented-and-interleaved-generation",
    "title": "Embedding-based agent-systems",
    "section": "Retrieval augmented and interleaved generation",
    "text": "Retrieval augmented and interleaved generation",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Embedding-based agent-systems</span>"
    ]
  },
  {
    "objectID": "content/embeddings.html#further-readings",
    "href": "content/embeddings.html#further-readings",
    "title": "Embedding-based agent-systems",
    "section": "Further Readings",
    "text": "Further Readings",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Embedding-based agent-systems</span>"
    ]
  },
  {
    "objectID": "content/function_calling.html",
    "href": "content/function_calling.html",
    "title": "Function Calling",
    "section": "",
    "text": "Code generation and function calling",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Function Calling</span>"
    ]
  },
  {
    "objectID": "content/function_calling.html#data-analysis",
    "href": "content/function_calling.html#data-analysis",
    "title": "Function Calling",
    "section": "Data analysis",
    "text": "Data analysis",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Function Calling</span>"
    ]
  },
  {
    "objectID": "content/function_calling.html#further-readings",
    "href": "content/function_calling.html#further-readings",
    "title": "Function Calling",
    "section": "Further Readings",
    "text": "Further Readings",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Function Calling</span>"
    ]
  },
  {
    "objectID": "content/agent_interaction.html",
    "href": "content/agent_interaction.html",
    "title": "Agent interaction",
    "section": "",
    "text": "Constitutional AI Tuning",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Agent interaction</span>"
    ]
  },
  {
    "objectID": "content/agent_interaction.html#preventing-prompt-injections",
    "href": "content/agent_interaction.html#preventing-prompt-injections",
    "title": "Agent interaction",
    "section": "Preventing prompt injections",
    "text": "Preventing prompt injections",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Agent interaction</span>"
    ]
  },
  {
    "objectID": "content/agent_interaction.html#further-readings",
    "href": "content/agent_interaction.html#further-readings",
    "title": "Agent interaction",
    "section": "Further Readings",
    "text": "Further Readings",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Agent interaction</span>"
    ]
  },
  {
    "objectID": "content/generator_basics.html",
    "href": "content/generator_basics.html",
    "title": "AI image generation",
    "section": "",
    "text": "AI image generator basics",
    "crumbs": [
      "Image Generation",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>AI image generation</span>"
    ]
  },
  {
    "objectID": "content/generator_basics.html#basics-of-using-open-source-ai-image-generation-models",
    "href": "content/generator_basics.html#basics-of-using-open-source-ai-image-generation-models",
    "title": "AI image generation",
    "section": "Basics of using Open Source AI image generation models",
    "text": "Basics of using Open Source AI image generation models",
    "crumbs": [
      "Image Generation",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>AI image generation</span>"
    ]
  },
  {
    "objectID": "content/generator_basics.html#generative-adversarial-networks-gans",
    "href": "content/generator_basics.html#generative-adversarial-networks-gans",
    "title": "AI image generation",
    "section": "Generative Adversarial Networks (GANs)",
    "text": "Generative Adversarial Networks (GANs)",
    "crumbs": [
      "Image Generation",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>AI image generation</span>"
    ]
  },
  {
    "objectID": "content/generator_basics.html#further-readings",
    "href": "content/generator_basics.html#further-readings",
    "title": "AI image generation",
    "section": "Further Readings",
    "text": "Further Readings",
    "crumbs": [
      "Image Generation",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>AI image generation</span>"
    ]
  },
  {
    "objectID": "content/augmentation.html",
    "href": "content/augmentation.html",
    "title": "Augmentation of image datasets",
    "section": "",
    "text": "(Generative) approaches for image dataset augmentation",
    "crumbs": [
      "Image Generation",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Augmentation of image datasets</span>"
    ]
  },
  {
    "objectID": "content/augmentation.html#further-readings",
    "href": "content/augmentation.html#further-readings",
    "title": "Augmentation of image datasets",
    "section": "Further Readings",
    "text": "Further Readings",
    "crumbs": [
      "Image Generation",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Augmentation of image datasets</span>"
    ]
  },
  {
    "objectID": "content/finetuning_approaches.html",
    "href": "content/finetuning_approaches.html",
    "title": "Finetuning Approaches",
    "section": "",
    "text": "Basics of Finetuning strategies",
    "crumbs": [
      "Finetuning",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Finetuning Approaches</span>"
    ]
  },
  {
    "objectID": "content/finetuning_approaches.html#alignment-and-finetuning-of-llms",
    "href": "content/finetuning_approaches.html#alignment-and-finetuning-of-llms",
    "title": "Finetuning Approaches",
    "section": "Alignment and Finetuning of (L)LMs",
    "text": "Alignment and Finetuning of (L)LMs",
    "crumbs": [
      "Finetuning",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Finetuning Approaches</span>"
    ]
  },
  {
    "objectID": "content/finetuning_approaches.html#further-readings",
    "href": "content/finetuning_approaches.html#further-readings",
    "title": "Finetuning Approaches",
    "section": "Further Readings",
    "text": "Further Readings",
    "crumbs": [
      "Finetuning",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Finetuning Approaches</span>"
    ]
  },
  {
    "objectID": "content/rank_adaptation.html",
    "href": "content/rank_adaptation.html",
    "title": "Rank adaptation",
    "section": "",
    "text": "Fundamentals of High and Low-Rank Adaptation of Language and Diffusion Models",
    "crumbs": [
      "Finetuning",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Rank adaptation</span>"
    ]
  },
  {
    "objectID": "content/rank_adaptation.html#qlora-fine-tuning-using-unsloth",
    "href": "content/rank_adaptation.html#qlora-fine-tuning-using-unsloth",
    "title": "Rank adaptation",
    "section": "(Q)LoRA fine-tuning using Unsloth",
    "text": "(Q)LoRA fine-tuning using Unsloth",
    "crumbs": [
      "Finetuning",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Rank adaptation</span>"
    ]
  },
  {
    "objectID": "content/rank_adaptation.html#further-readings",
    "href": "content/rank_adaptation.html#further-readings",
    "title": "Rank adaptation",
    "section": "Further Readings",
    "text": "Further Readings",
    "crumbs": [
      "Finetuning",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Rank adaptation</span>"
    ]
  },
  {
    "objectID": "content/getting_started_with_llms.html#basics-of-using-open-source-models",
    "href": "content/getting_started_with_llms.html#basics-of-using-open-source-models",
    "title": "Getting started with (L)LMs",
    "section": "Basics of using open source models",
    "text": "Basics of using open source models\n\n\n\n\n\n\nüìù Task\n\n\n\nNow it is your turn! In your project-groups, you will each have to build a small ‚ÄúHello World‚Äù-style application that uses an open source model.\n\nChoose a small model using the sources we discussed before.\nEach group is to use one of the following frameworks\n\nHuggingface\nOllama\nLM-Studio from python\nLlama.cpp to load and use the model in your application.\n\nPresent your results and your experiences with the frameworks in a short presentation.\nSubmit your code and report on moodle.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Getting started with (L)LMs</span>"
    ]
  }
]