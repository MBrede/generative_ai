[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Generative AI",
    "section": "",
    "text": "Introduction\nThis introductory script on the topic of “Generative AI” was created for the elective module “Generative AI” as given to the master students of the master programme “Data Science” at the University of Applied Sciences Kiel and was built using quarto.\nThe intention of this script is not to be a complete guide to all things generative AI, but to give an overview of the topics and applications of these emerging techniques.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Generative AI",
    "section": "Schedule:",
    "text": "Schedule:\n\n\n\n\n\n\n\n\n\n\nNumber:\nCW:\nDate:\nTitle:\nTopics:\n\n\n\n\n1\n46\n12.11.\nGetting started with (L)LMs\n* Language Model Basics\n\n\n\n\n\n\n* Choosing open source models\n\n\n\n\n\n\n* Basics of using open source models (Huggingface, Ollama, LLM-Studio, Llama.cpp, …)\n\n\n2\n46\n13.11.\nPrompting\n* Prompting strategies\n\n\n\n\n\n\n* Generation of synthetic texts\n\n\n3\n47\n19.11.\nAgent basics\n* Fundamentals of agents and train-of-thought prompting\n\n\n\n\n\n\n* Examples of agent-frameworks (Llamaindex, LangChain & Haystack)\n\n\n4\n47\n20.11.\nEmbedding-based agent-systems\n* Semantic embeddings and vector stores\n\n\n\n\n\n\n* Retrieval augmented and interleaved generation\n\n\n5\n48\n26.11.\nFunction Calling\n* Code generation and function calling\n\n\n\n\n\n\n* Data analysis\n\n\n6\n48\n27.11.\nAgent interaction\n* Constitutional AI Tuning\n\n\n\n\n\n\n* Preventing prompt injections\n\n\n7\n49\n3.12.\n???\n\n\n\n8\n49\n4.12.\nAI image generation I\n* AI image generator basics\n\n\n\n\n\n\n* Basics of using Open Source AI image generation models\n\n\n\n\n\n\n* Generative Adversarial Networks (GANs)\n\n\n9\n50\n10.12.\nAI image generation II\n* Multimodal embeddings\n\n\n\n\n\n\n* Variational Autoencoders / Diffusion Models\n\n\n10\n50\n11.12.\nAugmentation of image datasets\n* (Generative) approaches for image dataset augmentation\n\n\n11\n51\n17.12.\nFinetuning Basics\n* Basics of Finetuning strategies\n\n\n\n\n\n\n* Alignment and Finetuning of (L)LMs\n\n\n12\n51\n18.12.\nRank adaptation\n* Fundamentals of High and Low-Rank Adaptation of Language and Diffusion Models\n\n\n\n\n\n\n* (Q)LoRA fine-tuning using Unsloth\n\n\n\nTests für LLM-Modelle? Oder lieber nicht? - Multimodal embeddings? - Knowledge graphs?",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#notizen",
    "href": "index.html#notizen",
    "title": "Generative AI",
    "section": "Notizen",
    "text": "Notizen\n\nInstruct vs non-instruct",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#themenideen",
    "href": "index.html#themenideen",
    "title": "Generative AI",
    "section": "Themenideen",
    "text": "Themenideen\n\nKinderbuch mit Anspruch",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#in-session-aufgaben",
    "href": "index.html#in-session-aufgaben",
    "title": "Generative AI",
    "section": "In-session Aufgaben",
    "text": "In-session Aufgaben\nTermin: 1. Kleine Sprachmodelle zum Laufen bringen mit unterschiedlichen Backends 2. Prompting-Strategien testen Datensatz 3. Agent, der Zeit bis Semesterende, Kieler Woche, … ausrechnet 4. Datensatz(?) in vector-store überführen und dann durchsuchen und Agent zugänglich machen 5. Agent-System zur deskriptiven Analyse, Beispiel mit PandasAI? 6. Update eines Datensatzes nach Constitution",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "content/orga.html",
    "href": "content/orga.html",
    "title": "Organizational Details",
    "section": "",
    "text": "Planned Class Structure\nEach class meeting will follow this structure:\nStudents will be divided into teams of three at the start of the course, with projects culminating in a final presentation to the class. The project grade will count towards your final course grade.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Organizational Details</span>"
    ]
  },
  {
    "objectID": "content/orga.html#planned-class-structure",
    "href": "content/orga.html#planned-class-structure",
    "title": "Organizational Details",
    "section": "",
    "text": "Instructional Session: We’ll introduce new concepts and techniques.\nPractice Exercise: Students will apply these concepts through an exercise.\nProject Worktime: Students will work on their team projects.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Organizational Details</span>"
    ]
  },
  {
    "objectID": "content/orga.html#project-details",
    "href": "content/orga.html#project-details",
    "title": "Organizational Details",
    "section": "Project Details",
    "text": "Project Details\nProjects should allow students to apply what they’ve learned throughout the course. They must implement an LLM-based system that includes at least two of the following features:\n\nRetrieval Augmentation/RAG\nData Analysis\nMultiple Agents\nFine-tuning on Synthetic Data\n\nThe project should also include function-calling-based interface to an AI image generator.\nStudents are free to choose their project topic, as long as it fits within the course scope and is approved by the instructor. All projects must be implemented in Python.\nExample Project Ideas:\n\nLLM Tourist Guide: Uses TA.SH data to provide travel tips and enhances them with generated images.\nQuarto Data Presentation Pipeline: Builds and illustrates a Quarto presentation based on a given open dataset.\nSynthetic Author: Generates commit-messages based on commit history/diff. It could also suggest GitHub issues illustrated with AI-generated images.\nAI Storyteller: Creates illustrated short stories for children based on historical events.\nAI Webdesigner A tool that creates and illustrates a webpage based on a Amazon product page.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Organizational Details</span>"
    ]
  },
  {
    "objectID": "content/getting_started_with_llms.html",
    "href": "content/getting_started_with_llms.html",
    "title": "Getting started with (L)LMs",
    "section": "",
    "text": "Language Model Basics\nThis chapter is a brief introduction to the basics of language models and how to use them in practice. It assumes that you have some basic knowledge about machine learning and natural language processing.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting started with (L)LMs</span>"
    ]
  },
  {
    "objectID": "content/getting_started_with_llms.html#language-model-basics",
    "href": "content/getting_started_with_llms.html#language-model-basics",
    "title": "Getting started with (L)LMs",
    "section": "",
    "text": "A short history of natural language processing\n\n\n\n\n\n\nFig 2.1: BOW-representation of sentences.\n\n\n\nThe Bag Of Words (BOW) method represents text data by counting the frequency of each word in a given document or corpus. It treats all words as independent and ignores their order, making it suitable for tasks like text classification, for which it was traditionally the gold-standard. However, BOW has limitations when it comes to capturing semantic relationships between words and gets utterly useless if confronted with words not represented in the corpus. Additionally, it does not take into account the order of words in a sentence, which can be crucial for understanding its meaning. For example, the sentences “The cat is on the mat” and “The mat is on the cat” have different meanings despite having the same set of words.\n\n\n\n\n\n\nFig 2.2: CBOW-representation of corpus.\n\n\n\nThe Continuous Bag Of Words (CBOW) method extends traditional BOW by representing words as dense vectors in a continuous space. CBOW predicts a target word based on its context, learning meaningful word representations from large amounts of text data.\n\n\n\n\n\n\nFig 2.3: FastText1 Model using CBOW-Method to predict missing word.\n\n1 Well, kind of. One of the major advantages of fasttext was the introduction of subword information which were left out of this illustration to save on space. This meant that uncommon words that were either absent or far and few between in the training corpus could be represented by common syllables. The display like it is here is far closer to fasttext’s spiritual successor word2vec (Mikolov et al., 2013).\n\nfastText (Bojanowski et al., 2017), an open-source library developed by Facebook, builds upon the CBOW method and introduces significant improvements. It incorporates subword information and employs hierarchical softmax for efficient training on large-scale datasets. Even with limited data, fastText can learn meaningful word representations. fastText and its predecessor Word2Vec are considered precursors to modern language models due to their introduction of Embeddings, which laid the foundation for many modern NLP methods.\n\n\n\n\n\n\nFig 2.4: Model using CBOW-Method to predict missing word.\n\n\n\nLanguage Model Embeddings are learned by predicting the next word in a sequence. These embeddings capture semantic and syntactic relationships between words, enabling them to understand context effectively. Since these embeddings represent the conditional probability distribution that language models learn to comprehend natural language, they can be reused by other models for tasks such as text classification or text retrieval. But more on this later.\nStill, these models did not really solve the inherent issue of the order of words in a sentence. The input of models of this generation still used a dummyfied version of the corpus to represent context, which loses a lot of information.\n\n\n\n\n\n\nFig 2.5: RNN-model, in this cased trained on text prediction.\n\n\n\nTraditionally, this was approached by feeding these embeddings into Recurrent Neural Networks (RNNs). These models could learn to keep track of long-term dependencies in text data and improve the understanding of context. However, RNNs suffered from the vanishing gradient problem, which made it difficult for them to capture long-range dependencies effectively.\n\n\n\n\n\n\nFig 2.6: LSTM-model, also trained on text prediction.\n\n\n\nLong Short-Term Memory (LSTM) networks addressed this issue by introducing a mechanism called “gates” that allowed information to flow through the network more efficiently, but were, as the RNNs before, notoriuosly slow in training since only one word could be processed at a time. Additionally, a single LSTM is still only able to process the input sequence from left to right, which is not ideal for inputs that contain ambiguos words that need context after them to fully understand their meaning. Take the following part of a sentence:\n\nThe plant was growing\n\nThe word plant get’s wildly differing meanings, depending on how the sentence continues:\n\nThe plant was growing rapidly in the sunny corner of the garden.\n\n\nThe plant was growing to accommodate more machinery for production.\n\nA model that only processes the input sequence from left to right would just not be able to understand the meaning of “plant” in this context.\nThe ELMo model (Peters et al., 2018), which stands for Embeddings from Language Models, is an extension of LSTMs that improved contextual word representations. ELMo uses bidirectional LSTM layers to capture both past and future context, enabling it to understand the meaning of words in their surrounding context. This resulted in ELMo outperforming other models of it’s era on a variety of natural language processing tasks. Still as each of the LSTM-Layer were only able to process one part of the sequence at a time, it was still unfortunately slow in training and inference. It’s performance additionally decreased with the length of the input sequence since LSTM-cells have a better information retention than RNNs but are still not able to keep track of dependencies over long sequences.\n\n\nAttention is all you need\nIn their seminal paper “Attention is all you need”, Vaswani et al. (2023) described the transformer architecture. The implementation of the multi-headed attention meachnism allowed to solve all major issues of the language modelling approaches of the previous generation. It firstly allows the input of a whole text-sequence at once, rendering the training and inference far speedier then the recursive approaches. Furthermore, the multi-head attention mechanism allows the model to focus on different parts of the input sequence simultaneously, enabling it to capture more complex relationships between words and improve its understanding of context without losing information about long-term dependencies.\n\nFigure 2.7, taken from Kaplan et al. (2020), shows the test performance of Transformer models compared to LSTM-based models as a function of model size and context length. Transformers outperform LSTMs with increasing context length.\n\n\n\n\n\n\nFig 2.7: Comparison of Transformer- and LSTM-performance based on Model size and context length. Taken from Kaplan et al. (2020)\n\n\n\nFurthermore, Kaplan et al. (2020) and Hoffmann et al. (2022) after them postulated performace power-laws (see also #fig-powerlaw) that suggest that the performance of a Transformer directly scales with the models size and data availability. Though the task of prediction of natural language poses a non-zero limit to the performanve, it is suggested that this limit is not reached for any of the currently available models.2\n2 Incidentally, we might run out of data to train on before reaching that limit (Villalobos et al., 2024).\n\n\n\n\n\nFig 2.8: Performance power law for transformer models. Taken from Kaplan et al. (2020)\n\n\n\nThe advances made through leveraging transformer-based architectures for language modelling led to a family of general-purpose language models. Unlike the approaches before, these models were not trained for a specific task but rather on a general text base with the intention of allowing specific fine-tuning to adapt to a task. Classic examples of these early general-purpose natural language generating Transformer models are the Generative Pre-trained Transformer (the predecessor of ChatGPT you all know), first described in Radford et al. (2018), and the “Bidirectional Encoder Representations from Transformers” (BERT) architecture and training procedure, described by Devlin et al. (2019).\nThis general-purpose architecture is the base of modern LLMs as we know them today and most applications we will discuss in this course.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting started with (L)LMs</span>"
    ]
  },
  {
    "objectID": "content/getting_started_with_llms.html#choosing-open-source-models",
    "href": "content/getting_started_with_llms.html#choosing-open-source-models",
    "title": "Getting started with (L)LMs",
    "section": "Choosing open source models",
    "text": "Choosing open source models",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting started with (L)LMs</span>"
    ]
  },
  {
    "objectID": "content/getting_started_with_llms.html#basics-of-using-open-source-models-huggingface-ollama-llm-studio-llama.cpp",
    "href": "content/getting_started_with_llms.html#basics-of-using-open-source-models-huggingface-ollama-llm-studio-llama.cpp",
    "title": "Getting started with (L)LMs",
    "section": "Basics of using open source models (Huggingface, Ollama, LLM-Studio, Llama.cpp, …)",
    "text": "Basics of using open source models (Huggingface, Ollama, LLM-Studio, Llama.cpp, …)\nNow it is your turn! In your project-groups, you will each have to build a small “Hello World”-style application that uses an open source model.\n\nChoose a small model using the sources we discussed before.\nEach group is to use one of the following frameworks\n\nHuggingface\nOllama\nLM-Studio from python\nLlama.cpp to load and use the model in your application.\n\nPresent your results and your experiences with the frameworks in a short presentation.\nSubmit your code and report on moodle.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting started with (L)LMs</span>"
    ]
  },
  {
    "objectID": "content/getting_started_with_llms.html#further-readings",
    "href": "content/getting_started_with_llms.html#further-readings",
    "title": "Getting started with (L)LMs",
    "section": "Further Readings",
    "text": "Further Readings\n\nThe Attention is all you need-paper (Vaswani et al., 2023) and the brilliant video discussing it by Umar Jamil (Umar Jamil, 2023)",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting started with (L)LMs</span>"
    ]
  },
  {
    "objectID": "content/getting_started_with_llms.html#references",
    "href": "content/getting_started_with_llms.html#references",
    "title": "Getting started with (L)LMs",
    "section": "References",
    "text": "References\n\n\n\n\nBojanowski, P., Grave, E., Joulin, A., & Mikolov, T. (2017). Enriching Word Vectors with Subword Information (arXiv:1607.04606). arXiv. https://doi.org/10.48550/arXiv.1607.04606\n\n\nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (arXiv:1810.04805). arXiv. https://doi.org/10.48550/arXiv.1810.04805\n\n\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. de L., Hendricks, L. A., Welbl, J., Clark, A., Hennigan, T., Noland, E., Millican, K., Driessche, G. van den, Damoc, B., Guy, A., Osindero, S., Simonyan, K., Elsen, E., … Sifre, L. (2022). Training Compute-Optimal Large Language Models (arXiv:2203.15556). arXiv. https://doi.org/10.48550/arXiv.2203.15556\n\n\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., & Amodei, D. (2020). Scaling Laws for Neural Language Models (arXiv:2001.08361). arXiv. https://doi.org/10.48550/arXiv.2001.08361\n\n\nMikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space (arXiv:1301.3781). arXiv. https://doi.org/10.48550/arXiv.1301.3781\n\n\nPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., & Zettlemoyer, L. (2018). Deep contextualized word representations (arXiv:1802.05365). arXiv. https://doi.org/10.48550/arXiv.1802.05365\n\n\nRadford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). Improving language understanding with unsupervised learning.\n\n\nUmar Jamil. (2023). Attention is all you need (Transformer) - Model explanation (including math), Inference and Training.\n\n\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2023). Attention Is All You Need (arXiv:1706.03762). arXiv. https://doi.org/10.48550/arXiv.1706.03762\n\n\nVillalobos, P., Ho, A., Sevilla, J., Besiroglu, T., Heim, L., & Hobbhahn, M. (2024, June). Position: Will we run out of data? Limits of LLM scaling based on human-generated data. Forty-First International Conference on Machine Learning.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting started with (L)LMs</span>"
    ]
  },
  {
    "objectID": "content/prompting.html",
    "href": "content/prompting.html",
    "title": "Prompting",
    "section": "",
    "text": "Instruct-tuned models\nPrompting describes the utilization of the ability of language models to use zero or few-shot instrutions to perform a task. This ability, which we briefly touched on when we were discussing the history of language models (i.e., the paper by Radford et al. (2019)), is one of the most important aspects of modern large language models.\nPrompting can be used for various tasks such as text generation, summarization, question answering, and many more.\nInstruct-tuned models are trained on a dataset (for an example, see Figure 3.1) that consists of instructions and their corresponding outputs. This is different from the pretraining phase of language models where they were trained on large amounts of text data without any specific task in mind. The goal of instruct-tuning is to make the model better at following instructions and generating more accurate and relevant outputs.\nTest the difference between instruct and non-instruct-models by trying to get a gpt2-version (i.e., “QuantFactory/gpt2-xl-GGUF”) and a small Llama 3.2 Instruct-Model (i.e., “hugging-quants/Llama-3.2-1B-Instruct-Q8_0-GGUF” to write a small poem about the inception of the field of language modelling. Use LM-Studio to test this.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Prompting</span>"
    ]
  },
  {
    "objectID": "content/prompting.html#instruct-tuned-models",
    "href": "content/prompting.html#instruct-tuned-models",
    "title": "Prompting",
    "section": "",
    "text": "Fig 3.1: An example for a dataset that can be used for instruct-finetuning. This dataset can be found on huggingface\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) A poem written by Llama 3.2 1B - a model with Instruct-Finetuning\n\n\n\n\n\n\n\n\n\n\n\n(b) A “poem” written by GPT2 - a model without Instruct-Finetuning\n\n\n\n\n\n\n\nFig 3.2: A poem and a “poem”\n\n\n\n\n\nShow answer",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Prompting</span>"
    ]
  },
  {
    "objectID": "content/prompting.html#prompting-strategies",
    "href": "content/prompting.html#prompting-strategies",
    "title": "Prompting",
    "section": "Prompting strategies",
    "text": "Prompting strategies\nThe results of a prompted call to a LM is highly dependend on the exact wording of the prompt. This is especially true for more complex tasks, where the model needs to perform multiple steps in order to solve the task. It is not for naught that the field of “prompt engineering” has emerged. There is a veritable plethora of resources available online that discuss different strategies for prompting LMs. It has to be said though, that the strategies that work and don’t work can vary greatly between models and tasks. A bit of general advice that holds true for nearly all models though, is to\n\ndefine the task in as many small steps as possible\nto be as literal and descriptive as possible and\nto provide examples if possible.\n\nSince the quality of results is so highly dependable on the chosen model, it is good practice to test candidate strategies against each other and therefore to define a target on which the quality of results can be evaluated. One example for such a target could be a benchmark dataset that contains multiple examples of the task at hand.\n\n1. Test the above-mentioned prompting strategies on the MTOP Intent Dataset and evaluate the results against each other. The dataset contains instructions and labels indicating on which task the instruction was intendend to prompt. Use a python script to call one of the following three models in LM-Studio for this:\n\nPhi 3.1 mini\nGemma 2 2B\nLlama 3.2 1B\n\nUse the F1-score implemented in scikit learn to evaluate your results.\n2. You do sometimes read very specific tips on how to improve your results. Here are three, that you can find from time to time:\n\nDo promise rewards (i.e., monetary tips) instead of threatening punishments\nDo formulate using affirmation (“Do the task”) instead of negating behaviours to be avoided (“Don’t do this mistake”)\nLet the model reason about the problem before giving an answer\n\nCheck these strategies on whether they improve your results. If your first instruction already results in near-perfect classification, brainstorm a difficult task that you can validate qualitatively. Let the model write a recipe or describe Kiel for example.\n3. Present your results",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Prompting</span>"
    ]
  },
  {
    "objectID": "content/prompting.html#generation-of-synthetic-texts",
    "href": "content/prompting.html#generation-of-synthetic-texts",
    "title": "Prompting",
    "section": "Generation of synthetic texts",
    "text": "Generation of synthetic texts",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Prompting</span>"
    ]
  },
  {
    "objectID": "content/prompting.html#further-readings",
    "href": "content/prompting.html#further-readings",
    "title": "Prompting",
    "section": "Further Readings",
    "text": "Further Readings\n\n\n\n\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language models are unsupervised multitask learners. OpenAI Blog, 1(8), 9.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Prompting</span>"
    ]
  },
  {
    "objectID": "content/agent_basics.html",
    "href": "content/agent_basics.html",
    "title": "Agent basics",
    "section": "",
    "text": "Fundamentals of agents and train-of-thought prompting",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Agent basics</span>"
    ]
  },
  {
    "objectID": "content/agent_basics.html#examples-of-agent-frameworks-llamaindex-langchain-haystack",
    "href": "content/agent_basics.html#examples-of-agent-frameworks-llamaindex-langchain-haystack",
    "title": "Agent basics",
    "section": "Examples of agent-frameworks (Llamaindex, LangChain & Haystack)",
    "text": "Examples of agent-frameworks (Llamaindex, LangChain & Haystack)",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Agent basics</span>"
    ]
  },
  {
    "objectID": "content/agent_basics.html#further-readings",
    "href": "content/agent_basics.html#further-readings",
    "title": "Agent basics",
    "section": "Further Readings",
    "text": "Further Readings",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Agent basics</span>"
    ]
  },
  {
    "objectID": "content/embeddings.html",
    "href": "content/embeddings.html",
    "title": "Embedding-based agent-systems",
    "section": "",
    "text": "Semantic embeddings and vector stores",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Embedding-based agent-systems</span>"
    ]
  },
  {
    "objectID": "content/embeddings.html#retrieval-augmented-and-interleaved-generation",
    "href": "content/embeddings.html#retrieval-augmented-and-interleaved-generation",
    "title": "Embedding-based agent-systems",
    "section": "Retrieval augmented and interleaved generation",
    "text": "Retrieval augmented and interleaved generation",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Embedding-based agent-systems</span>"
    ]
  },
  {
    "objectID": "content/embeddings.html#further-readings",
    "href": "content/embeddings.html#further-readings",
    "title": "Embedding-based agent-systems",
    "section": "Further Readings",
    "text": "Further Readings",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Embedding-based agent-systems</span>"
    ]
  },
  {
    "objectID": "content/function_calling.html",
    "href": "content/function_calling.html",
    "title": "Function Calling",
    "section": "",
    "text": "Code generation and function calling",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Function Calling</span>"
    ]
  },
  {
    "objectID": "content/function_calling.html#data-analysis",
    "href": "content/function_calling.html#data-analysis",
    "title": "Function Calling",
    "section": "Data analysis",
    "text": "Data analysis",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Function Calling</span>"
    ]
  },
  {
    "objectID": "content/function_calling.html#further-readings",
    "href": "content/function_calling.html#further-readings",
    "title": "Function Calling",
    "section": "Further Readings",
    "text": "Further Readings",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Function Calling</span>"
    ]
  },
  {
    "objectID": "content/agent_interaction.html",
    "href": "content/agent_interaction.html",
    "title": "Agent interaction",
    "section": "",
    "text": "Constitutional AI Tuning",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Agent interaction</span>"
    ]
  },
  {
    "objectID": "content/agent_interaction.html#preventing-prompt-injections",
    "href": "content/agent_interaction.html#preventing-prompt-injections",
    "title": "Agent interaction",
    "section": "Preventing prompt injections",
    "text": "Preventing prompt injections",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Agent interaction</span>"
    ]
  },
  {
    "objectID": "content/agent_interaction.html#further-readings",
    "href": "content/agent_interaction.html#further-readings",
    "title": "Agent interaction",
    "section": "Further Readings",
    "text": "Further Readings",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Agent interaction</span>"
    ]
  },
  {
    "objectID": "content/generator_basics.html",
    "href": "content/generator_basics.html",
    "title": "AI image generation",
    "section": "",
    "text": "AI image generator basics",
    "crumbs": [
      "Image Generation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>AI image generation</span>"
    ]
  },
  {
    "objectID": "content/generator_basics.html#basics-of-using-open-source-ai-image-generation-models",
    "href": "content/generator_basics.html#basics-of-using-open-source-ai-image-generation-models",
    "title": "AI image generation",
    "section": "Basics of using Open Source AI image generation models",
    "text": "Basics of using Open Source AI image generation models",
    "crumbs": [
      "Image Generation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>AI image generation</span>"
    ]
  },
  {
    "objectID": "content/generator_basics.html#generative-adversarial-networks-gans",
    "href": "content/generator_basics.html#generative-adversarial-networks-gans",
    "title": "AI image generation",
    "section": "Generative Adversarial Networks (GANs)",
    "text": "Generative Adversarial Networks (GANs)",
    "crumbs": [
      "Image Generation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>AI image generation</span>"
    ]
  },
  {
    "objectID": "content/generator_basics.html#further-readings",
    "href": "content/generator_basics.html#further-readings",
    "title": "AI image generation",
    "section": "Further Readings",
    "text": "Further Readings",
    "crumbs": [
      "Image Generation",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>AI image generation</span>"
    ]
  },
  {
    "objectID": "content/augmentation.html",
    "href": "content/augmentation.html",
    "title": "Augmentation of image datasets",
    "section": "",
    "text": "(Generative) approaches for image dataset augmentation",
    "crumbs": [
      "Image Generation",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Augmentation of image datasets</span>"
    ]
  },
  {
    "objectID": "content/augmentation.html#further-readings",
    "href": "content/augmentation.html#further-readings",
    "title": "Augmentation of image datasets",
    "section": "Further Readings",
    "text": "Further Readings",
    "crumbs": [
      "Image Generation",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Augmentation of image datasets</span>"
    ]
  },
  {
    "objectID": "content/finetuning_approaches.html",
    "href": "content/finetuning_approaches.html",
    "title": "Finetuning Approaches",
    "section": "",
    "text": "Basics of Finetuning strategies",
    "crumbs": [
      "Finetuning",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Finetuning Approaches</span>"
    ]
  },
  {
    "objectID": "content/finetuning_approaches.html#alignment-and-finetuning-of-llms",
    "href": "content/finetuning_approaches.html#alignment-and-finetuning-of-llms",
    "title": "Finetuning Approaches",
    "section": "Alignment and Finetuning of (L)LMs",
    "text": "Alignment and Finetuning of (L)LMs",
    "crumbs": [
      "Finetuning",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Finetuning Approaches</span>"
    ]
  },
  {
    "objectID": "content/finetuning_approaches.html#further-readings",
    "href": "content/finetuning_approaches.html#further-readings",
    "title": "Finetuning Approaches",
    "section": "Further Readings",
    "text": "Further Readings",
    "crumbs": [
      "Finetuning",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Finetuning Approaches</span>"
    ]
  },
  {
    "objectID": "content/rank_adaptation.html",
    "href": "content/rank_adaptation.html",
    "title": "Rank adaptation",
    "section": "",
    "text": "Fundamentals of High and Low-Rank Adaptation of Language and Diffusion Models",
    "crumbs": [
      "Finetuning",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Rank adaptation</span>"
    ]
  },
  {
    "objectID": "content/rank_adaptation.html#qlora-fine-tuning-using-unsloth",
    "href": "content/rank_adaptation.html#qlora-fine-tuning-using-unsloth",
    "title": "Rank adaptation",
    "section": "(Q)LoRA fine-tuning using Unsloth",
    "text": "(Q)LoRA fine-tuning using Unsloth",
    "crumbs": [
      "Finetuning",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Rank adaptation</span>"
    ]
  },
  {
    "objectID": "content/rank_adaptation.html#further-readings",
    "href": "content/rank_adaptation.html#further-readings",
    "title": "Rank adaptation",
    "section": "Further Readings",
    "text": "Further Readings",
    "crumbs": [
      "Finetuning",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Rank adaptation</span>"
    ]
  }
]