[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Generative AI",
    "section": "",
    "text": "Introduction\nThis script serves as an introduction to Generative AI and was developed for the elective module ‚ÄúGenerative AI,‚Äù offered to master‚Äôs students of the ‚ÄúData Science‚Äù program at the University of Applied Sciences Kiel. Built using quarto, this resource is designed to provide an accessible overview of key topics and applications in this rapidly evolving field.\nWhile not an exhaustive guide to Generative AI, the script highlights foundational concepts, modern applications, and practical techniques that empower students to engage with and explore the possibilities of these transformative technologies.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#contents-and-learning-objectives",
    "href": "index.html#contents-and-learning-objectives",
    "title": "Generative AI",
    "section": "Contents and learning objectives",
    "text": "Contents and learning objectives\nContents listed in the module database entry:\nOpen Source Language Models\n\nOverview of model lists\nOllama\nGeneration of synthetic text as training sets\n\nAgent Systems\n\nLlamaindex, LangChain & Haystack\nFunction calling\nData analysis\n\nEmbeddings and Vector Stores\n\nSemantic Search\nRetrieval-augmented generation\nRecommendations\n\nAI Image Generators\n\nGenerative Adversarial Networks (GANs)\nVariational Autoencoders / Diffusion Models\nGenerative approaches for image dataset augmentation\n\nFine-Tuning of LLMs and Diffusion Models\n\nExamples: LoRA, QLoRA, MoRA\n\n\nLearning objectives listed in the module database entry:\nStudents\n\nknow the fundamentals of generative AI systems.\nknow various modern applications of generative AI systems.\nknow the theoretical foundations and practical applications of generative AI systems.\n\nStudents\n\nare able to explain and apply various open-source language models.\nare able to implement and utilize agent systems and their functionalities.\nare able to understand and use embeddings and vector stores for semantic search and recommendations.\nare able to explain and practically apply different methods for image generation.\nare able to fine-tune large language models (LLMs) and diffusion models for specific tasks.\n\nStudents\n\nare able to successfully organize teamwork for generative AI projects.\nare able to report and present team solutions for practical project tasks.\nare able to interpret and communicate the approaches in technical and functional terms.\n\nStudents\n\nare able to work professionally in the field of generative AI systems.\nare able to give and accept professional feedback to different topics of generative AI systems.\nare able to select relevant scientific literature about generative AI systems.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Generative AI",
    "section": "Schedule:",
    "text": "Schedule:\n\nCourse schedule\n\n\n\n\n\n\n\n\n\nNumber:\nCW:\nDate:\nTitle:\nTopics:\n\n\n\n\n1\n46\n12.11.\nGetting started with (L)LMs\nLanguage Model Basics\n\n\n\n\n\n\nChoosing open source models\n\n\n\n\n\n\nBasics of using open source models (Huggingface, Ollama, LLM-Studio, Llama.cpp, ‚Ä¶)\n\n\n2\n46\n13.11.\nPrompting\nPrompting strategies\n\n\n\n\n\n\nGeneration of synthetic texts\n\n\n3\n47\n19.11.\nAgent basics\nFundamentals of agents and chain-of-thought prompting\n\n\n\n\n\n\nExamples of agent-frameworks (Llamaindex, LangChain & Haystack)\n\n\n4\n47\n20.11.\nEmbedding-based agent-systems\nSemantic embeddings and vector stores\n\n\n\n\n\n\nRetrieval augmented and interleaved generation\n\n\n5\n48\n26.11.\nFunction Calling\nCode generation and function calling\n\n\n\n\n\n\nData analysis\n\n\n6\n48\n27.11.\nAgent interaction\nConstitutional AI Tuning\n\n\n\n\n\n\nPreventing prompt injections\n\n\n7\n49\n3.12.\nAI image generation I\nAI image generator basics\n\n\n\n\n\n\nBasics of using Open Source AI image generation models\n\n\n\n\n\n\nGenerative Adversarial Networks (GANs)\n\n\n8\n49\n4.12.\nAI image generation II\nMultimodal embeddings\n\n\n\n\n\n\nVariational Autoencoders / Diffusion Models\n\n\n9\n50\n10.12.\nAugmentation of image datasets\n(Generative) approaches for image dataset augmentation\n\n\n10\n50\n11.12.\nFinetuning Basics\nBasics of Finetuning strategies\n\n\n\n\n\n\nAlignment and Finetuning of (L)LMs\n\n\n11\n51\n17.12.\nRank adaptation\nFundamentals of High and Low-Rank Adaptation of Language and Diffusion Models\n\n\n\n\n\n\n(Q)LoRA fine-tuning using Unsloth\n\n\n12\n51\n18.12.\nProject presentations",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "content/orga.html",
    "href": "content/orga.html",
    "title": "Organizational Details",
    "section": "",
    "text": "Planned Class Structure\nEach class meeting will follow this structure:\nStudents will be divided into teams of three at the start of the course, with projects culminating in a final presentation to the class. The project grade will count towards your final course grade.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Organizational Details</span>"
    ]
  },
  {
    "objectID": "content/orga.html#planned-class-structure",
    "href": "content/orga.html#planned-class-structure",
    "title": "Organizational Details",
    "section": "",
    "text": "Instructional Session: We‚Äôll introduce new concepts and techniques.\nPractice Exercise: Students will apply these concepts through an exercise.\nProject Worktime: Students will work on their team projects.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Organizational Details</span>"
    ]
  },
  {
    "objectID": "content/orga.html#project-details",
    "href": "content/orga.html#project-details",
    "title": "Organizational Details",
    "section": "Project Details",
    "text": "Project Details\nProjects should allow students to apply what they‚Äôve learned throughout the course. They must implement an LLM-based system that includes at least two of the following features:\n\nRetrieval Augmentation/RAG\nData Analysis\nMultiple Agents\nFine-tuning on Synthetic Data\n\nThe project should also include function-calling-based interface to an AI image generator.\nStudents are free to choose their project topic, as long as it fits within the course scope and is approved by the instructor. All projects must be implemented in Python.\nExample Project Ideas:\n\nLLM Tourist Guide: Uses TA.SH data to provide travel tips and enhances them with generated images.\nQuarto Data Presentation Pipeline: Builds and illustrates a Quarto presentation based on a given open dataset.\nSynthetic Author: Generates commit-messages based on commit history/diff. It could also suggest GitHub issues illustrated with AI-generated images.\nAI Storyteller: Creates illustrated short stories for children based on historical events.\nAI Webdesigner A tool that creates and illustrates a webpage based on a Amazon product page.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Organizational Details</span>"
    ]
  },
  {
    "objectID": "content/getting_started_with_llms.html",
    "href": "content/getting_started_with_llms.html",
    "title": "Getting started with (L)LMs",
    "section": "",
    "text": "Language Model Basics\nThis chapter provides a brief introduction to the history and function of modern language models, focusing on their practical use in text generation tasks. It will then give a short introduction on how to utilize pretrained language models for your own applications.\nLanguage models have diverse applications, including speech recognition, machine translation, text generation, and question answering. While we‚Äôll concentrate on text generation for this course, understanding the general concept of language models is crucial. Given language‚Äôs inherent complexity and ambiguity, a fundamental challenge in NLP is creating structured representations that can be employed downstream. This section will first explore the evolution of these representations before introducing the transformer architecture, which forms the foundation of most modern language models.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Getting started with (L)LMs</span>"
    ]
  },
  {
    "objectID": "content/getting_started_with_llms.html#language-model-basics",
    "href": "content/getting_started_with_llms.html#language-model-basics",
    "title": "Getting started with (L)LMs",
    "section": "",
    "text": "A short history of natural language processing\n\n\n\n\n\n\nFig¬†2.1: BOW-representation of sentences.\n\n\n\nThe Bag Of Words (BOW) method represents text data by counting the frequency of each word in a given document or corpus. It treats all words as independent and ignores their order, making it suitable for tasks like text classification, for which it was traditionally the gold-standard. However, BOW has limitations when it comes to capturing semantic relationships between words and gets utterly useless if confronted with words not represented in the corpus. Additionally, it does not take into account the order of words in a sentence, which can be crucial for understanding its meaning. For example, the sentences ‚ÄúThe cat is on the mat‚Äù and ‚ÄúThe mat is on the cat‚Äù have different meanings despite having the same set of words.\n\n\n\n\n\n\nFig¬†2.2: CBOW-representation of corpus.\n\n\n\nThe Continuous Bag Of Words (CBOW) method extends traditional BOW by representing words as dense vectors in a continuous space. CBOW predicts a target word based on its context, learning meaningful word representations from large amounts of text data.\n\n\n\n\n\n\nFig¬†2.3: Shallow Model using CBOW-Method to predict missing word.\n\n\n\nfastText (Bojanowski et al., 2017), an open-source library developed by Facebook, builds upon the CBOW method and introduces significant improvements. It incorporates subword information and employs hierarchical softmax for efficient training on large-scale datasets. Even with limited data, fastText can learn meaningful word representations. fastText and its predecessor Word2Vec are considered precursors to modern language models due to their introduction of Embeddings, which laid the foundation for many modern NLP methods. Figure¬†2.3 illustrates this fastText-architecture1\n1¬†Well, kind of. One of the major advantages of fasttext was the introduction of subword information which were left out of this illustration to save on space. This meant that uncommon words that were either absent or far and few between in the training corpus could be represented by common syllables. The display like it is here is far closer to fasttext‚Äôs spiritual predecessor word2vec (Mikolov et al., 2013).\n\n\n\n\n\nFig¬†2.4: Model using CBOW-Method to predict missing word.\n\n\n\nLanguage Model Embeddings are learned by predicting the next word, or, in most cases, the next part of a word in a sequence. The utilisation of word-parts instead of whole words was another invention introduced by fastText (Bojanowski et al., 2017), that allowed the model to generalize to new, unknown words when moving to inference. These parts of words are also called tokens. Embeddings are the representation the model learns to map the context-tokens to a multiclass classification of the missing token in the space of all possible tokens. These embeddings capture semantic and syntactic relationships between words, enabling them to understand context effectively. Since these embeddings represent the conditional probability distribution that language models learn to comprehend natural language, they can be reused by other models for tasks such as text classification or text retrieval. But more on this later.\nStill, these models did not really solve the inherent issue of the order of words in a sentence. The input of models of this generation still used a dummyfied version of the corpus to represent context, which loses a lot of information.\n\n\n\n\n\n\nFig¬†2.5: Illustration of a simple RNN-model, (exaggeratingly) illustrating the issue of the model ‚Äúforgetting‚Äù parts of the input when processing long sequences.\n\n\n\nTraditionally, this was approached by feeding these embeddings into Recurrent Neural Networks (RNNs). These models could learn to keep track of sequential dependencies in text data and improve the understanding of context. However, RNNs suffered from their architecture‚Äôs inherent inability to retain information over long sequences. Simple RNN- cells2 iterate through a sequence and use both their last output and the next sequence element as input to predict the next output. This makes it hard for them to learn long-term dependencies, since they have to compress all information into one vector (Figure¬†2.5)3.\n\n2¬†And pretty much all of the more complex variants3¬†This is also (kind of) the reason for the so called vanishing gradient problem, where each iteration of the network is necessary for calculating the gradient in the steps before.Long Short-Term Memory (LSTM) networks addressed this issue by introducing a mechanism called ‚Äúgates‚Äù that allowed information to flow through the network selectively and more efficiently, but were, as the RNNs before, notoriuosly slow in training since only one word could be processed at a time. Additionally, a single LSTM is still only able to process the input sequence from left to right, which is not ideal for inputs that contain ambiguos words that need context after them to fully understand their meaning. Take the following part of a sentence:\n\nThe plant was growing\n\nThe word plant get‚Äôs wildly differing meanings, depending on how the sentence continues:\n\nThe plant was growing rapidly in the sunny corner of the garden.\n\n\nThe plant was growing to accommodate more machinery for production.\n\nA model that only processes the input sequence from left to right would just not be able to understand the meaning of ‚Äúplant‚Äù in this context.\nThe ELMo model (Peters et al., 2018), which stands for Embeddings from Language Models, is an extension of LSTMs that improved contextual word representations. ELMo uses bidirectional LSTM layers to capture both past and future context, enabling it to understand the meaning of words in their surrounding context. This resulted in ELMo outperforming other models of its era on a variety of natural language processing tasks. Still as each of the LSTM-Layer were only able to process one part of the sequence at a time, it was still unfortunately slow in training and inference. Its performance additionally decreased with the length of the input sequence since LSTM-cells have a better information retention than RNNs but are still not able to keep track of dependencies over long sequences.\n\n\nAttention is all you need\nIn their transformative paper ‚ÄúAttention is all you need‚Äù, Vaswani et al. (2023) described the transformer architecture.\nAs the paper‚Äôs title neatly suggests, the major breakthrough presented in this paper was the introduction of the so-called self-attention mechanism. This mechanism allows the model to ‚Äúfocus‚Äù on different parts of the input to a) determine the appropriate context for each word and b) to improve its performance on differing tasks by allowing the model to filter unnecessary information.\n\nSelf-Attention Mechanism\nThe self-attention mechanism relies on three components: Query (Q), Key (K), and Value (V), inspired by concepts in information retrieval. Imagine you search for a specific term in a library (query), match it against the catalog (key), and retrieve relevant books (value).\nIn practice, for each word in a sentence, the model calculates:\n\nRelevance Scores: Compare each Query vector (Q) with every Key vector (K) in the sequence using the dot product. These scores measure how much focus one word should have on another.\nAttention Weights: Normalize the scores using a softmax function to ensure they sum to 1, distributing focus proportionally across all words.\nWeighted Sum: Multiply each Value vector (V) by its corresponding attention weight to compute the final representation.\n\nFor example, in the sentence, ‚ÄúThe cat sat on the mat,‚Äù the model might assign more attention to ‚Äúcat‚Äù when analyzing ‚Äúsat,‚Äù capturing their relationship.\n\n\nCalculating Attention\nFor a sequence of words, the attention scores are computed as: \\[\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n\\]\nwhere:\n\n\\(Q\\) represents the query matrix.\n\\(K\\) is the key matrix.\n\\(V\\) is the value matrix.\n\\(d_k\\) is the dimensionality of the key vectors, ensuring scale invariance.\n\nLet‚Äôs first illustrate this concept with a practical example (not specifically from the context of NLP) to later circle back to its application in the transformer architecture.\nWe look at a retrieval task in which we query in a domain that has 5 attributes describing the items in it. The aforementioned ‚Äúlookup‚Äù is then implemented by calculating the dot product between the query and the transposed keys resulting in a vector of weights for each input-aspect.\nAs a simplification, we assume that all aspects can be described in binary terms. A hypothetical 1x5 query matrix (Q) represents the aspects we are querying in a 5-dimensional space, while a transposed 1x5 key matrix (K) represents the aspects of the search space. The dot product between these matrices results in a scalar that reflects the alignment or similarity between the query and the key, effectively indicating how many aspects of the query align with the search space.\n\n\n\n\n\n\n\n\n\nIf we now add a series of items we want to query for to our matrix \\(K\\), the result will be a vector representing the amount of matches, each item has with our query:\n\n\n\n\n\n\n\n\n\nThe result is a vector of scores that indicate the matches of the query per key. This principle does obviously also work for more than one query by adding more rows to our Query matrix \\(Q\\). This does result in a matrix, in which each row indicates the amount of matching keys for each query:\n\n\n\n\n\n\n\n\n\nInstead of binary indicators, the \\(Q\\) and \\(K\\) matrices in the attention mechanism are filled with floats. This does still result in the same kind of matched-key-result, although the results are now more like degrees of relevance instead of absolute matches:\n\\[\nQ \\times K^T =\n\\]\n\n\n\n\n\n\n\n\n\nAs you can already see in this small example, the values of individual cells can get relatively high compared to the rest of the matrix. As you remember - we want to use this product to rank our values. If these numbers are too large, it might lead to numerical instability or incorrect results. To address this issue, we will scale down the dot-product by dividing it with \\(\\sqrt{d_n}\\), where \\(d_n\\) is the dimension of the aspect space (in our case 5).\n\\[\n\\frac{Q \\times K^T}{\\sqrt{d_n}} =\n\\]\n\n\n\n\n\n\n\n\n\nSince we want to use this matrix for filtering our dataset, we would prefer the weights to sum up to one. To achieve that, we will apply a softmax function on each row of the matrix (remember that the rows currently represent the key-weighted aspects for each query). The resulting matrix with scaled weights for each aspect is then multiplied with the value-matrix that contains one datapoint in each row, described by 5 aspects along the columns.\n\\[\n\\text{softmax}(\\frac{Q \\times K^T}{\\sqrt{d_n}}) \\times V =\n\\]\n\n\n\n\n\n\n\n\n\nThe result is now an attention matrix in the sense that it tells us the importance of each value‚Äôs aspect for our query. In the specific example, the forth value seems to be the most important aspect for our third query. The crucial advantage is, that all aspects of all queries can be simultaneously compared with all aspects of all values without the necessity of sequential processing.\nThough this general idea of weighting aspects in the sense of self-attention4 to process a sequence without disadvantages of the distances of the items was used before (Bahdanau, 2014), the major contribution of the paper was the complete reliance on this mechanism without the need of LSTM/RNN parts. That their suggested architecture works is in part due to the utilisation of multiple self-attention layers, each learning its own weights for \\(Q\\), \\(K\\) and \\(V\\). This allows the model to learn more complex patterns and dependencies between words in a sentence. You can think of it as allowing the model to focus on different parts of the input sequence at different stages of processing. The outputs of the multiple heads are then concatenated and linearly transformed into the final output representation using a series of fully connected feed-forward layers.\n4¬†self in the sense of the model weighting its own embeddings, queries, keys and valuesThis small example is already pretty close to the general attention-mechanism described by Vaswani et al. (2023) (see also Figure¬†2.6), though the actual language model learns its own weights for \\(Q\\), \\(K\\) and \\(V\\).\n\n\n\n\n\n\nFig¬†2.6: Multi-headed attention as depicted in Vaswani et al. (2023)\n\n\n\nInstead of 5x5 matrices, the attenion mechanism as described in the paper implements \\(d_n \\times d_c\\)5 matrices, where \\(d_n\\) is the dimension of the embedding space6 and \\(d_c\\) is the size of the context window. In the original paper, Vaswani et al. (2023) implement the context-window as the same size as the embedding space (i.e., \\(d_n = d_c\\)). In Figure¬†2.7 you can see a brilliant illustration of the multiheaded-attention mechanism at work.\n5¬†\\(\\frac{d_n}{h} \\times \\frac{d_c}{h}\\) actually, the paper used feed-forward layers to reduce the dimensionality of each attention header to reduce the computational cost.6¬†I.e., the dimensionality used to represent each word‚Äôs meaning. In the previous toy-example illustrating the concept of embeddings (Figure¬†2.4), this would be the width of the hidden layer (8). In the case of transformers, this is usually 512 or 1024. These embeddings are learned during training and are a simple transformation of the one-hot vectors returned by the models tokenizer.\n\n\n\n\n\nFig¬†2.7: Illustration of the multi-headed attention mechanism. Taken from Hussain et al. (2024)\n\n\n\nThe implementation of the multi-headed attention mechanism allowed to solve all major issues of the language modelling approaches of the previous generation7. It firstly allows the input of a whole text-sequence at once, rendering the training and inference far speedier then the recursive approaches. Furthermore, the multi-head attention mechanism allows the model to focus on different parts of the input sequence simultaneously, enabling it to capture more complex relationships between words and improve its understanding of context without losing information about long-term dependencies. This mechanism also implicitly solves the bidirectionality-issue since each word can be taken into account when processsing every other word in the sequence.\n7¬†Well, kind of. Transformers are far superior language models due to their ability to parallely process long sequences without issues with stretched context - these advantages come at a price though. GPT-3s training is estimated to have emitted around 502 metric tons of carbon (AIAAIC - ChatGPT training emits 502 metric tons of carbon, n.d.). The computational cost of the architecture as described here does additionally scale quadratically with context window size.The description until now omitted one final but key detail - we only spoke about the weight matrices \\(Q\\), \\(K\\) and \\(V\\). Each of these weight matrices are actually the product of the learned weights and the input vectors. In other words, each of the three matrices is calculated as follows:\n\\[\n\\begin{align}\n    Q &= XW_Q \\\\\n    K &= XW_k \\\\\n    V &= XW_v\n\\end{align}\n\\]\nwhere \\(W_{Q, k, v}\\) are the learned weight matrices and \\(X\\) is the input matrix. This input matrix consists of a) the learned embeddings of the tokenized input-parts and b) the added, so called positional encoding.8\n8¬†While we are talking about omitted details, the whole architecture implements its layers as residual layers. This means that the output of each layer is added to the input of the layer before, before it is passed on to the next layer. But this detail is irrelevant for our understanding of the central mechanism.The positional encoding is a vector that encodes the position of each token in the input sequence. It is added to the embedding of each token to provide the model with information about the order of the tokens in the sequence. The positional encoding is calculated as follows:\n\\[\n\\begin{array}{lcl}\nPE_{(pos, 2i)} &=& sin(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}) \\\\\nPE_{(pos, 2i+1)} &=& cos(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})\n\\end{array}\n\\]\nWhere \\(i\\) is the dimension and \\(pos\\) is the position. Those 2 formulas are not the most intuitive, what they do is to add a unique offset to each embedding though, that allows the model to infer and weigh the token‚Äôs positions in the matrix on it‚Äôs own. Figure¬†2.8 illustrates the pattern this specific combination of sin and cos creates for each sequence-position and embedding-dimension.\n\n\n\n\n\n\n\n\nFig¬†2.8: The positional encoding for 50 dimensions and 512 embedding-dimensions. The x-axis represents the position and the y-axis represents the dimension. The color represents the value of the encoding.\n\n\n\n\n\nThese parts alltogether are all building-blocks of the basic transformer architecture. As you can see in Figure¬†2.9, all parts depicted by Vaswani et al. (2023) are parts we have discussed until now.\n\n\n\n\n\n\nFig¬†2.9: The transformer architecture as depicted in Vaswani et al. (2023)\n\n\n\nThe Encoder half uses the embedding -&gt; encoding -&gt; multi-headed-attention -&gt; feed-forward structure to create a semantic representation of the sequence. The Decoder half uses the same structure, but with an additional masked multi-head attention layer to prevent the model from looking at future tokens. This is necessary because we want to generate a sequence token by token.\nFigure¬†2.10, taken from Kaplan et al. (2020), shows the test performance of Transformer models compared to LSTM-based models as a function of model size and context length. Transformers outperform LSTMs with increasing context length.\n\n\n\n\n\n\nFig¬†2.10: Comparison of Transformer- and LSTM-performance based on Model size and context length. Taken from Kaplan et al. (2020)\n\n\n\nFurthermore, Kaplan et al. (2020) and Hoffmann et al. (2022) after them postulated performace power-laws (see also Figure¬†2.11) that suggest that the performance of a Transformer directly scales with the models size and data availability. Though the task of prediction of natural language poses a non-zero limit to the performance, it is suggested that this limit is not reached for any of the currently available models.9\n9¬†Incidentally, we might run out of data to train on before reaching that limit (Villalobos et al., 2024).\n\n\n\n\n\nFig¬†2.11: Performance power law for transformer models. Taken from Kaplan et al. (2020)\n\n\n\nThe advances made through leveraging transformer-based architectures for language modelling led to a family of general-purpose language models. Unlike the approaches before, these models were not trained for a specific task but rather on a general text base with the intention of allowing specific fine-tuning to adapt to a task. Classic examples of these early general-purpose natural language generating Transformer models are the Generative Pre-trained Transformer (the predecessor of ChatGPT you all know), first described in Radford et al. (2018), and the ‚ÄúBidirectional Encoder Representations from Transformers‚Äù (BERT) architecture and training procedure, described by Devlin et al. (2019).\nThis general-purpose architecture is the base of modern LLMs as we know them today and most applications we will discuss in this course.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Getting started with (L)LMs</span>"
    ]
  },
  {
    "objectID": "content/getting_started_with_llms.html#choosing-open-source-models",
    "href": "content/getting_started_with_llms.html#choosing-open-source-models",
    "title": "Getting started with (L)LMs",
    "section": "Choosing open source models",
    "text": "Choosing open source models\nThe 2023 release of ChatGPT by OpenAI has sparked a lot of interest in large language models (LLMs) and their capabilities. This has also led to an increase in the number of available open-source LLMs. The selection of a model for your application is always a trade-off between performance, size, and computational requirements.\nAlthough Kaplan et al. (2020) showed a relationship between performance and model-size, the resources available will most probably limit you to smaller models. Additionally, a lot of tasks can be solved by smaller models if they are appropriately fine-tuned (Hsieh et al., 2023).\nA good idea when choosing an open source model is to start small and test whether the performace is sufficient for your use case. If not, you can always try a larger model later on.\nAdditionally, it is good practice to check the license of the model you want to use. Some models are only available under a non-commercial license, which means that you cannot use them for commercial purposes.\nThirdly, you should make sure that the model you choose is appropriate for your use case. For example, if you want to use a model for text generation, you should make sure that it was trained on a dataset that is similar to the data you will be using. If you want to use a model for translation, you should make sure that it was trained on a dataset that includes the languages you are interested in. A lot of usecases do already have benchmark datasets that can be used to pit models against each other and evaluate there appropriateness for a given use case based on a few key metrics.\nA good starting point for getting an overview about such metrics and benchmarks is Hugging Face. This platform has long cemented itself as the go-to place for getting access to open source models, but also provides a lot of resources for evaluating and comparing them. This page provides an overview of benchmarks, leaderboards and comparisons for a variety of tasks.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Getting started with (L)LMs</span>"
    ]
  },
  {
    "objectID": "content/getting_started_with_llms.html#basics-of-using-open-source-models",
    "href": "content/getting_started_with_llms.html#basics-of-using-open-source-models",
    "title": "Getting started with (L)LMs",
    "section": "Basics of using open source models",
    "text": "Basics of using open source models\n\n\n\n\n\n\nüìù Task\n\n\n\nNow it is your turn! In your project-groups, you will each have to build a small ‚ÄúHello World‚Äù-style application that uses an open source model.\n\nChoose a small model using the sources we discussed before.\nEach group is to use one of the following frameworks\n\nHuggingface\nOllama\nLM-Studio from python\nLlama.cpp to load and use the model in your application.\n\nPresent your results and your experiences with the frameworks in a short presentation.\nSubmit your code and report on moodle.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Getting started with (L)LMs</span>"
    ]
  },
  {
    "objectID": "content/getting_started_with_llms.html#further-readings",
    "href": "content/getting_started_with_llms.html#further-readings",
    "title": "Getting started with (L)LMs",
    "section": "Further Readings",
    "text": "Further Readings\n\nThis quite high-level blog-article about foundational models by Heidloff (2023)\nThe Attention is all you need-paper (Vaswani et al., 2023) and the brilliant video discussing it by Umar Jamil (Vaswani et al., 2023)\nThis very good answer on stack exchange that explains the attention-concept ((https://stats.stackexchange.com/users/95569/dontloo), n.d.)",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Getting started with (L)LMs</span>"
    ]
  },
  {
    "objectID": "content/getting_started_with_llms.html#references",
    "href": "content/getting_started_with_llms.html#references",
    "title": "Getting started with (L)LMs",
    "section": "References",
    "text": "References\n\n\n\n\nAIAAIC - ChatGPT training emits 502 metric tons of carbon. (n.d.). https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chatgpt-training-emits-502-metric-tons-of-carbon.\n\n\nBahdanau, D. (2014). Neural machine translation by jointly learning to align and translate. arXiv Preprint arXiv:1409.0473. https://arxiv.org/abs/1409.0473\n\n\nBojanowski, P., Grave, E., Joulin, A., & Mikolov, T. (2017). Enriching Word Vectors with Subword Information (arXiv:1607.04606). arXiv. https://doi.org/10.48550/arXiv.1607.04606\n\n\nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (arXiv:1810.04805). arXiv. https://doi.org/10.48550/arXiv.1810.04805\n\n\nHeidloff, N. (2023). Foundation Models, Transformers, BERT and GPT. In Niklas Heidloff. https://heidloff.net/article/foundation-models-transformers-bert-and-gpt/.\n\n\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. de L., Hendricks, L. A., Welbl, J., Clark, A., Hennigan, T., Noland, E., Millican, K., Driessche, G. van den, Damoc, B., Guy, A., Osindero, S., Simonyan, K., Elsen, E., ‚Ä¶ Sifre, L. (2022). Training Compute-Optimal Large Language Models (arXiv:2203.15556). arXiv. https://doi.org/10.48550/arXiv.2203.15556\n\n\nHsieh, C.-Y., Li, C.-L., Yeh, C.-K., Nakhost, H., Fujii, Y., Ratner, A., Krishna, R., Lee, C.-Y., & Pfister, T. (2023). Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes (arXiv:2305.02301). arXiv. https://doi.org/10.48550/arXiv.2305.02301\n\n\n(https://stats.stackexchange.com/users/95569/dontloo), dontloo. (n.d.). What exactly are keys, queries, and values in attention mechanisms? Cross Validated.\n\n\nHussain, Z., Binz, M., Mata, R., & Wulff, D. U. (2024). A tutorial on open-source large language models for behavioral science. Behavior Research Methods, 56(8), 8214‚Äì8237. https://doi.org/10.3758/s13428-024-02455-8\n\n\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., & Amodei, D. (2020). Scaling Laws for Neural Language Models (arXiv:2001.08361). arXiv. https://doi.org/10.48550/arXiv.2001.08361\n\n\nMikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space (arXiv:1301.3781). arXiv. https://doi.org/10.48550/arXiv.1301.3781\n\n\nPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., & Zettlemoyer, L. (2018). Deep contextualized word representations (arXiv:1802.05365). arXiv. https://doi.org/10.48550/arXiv.1802.05365\n\n\nRadford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). Improving language understanding with unsupervised learning.\n\n\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2023). Attention Is All You Need (arXiv:1706.03762). arXiv. https://doi.org/10.48550/arXiv.1706.03762\n\n\nVillalobos, P., Ho, A., Sevilla, J., Besiroglu, T., Heim, L., & Hobbhahn, M. (2024, June). Position: Will we run out of data? Limits of LLM scaling based on human-generated data. Forty-First International Conference on Machine Learning.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Getting started with (L)LMs</span>"
    ]
  },
  {
    "objectID": "content/prompting.html",
    "href": "content/prompting.html",
    "title": "Prompting",
    "section": "",
    "text": "Instruct-tuned models\nPrompting describes the utilization of the ability of language models to use zero or few-shot instrutions to perform a task. This ability, which we briefly touched on when we were discussing the history of language models (i.e., the paper by Radford et al. (2019)), is one of the most important aspects of modern large language models.\nPrompting can be used for various tasks such as text generation, summarization, question answering, and many more.\nInstruct-tuned models are trained on a dataset (for an example, see Figure¬†3.1) that consists of instructions and their corresponding outputs. This is different from the pretraining phase of language models where they were trained on large amounts of text data without any specific task in mind. The goal of instruct-tuning is to make the model better at following instructions and generating more accurate and relevant outputs.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Prompting</span>"
    ]
  },
  {
    "objectID": "content/prompting.html#instruct-tuned-models",
    "href": "content/prompting.html#instruct-tuned-models",
    "title": "Prompting",
    "section": "",
    "text": "Fig¬†3.1: An example for a dataset that can be used for instruct-finetuning. This dataset can be found on huggingface\n\n\n\n\n\n\n\n\n\nüìù Task\n\n\n\nTest the difference between instruct and non-instruct-models.\nDo this by trying to get a gpt2-version (i.e., ‚ÄúQuantFactory/gpt2-xl-GGUF‚Äù) and a small Llama 3.2 Instruct-Model (i.e., ‚Äúhugging-quants/Llama-3.2-1B-Instruct-Q8_0-GGUF‚Äù to write a small poem about the inception of the field of language modelling.\nUse LM-Studio to test this.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) A poem written by Llama 3.2 1B - a model with Instruct-Finetuning\n\n\n\n\n\n\n\n\n\n\n\n(b) A ‚Äúpoem‚Äù written by GPT2 - a model without Instruct-Finetuning\n\n\n\n\n\n\n\nFig¬†3.2: A poem and a ‚Äúpoem‚Äù\n\n\n\n\n\nShow answer",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Prompting</span>"
    ]
  },
  {
    "objectID": "content/prompting.html#prompting-strategies",
    "href": "content/prompting.html#prompting-strategies",
    "title": "Prompting",
    "section": "Prompting strategies",
    "text": "Prompting strategies\nThe results of a prompted call to a LM is highly dependent on the exact wording of the prompt. This is especially true for more complex tasks, where the model needs to perform multiple steps in order to solve the task. It is not for naught that the field of ‚Äúprompt engineering‚Äù has emerged. There is a veritable plethora of resources available online that discuss different strategies for prompting LMs. It has to be said though, that the strategies that work and don‚Äôt work can vary greatly between models and tasks. A bit of general advice that holds true for nearly all models though, is to\n\ndefine the task in as many small steps as possible\nto be as literal and descriptive as possible and\nto provide examples if possible.\n\nSince the quality of results is so highly dependent on the chosen model, it is good practice to test candidate strategies against each other and therefore to define a target on which the quality of results can be evaluated. One example for such a target could be a benchmark dataset that contains multiple examples of the task at hand.\n\n\n\n\n\n\n\nüìù Task\n\n\n\n1. Test the above-mentioned prompting strategies on the MTOP Intent Dataset and evaluate the results against each other. The dataset contains instructions and labels indicating on which task the instruction was intended to prompt. Use a python script to call one of the following three models in LM-Studio for this:\n\nPhi 3.1 mini\nGemma 2 2B\nLlama 3.2 1B\n\nUse the F1-score implemented in scikit learn to evaluate your results.\n2. You do sometimes read very specific tips on how to improve your results. Here are three, that you can find from time to time:\n\nDo promise rewards (i.e., monetary tips) instead of threatening punishments\nDo formulate using affirmation (‚ÄúDo the task‚Äù) instead of negating behaviours to be avoided (‚ÄúDon‚Äôt do this mistake‚Äù)\nLet the model reason about the problem before giving an answer\n\nCheck these strategies on whether they improve your results. If your first instruction already results in near-perfect classification, brainstorm a difficult task that you can validate qualitatively. Let the model write a recipe or describe Kiel for example.\n3. Present your results\n3. Upload your code to moodle",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Prompting</span>"
    ]
  },
  {
    "objectID": "content/prompting.html#generation-of-synthetic-texts",
    "href": "content/prompting.html#generation-of-synthetic-texts",
    "title": "Prompting",
    "section": "Generation of synthetic texts",
    "text": "Generation of synthetic texts\nAs we discussed before, small models can perform on an acceptable level, if they are finetuned appropriately.\nA good way to do this is to use a larger model to generate synthetic data that you then use for training the smaller model. This approach has been used successfully in many applications, for example for improving graph-database queries (Zhong et al., 2024), for improving dataset search (Silva & Barbosa, 2024) or the generation of spreadsheet-formulas (Singh et al., 2024).\nSince even the largest LLMs are not perfect in general and might be even worse on some specific niche tasks, evidence suggests that a validation strategy for data generated in this way is beneficial (Kumar et al., 2024; Singh et al., 2024).\nStrategies to validate the synthetic data include:\n\nUsing a human annotator to label part of the data to test the models output\nForcing the model to answer in a structured way that is automatically testable (e.g., by using JSON)\nForcing the model to return 2 or more answers and checking for consistency\nCombining the two approaches above (i.e., forcing the model to return multiple structured outputs (JSON, XML, YAML, ‚Ä¶) and checking for consistency)\nUsing a second LLM/different prompt to rate the answers\n\n\n\n\n\n\n\nüìù Task\n\n\n\nUsing your script for batch-testing different prompts, generate synthetic data for a emotion detection task based on Paul Ekman‚Äôs six basic emotions: anger, disgust, fear, happiness, sadness and surprise1.\nThe generated data should consist of a sentence and the emotion that is expressed in it. Start by generating two examples for each emotion. Validate these results and adapt them if necessary. Then use these examples to generate 100 samples for each emotion.\nUse one of the above mentioned (non-manual) strategies to validate the data you generated.\nUpload your results to Moodle.\n\n\n1¬†Though this nomenclature has fallen a bit out of fashion",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Prompting</span>"
    ]
  },
  {
    "objectID": "content/prompting.html#temperature",
    "href": "content/prompting.html#temperature",
    "title": "Prompting",
    "section": "Temperature",
    "text": "Temperature\nYou might have encountered eerily similar answers from the language model, especially in the last task. Talking of it - why does the model return different answers to the same prompt at all if we do use pretrained-models in the first place? Shouldn‚Äôt the utilization of the frozen weight-matrix result in the same answer, every time we run the model with the same input?\nYes, it should. And it does.\nRemember that a language model trained on language generation as we discussed in the first session ends in a softmax-layer that returns probabilities for each token in the vocabulary. The generation-pipeline does not just use the token with the highest probability though, but samples from this distribution. This means, that even if the input is identical, the output will be different every time you run the model.\nThe temperature parameter controls the steepness of the softmax-function and thus the randomness of the sampling process. A higher temperature value results in more random outputs, while a lower temperature value results in more ‚Äúdeterministic‚Äù outputs. The temperatur, indicated as a float between 0 and 1, is used to modulate the probabilities of the next token. This is done by adding a \\(\\frac{1}{Temp}\\) factor to the model-outputs before applying the softmax.\nThis effectively changes the Sofmax-fomula from\n\\[\np_{Token} = \\frac{e^{z_{Token}}}{\\sum_{i=1}^k e^{z_{i}}}\n\\]\nto \\[\np_{Token}(Temp) = \\frac{e^{\\frac{z_{Token}}{Temp}}}{\\sum_{i=1}^k e ^{\\frac{z_{i}}{Temp}}}\n\\]\nWhere\n\n\\(z_{Token}\\) is the output for a given token\n\\(k\\) is the size of the vocabulary\n\\(Temp\\) is the temperature parameter (0 &lt; \\(Temp\\) &lt;= 1)\n\nThe effect of this temperature can be seen in Figure¬†3.3.\n\n\n\n\n\n\n\n\nFig¬†3.3: The effect of the temperature parameter on the softmax-output for a given input. The x-axis represents the temperature, the y-axis represents the token-position and the color represents the probability of the token.\n\n\n\n\n\nMost generation-frameworks do additionally provide a parameter called top_k or top_p. These parameters are used to limit the number of tokens that can be selected as the next token. This is done by sorting the probabilities in descending order and only considering the top k tokens or the top p percent of tokens.\nTemperature is the mayor setting to controll a LLMs ‚Äúcreativity‚Äù though.\n\n\n\n\n\n\nüìù Task\n\n\n\nUsing the script provided for generating snthetic data, test the effect of the temperature parameter on the output of the model.\n\nUse the same prompt and the same model\nRun the model with a temperature value of 0.1, 0.5, 1.0 and 2.0",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Prompting</span>"
    ]
  },
  {
    "objectID": "content/prompting.html#further-readings",
    "href": "content/prompting.html#further-readings",
    "title": "Prompting",
    "section": "Further Readings",
    "text": "Further Readings\n\nThis prompting-guide has some nice general advice\nOpenAI has its own set of tipps\ndeepset, the company behind Haystack, has a nice guide as well\nThis blog-article, again written by Heidloff (Heidloff, 2023)",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Prompting</span>"
    ]
  },
  {
    "objectID": "content/prompting.html#references",
    "href": "content/prompting.html#references",
    "title": "Prompting",
    "section": "References",
    "text": "References\n\n\n\n\nHeidloff, N. (2023). Fine-tuning small LLMs with Output from large LLMs. In Niklas Heidloff. https://heidloff.net/article/fine-tune-small-llm-with-big-llm/.\n\n\nKumar, B., Amar, J., Yang, E., Li, N., & Jia, Y. (2024). Selective Fine-tuning on LLM-labeled Data May Reduce Reliance on Human Annotation: A Case Study Using Schedule-of-Event Table Detection (arXiv:2405.06093). arXiv. https://doi.org/10.48550/arXiv.2405.06093\n\n\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language models are unsupervised multitask learners. OpenAI Blog, 1(8), 9.\n\n\nSilva, L., & Barbosa, L. (2024). Improving dense retrieval models with LLM augmented data for dataset search. Knowledge-Based Systems, 294, 111740. https://doi.org/10.1016/j.knosys.2024.111740\n\n\nSingh, U., Cambronero, J., Gulwani, S., Kanade, A., Khatry, A., Le, V., Singh, M., & Verbruggen, G. (2024). An Empirical Study of Validating Synthetic Data for Formula Generation (arXiv:2407.10657). arXiv. https://doi.org/10.48550/arXiv.2407.10657\n\n\nZhong, Z., Zhong, L., Sun, Z., Jin, Q., Qin, Z., & Zhang, X. (2024). SyntheT2C: Generating Synthetic Data for Fine-Tuning Large Language Models on the Text2Cypher Task (arXiv:2406.10710). arXiv. https://doi.org/10.48550/arXiv.2406.10710",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Prompting</span>"
    ]
  },
  {
    "objectID": "content/agent_basics.html",
    "href": "content/agent_basics.html",
    "title": "Agent basics",
    "section": "",
    "text": "What is an agent?\n‚ÄúAn AI agent is a system that uses an LLM to decide the control flow of an application.‚Äù (‚ÄúWhat Is an AI Agent?‚Äù 2024)\nIn the context of large language models, agents are LLM-based systems that can solve complex tasks. Imagine asking a question like:\n‚ÄúWhat were the key learnings from the Generative AI elective module in WiSe 24/25 at FH Kiel?‚Äù\nCould you just ask an LLM that question and expect a correct answer?\nIt is in theory possible, that an LLM could answer that directly, but only if it was trained on this information, that is, if a text describing the module exists, is accessible from the web and was used in training the model. However, usually we can not expect the LLM to have this knowledge.\nLet‚Äôs think for a moment how a human would answer that (one that did not attend the module). We would probably try to get a copy of the script, maybe we saved the script to our hard drive or other data storage. Maybe we could search the web for a description or text version of the module. Having obtained a copy of the script, we would probably read it. Then, we would try to distill the information hidden therein, to answer the question.\nSo, for our LLM to answer that question, it needs to be able to perform several tasks:\nThis is where agents come into play. Agents are LLM-based systems that can solve complex tasks by performing several subtasks in sequence, using an LLM to decide which subtask to perform next. In our example, the agent would first search the web for relevant documents, then read and understand them, summarize them and finally answer the question based on the summary.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Agent basics</span>"
    ]
  },
  {
    "objectID": "content/agent_basics.html#what-is-an-agent",
    "href": "content/agent_basics.html#what-is-an-agent",
    "title": "Agent basics",
    "section": "",
    "text": "Searching the web for relevant documents\nsearching in a local file storage or other database\nReading and understanding a document\nSummarizing the content of a document\nAnswering questions based on the summary of a document",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Agent basics</span>"
    ]
  },
  {
    "objectID": "content/agent_basics.html#agent-framework",
    "href": "content/agent_basics.html#agent-framework",
    "title": "Agent basics",
    "section": "Agent framework",
    "text": "Agent framework\n\n\n\nArchitecture of the agent framework (LLM Agents ‚Äì Nextra, 2024)\n\n\nTo facilitate this, an agent system consists of several components:\n\nAgent: the agent core acting as coordinator\nPlanning: Assists the agent in breaking down the complex task into subtasks\nTools: functions that the agent can use to perform a specific task\nMemory: used to store information about previous interactions with the agent\n\nWe will describe each of them below.\n\nAgent\nThis is a general-purpose LLM, that functions as the brain and main decision-making component of an agent. It determines which tools to use and how to combine their results to solve complex tasks. The agent core uses the output of the previous tool as input for the next tool. It also uses an LLM to decide when to stop using tools and return a final answer. The behavior of the agent and the tools, it has at its disposal, is defined by a prompt template.\n\n\nPlanning\nPlanning is the process of breaking down a complex task into subtasks and deciding which tools to use for each subtask. The planning module is usually also an LLM, it can be one fine-tuned to this specific task or receive a specialized prompt. It uses techniques like chain-of-thought (CoT) prompting to generate a plan of action (weiChainThoughtPromptingElicits2023?). CoT prompting is a technique that encourages the model to explain its reasoning step by step, making it easier for us to understand and evaluate its answers. Other strategies include Tree-of-Thoughts (Long (2023), Yao, Yu, et al. (2023)), ReAct (Yao, Zhao, et al., 2023) and Reflexion (Shinn et al., 2023). \n\n\nTools\nTools are functions that the agent can use to perform a specific task. They can be pre-defined or dynamically generated based on the user‚Äôs needs. Tools can be simple, such as a calculator, or complex, such as a web search engine. Tools can also be other agents, allowing for the creation of multi-agent systems. In our example, the tools would be a web search engine and a document reader. Other popular tools are a data store or a python interpreter.\n\n\nMemory\nMemory is used to store information about previous interactions with the agent. This allows the agent to remember past conversations and use this information in future interactions. Memory can be short-term, such as a conversation buffer, or long-term, such as a database. Memory can also be used to store the results of previous tool uses, allowing the agent to reuse them if necessary.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Agent basics</span>"
    ]
  },
  {
    "objectID": "content/agent_basics.html#examples-of-agent-frameworks-llamaindex-langchain-haystack",
    "href": "content/agent_basics.html#examples-of-agent-frameworks-llamaindex-langchain-haystack",
    "title": "Agent basics",
    "section": "Examples of agent-frameworks (Llamaindex, LangChain & Haystack)",
    "text": "Examples of agent-frameworks (Llamaindex, LangChain & Haystack)\nThere are a lot of agent frameworks out there. In this module we will focus on three of them: LlamaIndex, LangChain and Haystack. They all have their own strengths and weaknesses, but they all share the same basic architecture as described above. We will describe each of them below.\n\nLlamaindex: LlamaIndex is a data framework for your LLM applications. It provides a central interface to connect your LLMs and your data. It also provides a set of tools to help you build your own applications, such as a document reader, a web search engine, a data store, etc.\nLangChain: LangChain is a framework for developing applications powered by language models. It provides a set of tools to help you build your own applications, such as a document reader, a web search engine, a data store, etc. It also provides a set of agents that can use these tools to solve complex tasks.\nHaystack: Haystack is an open source NLP framework that enables you to build production-ready applications around LLMs and other models. It provides a set of tools to help you build your own applications, such as a document reader, a web search engine, a data store, etc. It also provides a set of agents that can use these tools to solve complex tasks.\n\n\n\n\n\n\n\n\nüìù Task\n\n\n\nNow it is your turn!\nEach group is to use one of the following frameworks to build a small demo agent:\n\nLlamaindex\nLangchain\nHaystack\n(optional) another framework of your choice\n\n\nSet up a local LLM (e.g.¬†using Ollama or LM Studio) to be used by the agent.\nChoose a small task for your agent, e.g.¬†answering questions about a specific topic, summarizing a document, etc. (use the one in the respective tutorial)\nImplement the agent using one of the frameworks listed above.\nPresent your results and your experiences with the frameworks in a short presentation.\nSubmit your code and report on moodle.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Agent basics</span>"
    ]
  },
  {
    "objectID": "content/agent_basics.html#further-readings",
    "href": "content/agent_basics.html#further-readings",
    "title": "Agent basics",
    "section": "Further Readings",
    "text": "Further Readings\n\nThis paper compares different planning strategies\nIn addition to the websites listed above see also (‚ÄúIntroduction to LLM Agents,‚Äù 2023)",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Agent basics</span>"
    ]
  },
  {
    "objectID": "content/agent_basics.html#references",
    "href": "content/agent_basics.html#references",
    "title": "Agent basics",
    "section": "References",
    "text": "References\n\n\n\n\nIntroduction to LLM Agents. (2023). In NVIDIA Technical Blog. https://developer.nvidia.com/blog/introduction-to-llm-agents/.\n\n\nLLM Agents ‚Äì Nextra. (2024). https://www.promptingguide.ai/research/llm-agents.\n\n\nLong, J. (2023). Large Language Model Guided Tree-of-Thought (arXiv:2305.08291). arXiv. https://arxiv.org/abs/2305.08291\n\n\nShinn, N., Cassano, F., Berman, E., Gopinath, A., Narasimhan, K., & Yao, S. (2023). Reflexion: Language Agents with Verbal Reinforcement Learning (arXiv:2303.11366). arXiv. https://doi.org/10.48550/arXiv.2303.11366\n\n\nWhat is an AI agent? (2024). In LangChain Blog. https://blog.langchain.dev/what-is-an-agent/.\n\n\nYao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y., & Narasimhan, K. (2023). Tree of Thoughts: Deliberate Problem Solving with Large Language Models (arXiv:2305.10601). arXiv. https://arxiv.org/abs/2305.10601\n\n\nYao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., & Cao, Y. (2023). ReAct: Synergizing Reasoning and Acting in Language Models (arXiv:2210.03629). arXiv. https://doi.org/10.48550/arXiv.2210.03629",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Agent basics</span>"
    ]
  },
  {
    "objectID": "content/embeddings.html",
    "href": "content/embeddings.html",
    "title": "Embedding-based agent-systems",
    "section": "",
    "text": "Semantic embeddings and vector stores\nAll agents we discussed until here are using tools that allow them to use their generated inputs in some way. In most of the task we want to utilize agents, we do not only want to generate text but to also inform the generation based on some kind of existing knowledge base. Examples for these kinds of usecases include:\nThough most modern LLMs are increasingly capable in answering basic knowledge-questions, the more comples a topic or the more relevant the factual basis of an answer is, the more it is important to base generated answers on actual data.\nTo empower an agent too look up information during its thought-process, one has to build a tool that allows an agent to use natural language to retrieve information necessary for a task. The fundamental principle to do this are so-called semantic embeddings. These are pretty close to the concept we introduced when talking about the foundations of LLMs (see here) and can be understood as a way to map textual data into a vector space. The main idea is that semantically similar texts should have similar embeddings, i.e., they are close in the vector space. Close in this context is meant as having a reasonibly small distance between them. The go-to standard to measure this distance is the cosine similarity, which has proven usefull enough to be the standard for a range of semantic retrieval implementations (i.e., they are used in OpenAI tutorials and in Azure embedding-applications). The cosine similarity is defined as:\n\\[\n\\text{cosine\\_similarity}(u, v) = \\frac{u \\cdot v}{\\|u\\| \\|v\\|} = \\frac{\\sum_{i=1}^{n} u_i v_i}{\\sqrt{\\sum_{i=1}^{n} u_i^2} \\sqrt{\\sum_{i=1}^{n} v_i^2}}\n\\] The rationale here is that sequences with semantically similar contents should point to similar directions in the high dimensional vector space. See Figure¬†5.1 for an illustration of this and other common similarity concepts seen in semantic retrieval.\n(a) Illustration of ‚Äúsemantic embeddings‚Äù of different word.\n\n\n\n\n\n\n\n\n\n\n\n(b) Illustration of 4 common similarity concepts seen in semantic retrieval: cosine, euclidean, dot product and manhattan. dot product and cosine are taking the direction of the vector into account, while the cosine ignores the length of the vectors and the dot product does not. Manhattan and euclidean are both measuring the distance between two points in a vector space, but they do it differently. Euclidean is the straight line between two points, while manhattan is the sum of the absolute differences between the coordinates of the two points.\n\n\n\n\n\n\nFig¬†5.1: Illustration of common similarity metrics in semantic search.\nAs always, there is not the one solution to all problems though and the applicability of cosine similarity might not be optimal for you usecase (Goyal & Sharma, 2022; Steck et al., 2024).\nThough one could use any kind of (L)LM to calculate embeddings for this case1, it is advisable to use models specifically trained for this purpose. Reimers & Gurevych (2019) proposed Sentence-BERT which is a simple but effective approach to calculate semantic embeddings. SBERT and similar approaches are based on a (L)LM that was trained to predict missing words as we discussed before, resulting in a general representation of natural language. In the case of the original paper, they used (among others) the BERT model Devlin et al. (2019) mentioned before.\nThe authors then use this to embed a pair of sentences into one embedding-vector each2, for which some measure of semantic similarity is known. An example for a dataset containing such sentences is the Stanford Natural Language Inferenc(SNLI) corpus Bowman et al. (2015) which labels 550k pairs of sentences as either entailment, contradiction or neutral. Reimers & Gurevych (2019) then concated the both senteces embeddings and their element-wise difference into a single vector which is fed to a multiclass classifier, indicating in which category the sentences relationship falls. At inference, this classification head was removed and replaced as the cosine similarity as discussed above. The resulting network is highly effective in calculating semantic similarities between sentences.\nA look at the sbert-website shows that the module has somewhat grown and now does supply a series of learning paradigms that can be used to efficiently tune a model for your specific usecase3. As the library has grown, so has the sheer amount of pretrained embedding-models in some way based on this architecture that are hosted on huggingface. The MTEB-Leaderboard is a good strat to search for a model for your application. One utilization of this model-family, which has already been implicitly used in this script, is their very efficient ability to semantically search for documents. If a model is very good at finding similar sentences, it can also be very good to find documents that are very similar to a question.\nLook at the example illustrated in Figure¬†5.2. The question ‚Äúwhy is the sky blue‚Äù embedded with the same model as our 5 documents stating some facts.\nWe can then calculate the cosine-similarity between these embeddings and return the document, that has the highest similarity to our question.\nThis approach of using a model to embed documents and questions into a vector space is the basis for the so-called Retrieval augmented generation.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Embedding-based agent-systems</span>"
    ]
  },
  {
    "objectID": "content/embeddings.html#semantic-embeddings-and-vector-stores",
    "href": "content/embeddings.html#semantic-embeddings-and-vector-stores",
    "title": "Embedding-based agent-systems",
    "section": "",
    "text": "1¬†And there are approaches to use LLMs to solve this taks i.e., Jiang et al. (2023)2¬†The original BERT-paper did this by adding a pooling layer before the task-header that extracted and weighed the context-dependend embedding of the first token. The SBERT paper tried different pooling-strategies and used a mean over each embedding dimension of the sequence.\n\n3¬†And this does not have to be expensive. Tunstall et al. (2022) have shown a highly efficient contrastive learning paradigm that limts the amount of necessary labels for a ridiuculously small amount of labels.\n\n\n\n\n\n\nFig¬†5.2\n\n\n\n\n\n\n\n\n\n\nüìù Task\n\n\n\nInstall the sentence-transformer package and download the climate_fever-dataset.\nChoose one model from the MTEB-Leaderboard that you deem adequatly sized and appropriate for the task\nTest the different metrics for the first twenty claims of the dataset and a question you formulate.\nUse the similarity-implementations from sklearn.metrics.pairwise.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Embedding-based agent-systems</span>"
    ]
  },
  {
    "objectID": "content/embeddings.html#retrieval-augmented-generation",
    "href": "content/embeddings.html#retrieval-augmented-generation",
    "title": "Embedding-based agent-systems",
    "section": "Retrieval augmented generation",
    "text": "Retrieval augmented generation\nRetrieval augmented generation (RAG) is a framework that does pretty much do what it says on the tin. You use a retrieval model to find documents that are similar to your question and then either return these documents our feed them into a generative model, which then generates an answer based on these documents. This process can additionally be wrapped as a tool to be used by an agent, so that your existing agent can now also use external knowledge sources to answer questions.\nRetrieval does not have to be semantics-based in this context - all kinds of data sources and databases can be made accessible for a LLM - we will focus on a purely embbedding based approach here though.\nAlthough the small example in the last task was working, it is not really scalable. It was fine for a limited set of examples, if you want to realistically make a whole knowledge base searchable, you need to use an appropriate database system.\n\nVector databases\nA vector database is a database that stores vectors and allows for efficient similarity searches. As can be seen in the db-engines ranking there has been a surge of interest in this area recently, with many new players entering the market. From the plethora of vector databases, these three are examples that virtue a honorary mention:\n\nChroma - a in-memory database for small applications that is especially easy to get to run.\nElasticsearch - a well established database that is the go to system for open source search engines and has recently (and kind of naturally) also branche out into vector databases.\nQdrant - the product of a Berlin-based startup that focusses on stability and scalability. It can also run in memory, but does natively support hard drive storage.\n\nThe best way to use qdrant is to use docker to run it and the python sdk to interact with it.\n\n\n\n\n\n\nüìù Task\n\n\n\nInstall qdrant and the python sdk.\nCreate a collection for the claims and one for the evidence in the climate_fever-dataset. Add the first 200 entries to each of these collections.\nTest the similarity search on a question you formulate.\n\n\n\n\nRAG\nThe last step to make this into a RAG pipeline is to use a generative model to answer the question based on the retrieved documents.\nMost agent frameworks provide integrations for a variety of vector databases.\nIn terms of haystack, there are not just one but two tutorials on how to get qdrant to integrate into your pipeline, one from qdrant for general integration and one from haystack for sparse vectors (though the procedure shown should also work for our dense case).\n\n\n\n\n\n\nüìù Task\n\n\n\nBuild a haystack pipeline that allows you to chat with the climate_fever evidence.\nThis tutorial might be helpful in approaching that task.\n\n\n\n\nDocument chunking\nThe examples we looked at until now were all working with short text-snippets that comforably fit into the context window of a LLM. If you think about usual usecases for RAG-systems, this is not the most common case though. Usually, you will have a base of documents that can span multiple 1000‚Äôs of tokens and you want to be able to answer questions about these documents. Furthermore, you do not only want to know which document might be relevant, but ideally also which part of the document matches your question best.\nThis is where the process of doctument chunking or document splitting comes into play. There is a series of possible approaches to split a document, the most common, so called naive chunking method, is to use a structural element of the document though. This means that you parse the documents into sentences, paragraphs or pages and then use these as chunks that you individually embed and store in your vector database. To prevent loss of relevant context when splitting a document into chunks, it is additionally common to add some overlap between the chunks. This tries to solve the lost context problem, does however create reduncencies in the data.\nAn alternative approach is to use semantic chunking. This means that you split a document into chunks based on their meaning. Jina.ai explained in a blogpost (Late Chunking in Long-Context Embedding Models, 2024) their so called ‚Äúlate chunking‚Äù method. which iteratively runs the whole document through the attention head of the transformer to gain embeddings per token, and then averages these embeddings per naive chunk. This way, the chunks are still structure based but contain semantic information about the whole context. Haystack does not implement this feature yet, though it is planned.\nAnother approach to semantic chunking is descirbed on the doc-pages of LlamaIndex. In their approach to semantic chunking, an adaptive splitting-rule is used, that splits the documents based on semantic similarity of sentences. This means that sentences that are semantically similar are grouped together into chunks.\n\n\n\n\n\n\nüìù Task\n\n\n\nImplement a document chunking strategy for a book of your choice from the project_gutenberg dataset.\nYou can use any approach you like, but you should explain your choice and why it is appropriate for this dataset.\n\n\n\n\nQuery Expansion\nUntil now, we have based our retrieval on the assumption, that the question the user formulates is a good representation of their information need. This is not always the case though. Often, users do not know what they are looking for or they use synonyms or paraphrases that are not present in the documents. If the question is not formulated well, or if it is too specific, the system might not be able to find relevant documents. To improve the quality of the questions, we can use query expansion. This means that we take the original question and expand it with additional information to make it more specific and to increase the chances of finding relevant documents. This can be done in multiple ways, one common approach is to use a generative model to generate multiple queries based on the original question. Another approach is to use a keyword extraction algorithm to extract keywords from the question and then use these keywords to expand the query.\nHaystack provides an implementation of query expansion using a custom pipeline component.\n\n\n\n\n\n\nüìù Task\n\n\n\nImplement query expansion for the climate_fever dataset using Haystack.\nExperiment with different prompts and temperatures.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Embedding-based agent-systems</span>"
    ]
  },
  {
    "objectID": "content/embeddings.html#further-readings",
    "href": "content/embeddings.html#further-readings",
    "title": "Embedding-based agent-systems",
    "section": "Further Readings",
    "text": "Further Readings\n\nThis blogpost by DeepSet gives a good overview of the concept of RAG",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Embedding-based agent-systems</span>"
    ]
  },
  {
    "objectID": "content/embeddings.html#references",
    "href": "content/embeddings.html#references",
    "title": "Embedding-based agent-systems",
    "section": "References",
    "text": "References\n\n\n\n\nBowman, S. R., Angeli, G., Potts, C., & Manning, C. D. (2015). A large annotated corpus for learning natural language inference. In L. M√†rquez, C. Callison-Burch, & J. Su (Eds.), Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (pp. 632‚Äì642). Association for Computational Linguistics. https://doi.org/10.18653/v1/D15-1075\n\n\nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (arXiv:1810.04805). arXiv. https://doi.org/10.48550/arXiv.1810.04805\n\n\nGoyal, K., & Sharma, M. (2022). Comparative Analysis of Different Vectorizing Techniques for Document Similarity using Cosine Similarity. 2022 Second International Conference on Advanced Technologies in Intelligent Control, Environment, Computing & Communication Engineering (ICATIECE), 1‚Äì5. https://doi.org/10.1109/ICATIECE56365.2022.10046766\n\n\nJiang, T., Huang, S., Luan, Z., Wang, D., & Zhuang, F. (2023). Scaling Sentence Embeddings with Large Language Models (arXiv:2307.16645). arXiv. https://doi.org/10.48550/arXiv.2307.16645\n\n\nLate Chunking in Long-Context Embedding Models. (2024). https://jina.ai/news/late-chunking-in-long-context-embedding-models.\n\n\nReimers, N., & Gurevych, I. (2019). Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks (arXiv:1908.10084). arXiv. https://doi.org/10.48550/arXiv.1908.10084\n\n\nSteck, H., Ekanadham, C., & Kallus, N. (2024). Is Cosine-Similarity of Embeddings Really About Similarity? Companion Proceedings of the ACM Web Conference 2024, 887‚Äì890. https://doi.org/10.1145/3589335.3651526\n\n\nTunstall, L., Reimers, N., Jo, U. E. S., Bates, L., Korat, D., Wasserblat, M., & Pereg, O. (2022). Efficient Few-Shot Learning Without Prompts (arXiv:2209.11055). arXiv. https://arxiv.org/abs/2209.11055",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Embedding-based agent-systems</span>"
    ]
  },
  {
    "objectID": "content/function_calling.html",
    "href": "content/function_calling.html",
    "title": "Function Calling",
    "section": "",
    "text": "Code generation and function calling",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Function Calling</span>"
    ]
  },
  {
    "objectID": "content/function_calling.html#data-analysis",
    "href": "content/function_calling.html#data-analysis",
    "title": "Function Calling",
    "section": "Data analysis",
    "text": "Data analysis",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Function Calling</span>"
    ]
  },
  {
    "objectID": "content/function_calling.html#further-readings",
    "href": "content/function_calling.html#further-readings",
    "title": "Function Calling",
    "section": "Further Readings",
    "text": "Further Readings",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Function Calling</span>"
    ]
  },
  {
    "objectID": "content/agent_interaction.html",
    "href": "content/agent_interaction.html",
    "title": "Agent interaction",
    "section": "",
    "text": "Constitutional AI Tuning",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Agent interaction</span>"
    ]
  },
  {
    "objectID": "content/agent_interaction.html#preventing-prompt-injections",
    "href": "content/agent_interaction.html#preventing-prompt-injections",
    "title": "Agent interaction",
    "section": "Preventing prompt injections",
    "text": "Preventing prompt injections",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Agent interaction</span>"
    ]
  },
  {
    "objectID": "content/agent_interaction.html#further-readings",
    "href": "content/agent_interaction.html#further-readings",
    "title": "Agent interaction",
    "section": "Further Readings",
    "text": "Further Readings",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Agent interaction</span>"
    ]
  },
  {
    "objectID": "content/generator_basics.html",
    "href": "content/generator_basics.html",
    "title": "AI image generation",
    "section": "",
    "text": "AI image generator basics",
    "crumbs": [
      "Image Generation",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>AI image generation</span>"
    ]
  },
  {
    "objectID": "content/generator_basics.html#basics-of-using-open-source-ai-image-generation-models",
    "href": "content/generator_basics.html#basics-of-using-open-source-ai-image-generation-models",
    "title": "AI image generation",
    "section": "Basics of using Open Source AI image generation models",
    "text": "Basics of using Open Source AI image generation models",
    "crumbs": [
      "Image Generation",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>AI image generation</span>"
    ]
  },
  {
    "objectID": "content/generator_basics.html#generative-adversarial-networks-gans",
    "href": "content/generator_basics.html#generative-adversarial-networks-gans",
    "title": "AI image generation",
    "section": "Generative Adversarial Networks (GANs)",
    "text": "Generative Adversarial Networks (GANs)",
    "crumbs": [
      "Image Generation",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>AI image generation</span>"
    ]
  },
  {
    "objectID": "content/generator_basics.html#further-readings",
    "href": "content/generator_basics.html#further-readings",
    "title": "AI image generation",
    "section": "Further Readings",
    "text": "Further Readings",
    "crumbs": [
      "Image Generation",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>AI image generation</span>"
    ]
  },
  {
    "objectID": "content/augmentation.html",
    "href": "content/augmentation.html",
    "title": "Augmentation of image datasets",
    "section": "",
    "text": "(Generative) approaches for image dataset augmentation",
    "crumbs": [
      "Image Generation",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Augmentation of image datasets</span>"
    ]
  },
  {
    "objectID": "content/augmentation.html#further-readings",
    "href": "content/augmentation.html#further-readings",
    "title": "Augmentation of image datasets",
    "section": "Further Readings",
    "text": "Further Readings",
    "crumbs": [
      "Image Generation",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Augmentation of image datasets</span>"
    ]
  },
  {
    "objectID": "content/finetuning_approaches.html",
    "href": "content/finetuning_approaches.html",
    "title": "Finetuning Approaches",
    "section": "",
    "text": "Basics of Finetuning strategies",
    "crumbs": [
      "Finetuning",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Finetuning Approaches</span>"
    ]
  },
  {
    "objectID": "content/finetuning_approaches.html#alignment-and-finetuning-of-llms",
    "href": "content/finetuning_approaches.html#alignment-and-finetuning-of-llms",
    "title": "Finetuning Approaches",
    "section": "Alignment and Finetuning of (L)LMs",
    "text": "Alignment and Finetuning of (L)LMs",
    "crumbs": [
      "Finetuning",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Finetuning Approaches</span>"
    ]
  },
  {
    "objectID": "content/finetuning_approaches.html#further-readings",
    "href": "content/finetuning_approaches.html#further-readings",
    "title": "Finetuning Approaches",
    "section": "Further Readings",
    "text": "Further Readings",
    "crumbs": [
      "Finetuning",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Finetuning Approaches</span>"
    ]
  },
  {
    "objectID": "content/rank_adaptation.html",
    "href": "content/rank_adaptation.html",
    "title": "Rank adaptation",
    "section": "",
    "text": "Fundamentals of High and Low-Rank Adaptation of Language and Diffusion Models",
    "crumbs": [
      "Finetuning",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Rank adaptation</span>"
    ]
  },
  {
    "objectID": "content/rank_adaptation.html#qlora-fine-tuning-using-unsloth",
    "href": "content/rank_adaptation.html#qlora-fine-tuning-using-unsloth",
    "title": "Rank adaptation",
    "section": "(Q)LoRA fine-tuning using Unsloth",
    "text": "(Q)LoRA fine-tuning using Unsloth",
    "crumbs": [
      "Finetuning",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Rank adaptation</span>"
    ]
  },
  {
    "objectID": "content/rank_adaptation.html#further-readings",
    "href": "content/rank_adaptation.html#further-readings",
    "title": "Rank adaptation",
    "section": "Further Readings",
    "text": "Further Readings",
    "crumbs": [
      "Finetuning",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Rank adaptation</span>"
    ]
  }
]