{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Session 2\n",
        "\n",
        "Max\n",
        "\n",
        "## Task 2:\n",
        "\n",
        "> **📝 Task**\n",
        ">\n",
        "> **1.** Test the above-mentioned prompting strategies on the [MTOP\n",
        "> Intent\n",
        "> Dataset](https://huggingface.co/datasets/mteb/mtop_intent/viewer/de)\n",
        "> and evaluate the results against each other. The dataset contains\n",
        "> instructions and labels indicating on which task the instruction was\n",
        "> intended to prompt. Use a python script to call one of the following\n",
        "> three models in LM-Studio for this:\n",
        ">\n",
        "> 1.  [Phi 3.1\n",
        ">     mini](https://model.lmstudio.ai/download/lmstudio-community/Phi-3.1-mini-128k-instruct-GGUF)\n",
        "> 2.  [Gemma 2\n",
        ">     2B](https://model.lmstudio.ai/download/lmstudio-community/gemma-2-2b-it-GGUF)\n",
        "> 3.  [Llama 3.2\n",
        ">     1B](https://model.lmstudio.ai/download/hugging-quants/Llama-3.2-1B-Instruct-Q8_0-GGUF)\n",
        ">\n",
        "> Use the [F1-score](https://en.wikipedia.org/wiki/F-score) implemented\n",
        "> in [scikit\n",
        "> learn](https://scikit-learn.org/1.5/modules/generated/sklearn.metrics.f1_score.html)\n",
        "> to evaluate your results. pip **2.** You do sometimes read very\n",
        "> specific tips on how to improve your results. Here are three, that you\n",
        "> can find from time to time:\n",
        ">\n",
        "> -   Do promise rewards (i.e., monetary tips) instead of threatening\n",
        ">     punishments\n",
        "> -   Do formulate using affirmation (“*Do the task*”) instead of\n",
        ">     negating behaviours to be avoided (“*Don’t do this mistake*”)\n",
        "> -   Let the model reason about the problem before giving an answer\n",
        ">\n",
        "> Check these strategies on whether they improve your results. If your\n",
        "> first instruction already results in near-perfect classification,\n",
        "> brainstorm a difficult task that you can validate qualitatively. Let\n",
        "> the model write a recipe or describe Kiel for example.\n",
        ">\n",
        "> **3.** Present your results\n",
        ">\n",
        "> **3.** Upload your code to moodle\n",
        "\n",
        "Let’s start by connecting to the llama model hosted in lmstudio and test\n",
        "the connection:"
      ],
      "id": "ca97709e-b9f0-4fbe-956c-3663399779ec"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a test."
          ]
        }
      ],
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key='lm-studio',  \n",
        "    base_url=\"http://localhost:1234/v1\"\n",
        ")\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Say this is a test\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama-3.2-1b-instruct\",\n",
        ")\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "id": "698ea645"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since this seems to run but writing the whole interface every time feels\n",
        "like a lot of work, let’s define a wrapper that does it for us:"
      ],
      "id": "8ba22e9a-1924-4fcf-aca6-8f90d52e81c3"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_answer(prompt, system = None):\n",
        "    if system is None:\n",
        "        system = {'role': 'system',\n",
        "                  'content': 'You are a helpful AI assistant.'}\n",
        "    chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        system | {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt,\n",
        "        }\n",
        "    ],\n",
        "        model=\"llama-3.2-1b-instruct\",\n",
        "    )\n",
        "    return chat_completion.choices[0].message.content\n",
        "\n",
        "\n",
        "def classify_one_example(example, prompt):\n",
        "    wrapper_prompt = \"\"\"\n",
        "    ### Task\n",
        "    You will be given instructions to a classification task and an input.\n",
        "    Do answer with only the label you assign based on the instructions provided. \n",
        "    The label should only be one word.\n",
        "\n",
        "    ### Instruction\n",
        "    {prompt}\n",
        "\n",
        "    ### Example\n",
        "    {example}\n",
        "\n",
        "    ### Label\n",
        "    \"\"\"\n",
        "    return get_answer(wrapper_prompt.format(prompt = prompt,\n",
        "                                    example = example))"
      ],
      "id": "68f2b1b3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let’s test this on our first example:"
      ],
      "id": "edec381a-a8b9-42a0-a8f5-50e27dab5e08"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "'NEUTRAL'"
            ]
          }
        }
      ],
      "source": [
        "classify_one_example(\n",
        "    example = 'Franz war böse, als sein Antrag abgelehnt wurde.',\n",
        "    prompt = ('Classify the sentiment of the given example.' + \n",
        "              'Only use positive, neutral or negative as classes.')\n",
        ")"
      ],
      "id": "e165980f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since the answer does look sensible, let’s move to the mtop intent\n",
        "dataset. We will focus on the reduced task of binary classifying\n",
        "messages into GET_MESSAGE and OTHER. The first step is to import and\n",
        "wrangle the JSONL data:"
      ],
      "id": "fa6ad8e1-9422-48e2-8065-892e4df4aef1"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "data = []\n",
        "with open('data/de_test.jsonl', 'r') as f:\n",
        "    for line in f:\n",
        "        data.append(json.loads(line))\n",
        "\n",
        "possible_labels = list(set([entry['label_text'] for entry in data]))\n",
        "\n",
        "texts_to_classify = [\n",
        "    {'example': entry['text'],\n",
        "     'label': 'GET_MESSAGE' if entry['label_text'] == 'GET_MESSAGE' \n",
        "              else 'OTHER'} for entry in data\n",
        "]"
      ],
      "id": "8383b570"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To test the different prompting-strategies against each other, we have\n",
        "to set up four prompts, each utilizing one of the possible strategies:"
      ],
      "id": "ac70b107-473e-412b-8c9d-a4626e831009"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_1 = \"\"\"\n",
        "Classify the task instructed by the given example.\n",
        "Only use 'GET_MESSAGE' or 'OTHER' as class-labels.\n",
        "\"\"\"\n",
        "\n",
        "prompt_2 = prompt_1 + \"\"\"\n",
        "Think about which task the instruction prompts before you give the label. \n",
        "Take note of verbs in the instruction hinting at the result the user \n",
        "is expecting.\n",
        "\"\"\"\n",
        "\n",
        "prompt_3 = prompt_1 +\"\"\"\n",
        "Examples should be labeled 'GET_MESSAGE' if the request could be answered by\n",
        "looking up a message.\n",
        "In every other case, the label shoul be 'OTHER'.\n",
        "\"\"\"\n",
        "\n",
        "prompt_4 = \"\"\"\n",
        "These are examples for possible labellings:\n",
        "Lass mich die Nachrichten von meiner Oma Marilyn sehen - GET_MESSAGE\n",
        "ache einen Videoanruf an Mariana per Whatsapp - OTHER\n",
        "\"\"\"\n",
        "\n",
        "for entry in texts_to_classify[0:20]:\n",
        "    entry['prompt_1'] = classify_one_example(entry['example'],\n",
        "                                             prompt_1).lower()\n",
        "    entry['prompt_2'] = classify_one_example(entry['example'],\n",
        "                                            prompt_2).lower()\n",
        "    entry['prompt_3'] = classify_one_example(entry['example'],\n",
        "                                             prompt_3).lower()\n",
        "    entry['prompt_4'] = classify_one_example(entry['example'],\n",
        "                                             prompt_4).lower()"
      ],
      "id": "e148e629"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt number: prompt_1\n",
            "Examples: 20 Accuracy: 0.5, F1: 0.6153846153846154\n",
            "Prompt number: prompt_2\n",
            "Examples: 20 Accuracy: 0.35, F1: 0.5185185185185185\n",
            "Prompt number: prompt_3\n",
            "Examples: 20 Accuracy: 0.5, F1: 0.6153846153846154\n",
            "Prompt number: prompt_4\n",
            "Examples: 20 Accuracy: 0.4, F1: 0.5714285714285714"
          ]
        }
      ],
      "source": [
        "results = {\n",
        "    'prompt_1': [],\n",
        "    'prompt_2': [],\n",
        "    'prompt_3': [],\n",
        "    'prompt_4': [],\n",
        "    'y_true': []\n",
        "}\n",
        "\n",
        "classes = {'GET_MESSAGE': 1, 'OTHER': 0}\n",
        "for entry in texts_to_classify[0:20]:\n",
        "    try:\n",
        "        for k in [k for k in results.keys() if 'prompt' in k]:\n",
        "            results[k].append(0 if 'other' in entry[k] else 1)\n",
        "        results['y_true'].append(classes[entry['label']])\n",
        "    except KeyError:\n",
        "        break\n",
        "        \n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "for k in [k for k in results.keys() if 'prompt' in k]:\n",
        "    y_pred = results[k]\n",
        "    y_true = results['y_true']\n",
        "    acc = accuracy_score(y_true, y_pred[0:len(y_true)])\n",
        "    f1 = f1_score(y_true, y_pred[0:len(y_true)])\n",
        "    print(f'Prompt number: {k}')\n",
        "    print(f'Examples: {len(y_pred)} Accuracy: {acc}, F1: {f1}')"
      ],
      "id": "e3f14aaa"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The base-prompt seems to perform whe worst, while adding few-shot\n",
        "examples seems to improve the performance, only topped by providing\n",
        "small steps to the model. The somewhat arbitrary, more literal\n",
        "description of the task seems to help - though not by as much as the\n",
        "other strategies.\n",
        "\n",
        "Let’s lastly test, how all four strategies perform combined:"
      ],
      "id": "832b9cd9-3540-4401-9280-ec12fc352f9e"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt number: prompt_1\n",
            "Examples: 20 Accuracy: 0.5, F1: 0.6153846153846154\n",
            "Prompt number: prompt_2\n",
            "Examples: 20 Accuracy: 0.35, F1: 0.5185185185185185\n",
            "Prompt number: prompt_3\n",
            "Examples: 20 Accuracy: 0.5, F1: 0.6153846153846154\n",
            "Prompt number: prompt_4\n",
            "Examples: 20 Accuracy: 0.4, F1: 0.5714285714285714\n",
            "Prompt number: all_prompts\n",
            "Examples: 20 Accuracy: 0.4, F1: 0.5714285714285714"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "Classify the task instructed by the given example. \n",
        "Only use 'GET_MESSAGE' or 'OTHER' as class-labels.\n",
        "Think about which task the instruction prompts before you give the label. \n",
        "Take note of verbs in the instruction hinting at the result the user is\n",
        "expecting.\n",
        "Examples should be labeled 'GET_MESSAGE' if the request could be answered\n",
        "by looking up a message.\n",
        "In every other case, the label shoul be 'OTHER'.\n",
        "These are examples for possible labellings:\n",
        "Lass mich die Nachrichten von meiner Oma Marilyn sehen - GET_MESSAGE\n",
        "ache einen Videoanruf an Mariana per Whatsapp - OTHER\n",
        "\"\"\"\n",
        "for entry in texts_to_classify[0:20]:\n",
        "    entry['all_prompts'] = classify_one_example(entry['example'],\n",
        "                                                prompt).lower()\n",
        "\n",
        "results = {\n",
        "    'prompt_1': [],\n",
        "    'prompt_2': [],\n",
        "    'prompt_3': [],\n",
        "    'prompt_4': [],\n",
        "    'all_prompts': [],\n",
        "    'y_true': []\n",
        "}\n",
        "\n",
        "classes = {'GET_MESSAGE': 1, 'OTHER': 0}\n",
        "for entry in texts_to_classify[0:20]:\n",
        "    try:\n",
        "        for k in [k for k in results.keys() if 'prompt' in k]:\n",
        "            results[k].append(0 if 'other' in entry[k] else 1)\n",
        "        results['y_true'].append(classes[entry['label']])\n",
        "    except KeyError:\n",
        "        break\n",
        "        \n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "for k in [k for k in results.keys() if 'prompt' in k]:\n",
        "    y_pred = results[k]\n",
        "    y_true = results['y_true']\n",
        "    acc = accuracy_score(y_true, y_pred[0:len(y_true)])\n",
        "    f1 = f1_score(y_true, y_pred[0:len(y_true)])\n",
        "    print(f'Prompt number: {k}')\n",
        "    print(f'Examples: {len(y_pred)} Accuracy: {acc}, F1: {f1}')\n"
      ],
      "id": "e1acf3e0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The results got worse - the model classified each sentence as\n",
        "“get_message”:"
      ],
      "id": "c68d2119-7827-4eb9-898f-aeb94707f552"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'get_message'}"
          ]
        }
      ],
      "source": [
        "print(set([t['all_prompts'] for t in texts_to_classify[0:20]]))"
      ],
      "id": "acc2b700"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This goes to show that though the tips might work, there is such thing\n",
        "as too much instruction and that there is no such thing as the\n",
        "one-size-fits-all prompt.\n",
        "\n",
        "## Task 3\n",
        "\n",
        "> **📝 Task**\n",
        ">\n",
        "> Using your script for batch-testing different prompts, generate\n",
        "> synthetic data for a emotion detection task based on [Paul\n",
        "> Ekman’s](https://en.wikipedia.org/wiki/Paul_Ekman) six basic emotions:\n",
        "> anger, disgust, fear, happiness, sadness and surprise[1].\n",
        ">\n",
        "> The generated data should consist of a sentence and the emotion that\n",
        "> is expressed in it. Start by generating two examples for each emotion.\n",
        "> Validate these results and adapt them if necessary. Then use these\n",
        "> examples to generate 100 samples for each emotion.\n",
        ">\n",
        "> Use one of the above mentioned (non-manual) strategies to validate the\n",
        "> data you generated.\n",
        ">\n",
        "> Upload your results to Moodle.\n",
        "\n",
        "> **Note**\n",
        ">\n",
        "> Since we did not have time to individually work on this task in\n",
        "> session, we tried to solve it together.\n",
        ">\n",
        "> This solution is what we came up with during the session. We only had\n",
        "> time to visually validate the results and weren’t able to implement\n",
        "> one of the other strategies.\n",
        "\n",
        "[1] Though this nomenclature has fallen a bit out of fashion"
      ],
      "id": "1685c0f8-0a70-4e64-ac50-c7a34cf08d53"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "angry: The incompetence of the government is staggering and it's infuriating to think that they're still not doing enough to address this crisis.\n",
            "angry: The incompetence of the government is staggering and it's infuriating to think that they're still not doing enough to address this crisis.\n",
            "disgust: The putrid smell that wafted from the dumpster was so overwhelming it made my stomach turn.\n",
            "disgust: The putrid smell that wafted from the dumpster was so overwhelming it made my stomach turn.\n",
            "fear: As I lay in bed, I couldn't shake the feeling that something was watching me from the shadows.\n",
            "fear: As I lay in bed, I couldn't shake the feeling that something was watching me from the shadows.\n",
            "happiness: The sun was shining brightly in the sky as children laughed and played together on a beautiful summer day.\n",
            "happiness: The sun was shining brightly in the sky, casting a warm glow over the entire town."
          ]
        }
      ],
      "source": [
        "prompts = {'angry': \"Write a sentence angrily\",\n",
        "           'disgust' : \"Write a disgusted sentence.\",\n",
        "           'fear': \"Write a fearful sentence\",\n",
        "           'happiness': \"Write a happy sentence\"}\n",
        "\n",
        "for k, prompt in prompts.items():\n",
        "    for i in range(2):\n",
        "        print(f'{k}: {get_answer(prompt)}')"
      ],
      "id": "751c143f"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "path": "/home/brede/.local/share/jupyter/kernels/python3"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  }
}