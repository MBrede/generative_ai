[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Generative AI",
    "section": "",
    "text": "Introduction\nThis script serves as an introduction to Generative AI and was developed for the elective module “Generative AI,” offered to master’s students of the “Data Science” program at the University of Applied Sciences Kiel. Built using quarto, this resource is designed to provide an accessible overview of key topics and applications in this rapidly evolving field.\nWhile not an exhaustive guide to Generative AI, the script highlights foundational concepts, modern applications, and practical techniques that empower students to engage with and explore the possibilities of these transformative technologies.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#contents-and-learning-objectives",
    "href": "index.html#contents-and-learning-objectives",
    "title": "Generative AI",
    "section": "Contents and learning objectives",
    "text": "Contents and learning objectives\nContents listed in the module database entry:\nOpen Source Language Models\n\nOverview of model lists\nOllama\nGeneration of synthetic text as training sets\n\nAgent Systems\n\nLlamaindex, LangChain & Haystack\nFunction calling\nData analysis\n\nEmbeddings and Vector Stores\n\nSemantic Search\nRetrieval-augmented generation\nRecommendations\n\nAI Image Generators\n\nGenerative Adversarial Networks (GANs)\nVariational Autoencoders / Diffusion Models\nGenerative approaches for image dataset augmentation\n\nFine-Tuning of LLMs and Diffusion Models\n\nExamples: LoRA, QLoRA, MoRA\n\n\nLearning objectives listed in the module database entry:\nStudents\n\nknow the fundamentals of generative AI systems.\nknow various modern applications of generative AI systems.\nknow the theoretical foundations and practical applications of generative AI systems.\n\nStudents\n\nare able to explain and apply various open-source language models.\nare able to implement and utilize agent systems and their functionalities.\nare able to understand and use embeddings and vector stores for semantic search and recommendations.\nare able to explain and practically apply different methods for image generation.\nare able to fine-tune large language models (LLMs) and diffusion models for specific tasks.\n\nStudents\n\nare able to successfully organize teamwork for generative AI projects.\nare able to report and present team solutions for practical project tasks.\nare able to interpret and communicate the approaches in technical and functional terms.\n\nStudents\n\nare able to work professionally in the field of generative AI systems.\nare able to give and accept professional feedback to different topics of generative AI systems.\nare able to select relevant scientific literature about generative AI systems.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Generative AI",
    "section": "Schedule:",
    "text": "Schedule:\n\nCourse schedule\n\n\n\n\n\n\n\n\n\nNumber:\nCW:\nDate:\nTitle:\nTopics:\n\n\n\n\n1\n46\n12.11.\nGetting started with (L)LMs\nLanguage Model Basics\n\n\n\n\n\n\nChoosing open source models\n\n\n\n\n\n\nBasics of using open source models (Huggingface, Ollama, LLM-Studio, Llama.cpp, …)\n\n\n2\n46\n13.11.\nPrompting\nPrompting strategies\n\n\n\n\n\n\nGeneration of synthetic texts\n\n\n3\n47\n19.11.\nAgent basics\nFundamentals of agents and chain-of-thought prompting\n\n\n\n\n\n\nExamples of agent-frameworks (Llamaindex, LangChain & Haystack)\n\n\n4\n47\n20.11.\nEmbedding-based agent-systems\nSemantic embeddings and vector stores\n\n\n\n\n\n\nRetrieval augmented and interleaved generation\n\n\n5\n48\n26.11.\nFunction Calling\nCode generation and function calling\n\n\n6\n48\n27.11.\nAgent interaction\nLLM as a Judge\n\n\n\n\n\n\nConstitutional AI\n\n\n7\n49\n3.12.\nAI image generation I\nAI image generator basics\n\n\n\n\n\n\nDiffusion Models and Variational Autoencoders\n\n\n\n\n\n\nMultimodal models\n\n\n8\n49\n4.12.\nAI image generation II\nGenerative Adversarial Networks (GANs)\n\n\n\n\n\n\n(Generative) approaches for image dataset augmentation\n\n\n9\n50\n10.12.\nAI image generation III\nUsing Open Source AI image generation models\n\n\n\n\n\n\nAI image generators in agent systems\n\n\n10\n50\n11.12.\nFinetuning Basics\nBasics of Finetuning strategies\n\n\n\n\n\n\nAlignment and Finetuning of (L)LMs\n\n\n11\n51\n17.12.\nRank adaptation\nFundamentals of High and Low-Rank Adaptation of Language and Diffusion Models\n\n\n\n\n\n\n(Q)LoRA fine-tuning using Unsloth\n\n\n12\n51\n18.12.\nProject presentations",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "content/orga.html",
    "href": "content/orga.html",
    "title": "Organizational Details",
    "section": "",
    "text": "Planned Class Structure\nEach class meeting will follow this structure:\nStudents will be divided into teams of three at the start of the course, with projects culminating in a final presentation to the class. The project grade will count towards your final course grade.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Organizational Details</span>"
    ]
  },
  {
    "objectID": "content/orga.html#planned-class-structure",
    "href": "content/orga.html#planned-class-structure",
    "title": "Organizational Details",
    "section": "",
    "text": "Instructional Session: We’ll introduce new concepts and techniques.\nPractice Exercise: Students will apply these concepts through an exercise.\nProject Worktime: Students will work on their team projects.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Organizational Details</span>"
    ]
  },
  {
    "objectID": "content/project_details.html",
    "href": "content/project_details.html",
    "title": "Project Details",
    "section": "",
    "text": "Projects should allow students to apply what they’ve learned throughout the course. They must implement an LLM-based system that includes at least two of the following features:\n\nRetrieval Augmentation/RAG (i.e., the system should query documents or other content in an index for its answers and reference the sources of its generation)\nData Analysis (i.e., the system should “talk” to a dataset and decide on which analysis-steps are to be taken to then execute them)\nMultiple Agents (i.e., at least two agents should work in tandem, for example in a generator-reviewer arrangement)\nFine-tuning on (Synthetic) Data (i.e., a small LM or SDM should be finetuned on (synthetic) data to adapt it to your needs. You could as an example train a model to only answer in nouns.)\n\nThe project should also include function-calling-based interface (“a tool”) to an AI image generator.\nStudents are free to choose their project topic, as long as it fits within the course scope and is approved by the instructor. All projects must be implemented in Python.\nThe active participation on the course will be taken into account before grading. This means that all tasks asking the students to upload their results to moodle should be completed. If more than one of the required tasks is missing, the student will not be graded.\nThe projects are to be presented in the last session of the course. The students of each group need to take part in this session. The presentation will become part of the overall grade. The presentation can but does not have to be prepared in PPT, any other mode of presentation (including a live-demo based on a nice notebook) is fine.\nThe project will then be graded based on these contents in addition to the following criteria:\n\nThe minimum of components mentioned above have to be used\nThe more components are used, the better the grade\nThe project-solution has to work.(Since we are talking about LLMs it does not have to generate perfect results, the pipeline has to generally work though.)\nThe students have to hand in code the instructors can run. The code has to be documented. This can be done either in sensible docstrings, appropriately commented notebooks or a report. The students can choose the mode. It is possible and recommended to create a github repository with the code and the documentation.\n\nExample Project Ideas:\n\nLLM Tourist Guide: Uses TA.SH data to provide travel tips and enhances them with generated images.\nQuarto Data Presentation Pipeline: Builds and illustrates a Quarto presentation based on a given open dataset.\nSynthetic Author: Generates commit-messages based on commit history/diff. It could also suggest GitHub issues illustrated with AI-generated images.\nAI Storyteller: Creates illustrated short stories for children based on historical events.\nAI Webdesigner A tool that creates and illustrates a webpage based on a Amazon product page.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Details</span>"
    ]
  },
  {
    "objectID": "content/getting_started_with_llms.html",
    "href": "content/getting_started_with_llms.html",
    "title": "Getting started with (L)LMs",
    "section": "",
    "text": "Language Model Basics\nThis chapter provides a brief introduction to the history and function of modern language models, focusing on their practical use in text generation tasks. It will then give a short introduction on how to utilize pretrained language models for your own applications.\nLanguage models have diverse applications, including speech recognition, machine translation, text generation, and question answering. While we’ll concentrate on text generation for this course, understanding the general concept of language models is crucial. Given language’s inherent complexity and ambiguity, a fundamental challenge in NLP is creating structured representations that can be employed downstream. This section will first explore the evolution of these representations before introducing the transformer architecture, which forms the foundation of most modern language models.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting started with (L)LMs</span>"
    ]
  },
  {
    "objectID": "content/getting_started_with_llms.html#language-model-basics",
    "href": "content/getting_started_with_llms.html#language-model-basics",
    "title": "Getting started with (L)LMs",
    "section": "",
    "text": "A short history of natural language processing\n\n\n\n\n\n\nFig 3.1: BOW-representation of sentences.\n\n\n\nThe Bag Of Words (BOW) method represents text data by counting the frequency of each word in a given document or corpus. It treats all words as independent and ignores their order, making it suitable for tasks like text classification, for which it was traditionally the gold-standard. However, BOW has limitations when it comes to capturing semantic relationships between words and gets utterly useless if confronted with words not represented in the corpus. Additionally, it does not take into account the order of words in a sentence, which can be crucial for understanding its meaning. For example, the sentences “The cat is on the mat” and “The mat is on the cat” have different meanings despite having the same set of words.\n\n\n\n\n\n\nFig 3.2: CBOW-representation of corpus.\n\n\n\nThe Continuous Bag Of Words (CBOW) method extends traditional BOW by representing words as dense vectors in a continuous space. CBOW predicts a target word based on its context, learning meaningful word representations from large amounts of text data.\n\n\n\n\n\n\nFig 3.3: Shallow Model using CBOW-Method to predict missing word.\n\n\n\nfastText (Bojanowski et al., 2017), an open-source library developed by Facebook, builds upon the CBOW method and introduces significant improvements. It incorporates subword information and employs hierarchical softmax for efficient training on large-scale datasets. Even with limited data, fastText can learn meaningful word representations. fastText and its predecessor Word2Vec are considered precursors to modern language models due to their introduction of Embeddings, which laid the foundation for many modern NLP methods. Figure 3.3 illustrates this fastText-architecture1\n1 Well, kind of. One of the major advantages of fasttext was the introduction of subword information which were left out of this illustration to save on space. This meant that uncommon words that were either absent or far and few between in the training corpus could be represented by common syllables. The display like it is here is far closer to fasttext’s spiritual predecessor word2vec (Mikolov et al., 2013).\n\n\n\n\n\nFig 3.4: Model using CBOW-Method to predict missing word.\n\n\n\nLanguage Model Embeddings are learned by predicting the next word, or, in most cases, the next part of a word in a sequence. The utilisation of word-parts instead of whole words was another invention introduced by fastText (Bojanowski et al., 2017), that allowed the model to generalize to new, unknown words when moving to inference. These parts of words are also called tokens. Embeddings are the representation the model learns to map the context-tokens to a multiclass classification of the missing token in the space of all possible tokens. These embeddings capture semantic and syntactic relationships between words, enabling them to understand context effectively. Since these embeddings represent the conditional probability distribution that language models learn to comprehend natural language, they can be reused by other models for tasks such as text classification or text retrieval. But more on this later.\nStill, these models did not really solve the inherent issue of the order of words in a sentence. The input of models of this generation still used a dummyfied version of the corpus to represent context, which loses a lot of information.\n\n\n\n\n\n\nFig 3.5: Illustration of a simple RNN-model, (exaggeratingly) illustrating the issue of the model “forgetting” parts of the input when processing long sequences.\n\n\n\nTraditionally, this was approached by feeding these embeddings into Recurrent Neural Networks (RNNs). These models could learn to keep track of sequential dependencies in text data and improve the understanding of context. However, RNNs suffered from their architecture’s inherent inability to retain information over long sequences. Simple RNN- cells2 iterate through a sequence and use both their last output and the next sequence element as input to predict the next output. This makes it hard for them to learn long-term dependencies, since they have to compress all information into one vector (Figure 3.5)3.\n\n2 And pretty much all of the more complex variants3 This is also (kind of) the reason for the so called vanishing gradient problem, where each iteration of the network is necessary for calculating the gradient in the steps before.Long Short-Term Memory (LSTM) networks addressed this issue by introducing a mechanism called “gates” that allowed information to flow through the network selectively and more efficiently, but were, as the RNNs before, notoriuosly slow in training since only one word could be processed at a time. Additionally, a single LSTM is still only able to process the input sequence from left to right, which is not ideal for inputs that contain ambiguos words that need context after them to fully understand their meaning. Take the following part of a sentence:\n\nThe plant was growing\n\nThe word plant get’s wildly differing meanings, depending on how the sentence continues:\n\nThe plant was growing rapidly in the sunny corner of the garden.\n\n\nThe plant was growing to accommodate more machinery for production.\n\nA model that only processes the input sequence from left to right would just not be able to understand the meaning of “plant” in this context.\nThe ELMo model (Peters et al., 2018), which stands for Embeddings from Language Models, is an extension of LSTMs that improved contextual word representations. ELMo uses bidirectional LSTM layers to capture both past and future context, enabling it to understand the meaning of words in their surrounding context. This resulted in ELMo outperforming other models of its era on a variety of natural language processing tasks. Still as each of the LSTM-Layer were only able to process one part of the sequence at a time, it was still unfortunately slow in training and inference. Its performance additionally decreased with the length of the input sequence since LSTM-cells have a better information retention than RNNs but are still not able to keep track of dependencies over long sequences.\n\n\nAttention is all you need\nIn their transformative paper “Attention is all you need”, Vaswani et al. (2023) described the transformer architecture.\nAs the paper’s title neatly suggests, the major breakthrough presented in this paper was the introduction of the so-called self-attention mechanism. This mechanism allows the model to “focus” on different parts of the input to a) determine the appropriate context for each word and b) to improve its performance on differing tasks by allowing the model to filter unnecessary information.\n\nSelf-Attention Mechanism\nThe self-attention mechanism relies on three components: Query (Q), Key (K), and Value (V), inspired by concepts in information retrieval. Imagine you search for a specific term in a library (query), match it against the catalog (key), and retrieve relevant books (value).\nIn practice, for each word in a sentence, the model calculates:\n\nRelevance Scores: Compare each Query vector (Q) with every Key vector (K) in the sequence using the dot product. These scores measure how much focus one word should have on another.\nAttention Weights: Normalize the scores using a softmax function to ensure they sum to 1, distributing focus proportionally across all words.\nWeighted Sum: Multiply each Value vector (V) by its corresponding attention weight to compute the final representation.\n\nFor example, in the sentence, “The cat sat on the mat,” the model might assign more attention to “cat” when analyzing “sat,” capturing their relationship.\n\n\nCalculating Attention\nFor a sequence of words, the attention scores are computed as: \\[\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n\\]\nwhere:\n\n\\(Q\\) represents the query matrix.\n\\(K\\) is the key matrix.\n\\(V\\) is the value matrix.\n\\(d_k\\) is the dimensionality of the key vectors, ensuring scale invariance.\n\nLet’s first illustrate this concept with a practical example (not specifically from the context of NLP) to later circle back to its application in the transformer architecture.\nWe look at a retrieval task in which we query in a domain that has 5 attributes describing the items in it. The aforementioned “lookup” is then implemented by calculating the dot product between the query and the transposed keys resulting in a vector of weights for each input-aspect.\nAs a simplification, we assume that all aspects can be described in binary terms. A hypothetical 1x5 query matrix (Q) represents the aspects we are querying in a 5-dimensional space, while a transposed 1x5 key matrix (K) represents the aspects of the search space. The dot product between these matrices results in a scalar that reflects the alignment or similarity between the query and the key, effectively indicating how many aspects of the query align with the search space.\n\n\n\n\n\n\n\n\n\nIf we now add a series of items we want to query for to our matrix \\(K\\), the result will be a vector representing the amount of matches, each item has with our query:\n\n\n\n\n\n\n\n\n\nThe result is a vector of scores that indicate the matches of the query per key. This principle does obviously also work for more than one query by adding more rows to our Query matrix \\(Q\\). This does result in a matrix, in which each row indicates the amount of matching keys for each query:\n\n\n\n\n\n\n\n\n\nInstead of binary indicators, the \\(Q\\) and \\(K\\) matrices in the attention mechanism are filled with floats. This does still result in the same kind of matched-key-result, although the results are now more like degrees of relevance instead of absolute matches:\n\\[\nQ \\times K^T =\n\\]\n\n\n\n\n\n\n\n\n\nAs you can already see in this small example, the values of individual cells can get relatively high compared to the rest of the matrix. As you remember - we want to use this product to rank our values. If these numbers are too large, it might lead to numerical instability or incorrect results. To address this issue, we will scale down the dot-product by dividing it with \\(\\sqrt{d_n}\\), where \\(d_n\\) is the dimension of the aspect space (in our case 5).\n\\[\n\\frac{Q \\times K^T}{\\sqrt{d_n}} =\n\\]\n\n\n\n\n\n\n\n\n\nSince we want to use this matrix for filtering our dataset, we would prefer the weights to sum up to one. To achieve that, we will apply a softmax function on each row of the matrix (remember that the rows currently represent the key-weighted aspects for each query). The resulting matrix with scaled weights for each aspect is then multiplied with the value-matrix that contains one datapoint in each row, described by 5 aspects along the columns.\n\\[\n\\text{softmax}(\\frac{Q \\times K^T}{\\sqrt{d_n}}) \\times V =\n\\]\n\n\n\n\n\n\n\n\n\nThe result is now an attention matrix in the sense that it tells us the importance of each value’s aspect for our query. In the specific example, the forth value seems to be the most important aspect for our third query. The crucial advantage is, that all aspects of all queries can be simultaneously compared with all aspects of all values without the necessity of sequential processing.\nThough this general idea of weighting aspects in the sense of self-attention4 to process a sequence without disadvantages of the distances of the items was used before (Bahdanau, 2014), the major contribution of the paper was the complete reliance on this mechanism without the need of LSTM/RNN parts. That their suggested architecture works is in part due to the utilisation of multiple self-attention layers, each learning its own weights for \\(Q\\), \\(K\\) and \\(V\\). This allows the model to learn more complex patterns and dependencies between words in a sentence. You can think of it as allowing the model to focus on different parts of the input sequence at different stages of processing. The outputs of the multiple heads are then concatenated and linearly transformed into the final output representation using a series of fully connected feed-forward layers.\n4 self in the sense of the model weighting its own embeddings, queries, keys and valuesThis small example is already pretty close to the general attention-mechanism described by Vaswani et al. (2023) (see also Figure 3.6), though the actual language model learns its own weights for \\(Q\\), \\(K\\) and \\(V\\).\n\n\n\n\n\n\nFig 3.6: Multi-headed attention as depicted in Vaswani et al. (2023)\n\n\n\nInstead of 5x5 matrices, the attenion mechanism as described in the paper implements \\(d_n \\times d_c\\)5 matrices, where \\(d_n\\) is the dimension of the embedding space6 and \\(d_c\\) is the size of the context window. In the original paper, Vaswani et al. (2023) implement the context-window as the same size as the embedding space (i.e., \\(d_n = d_c\\)). In Figure 3.7 you can see a brilliant illustration of the multiheaded-attention mechanism at work.\n5 \\(\\frac{d_n}{h} \\times \\frac{d_c}{h}\\) actually, the paper used feed-forward layers to reduce the dimensionality of each attention header to reduce the computational cost.6 I.e., the dimensionality used to represent each word’s meaning. In the previous toy-example illustrating the concept of embeddings (Figure 3.4), this would be the width of the hidden layer (8). In the case of transformers, this is usually 512 or 1024. These embeddings are learned during training and are a simple transformation of the one-hot vectors returned by the models tokenizer.\n\n\n\n\n\nFig 3.7: Illustration of the multi-headed attention mechanism. Taken from Hussain et al. (2024)\n\n\n\nThe implementation of the multi-headed attention mechanism allowed to solve all major issues of the language modelling approaches of the previous generation7. It firstly allows the input of a whole text-sequence at once, rendering the training and inference far speedier then the recursive approaches. Furthermore, the multi-head attention mechanism allows the model to focus on different parts of the input sequence simultaneously, enabling it to capture more complex relationships between words and improve its understanding of context without losing information about long-term dependencies. This mechanism also implicitly solves the bidirectionality-issue since each word can be taken into account when processsing every other word in the sequence.\n7 Well, kind of. Transformers are far superior language models due to their ability to parallely process long sequences without issues with stretched context - these advantages come at a price though. GPT-3s training is estimated to have emitted around 502 metric tons of carbon (AIAAIC - ChatGPT training emits 502 metric tons of carbon, n.d.). The computational cost of the architecture as described here does additionally scale quadratically with context window size.The description until now omitted one final but key detail - we only spoke about the weight matrices \\(Q\\), \\(K\\) and \\(V\\). Each of these weight matrices are actually the product of the learned weights and the input vectors. In other words, each of the three matrices is calculated as follows:\n\\[\n\\begin{align}\n    Q &= XW_Q \\\\\n    K &= XW_k \\\\\n    V &= XW_v\n\\end{align}\n\\]\nwhere \\(W_{Q, k, v}\\) are the learned weight matrices and \\(X\\) is the input matrix. This input matrix consists of a) the learned embeddings of the tokenized input-parts and b) the added, so called positional encoding.8\n8 While we are talking about omitted details, the whole architecture implements its layers as residual layers. This means that the output of each layer is added to the input of the layer before, before it is passed on to the next layer. But this detail is irrelevant for our understanding of the central mechanism.The positional encoding is a vector that encodes the position of each token in the input sequence. It is added to the embedding of each token to provide the model with information about the order of the tokens in the sequence. The positional encoding is calculated as follows:\n\\[\n\\begin{array}{lcl}\nPE_{(pos, 2i)} &=& sin(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}) \\\\\nPE_{(pos, 2i+1)} &=& cos(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})\n\\end{array}\n\\]\nWhere \\(i\\) is the dimension and \\(pos\\) is the position. Those 2 formulas are not the most intuitive, what they do is to add a unique offset to each embedding though, that allows the model to infer and weigh the token’s positions in the matrix on it’s own. Figure 3.8 illustrates the pattern this specific combination of sin and cos creates for each sequence-position and embedding-dimension.\n\n\n\n\n\n\n\n\nFig 3.8: The positional encoding for 50 dimensions and 512 embedding-dimensions. The x-axis represents the position and the y-axis represents the dimension. The color represents the value of the encoding.\n\n\n\n\n\nThese parts alltogether are all building-blocks of the basic transformer architecture. As you can see in Figure 3.9, all parts depicted by Vaswani et al. (2023) are parts we have discussed until now.\n\n\n\n\n\n\nFig 3.9: The transformer architecture as depicted in Vaswani et al. (2023)\n\n\n\nThe Encoder half uses the embedding -&gt; encoding -&gt; multi-headed-attention -&gt; feed-forward structure to create a semantic representation of the sequence. The Decoder half uses the same structure, but with an additional masked multi-head attention layer to prevent the model from looking at future tokens. This is necessary because we want to generate a sequence token by token.\nFigure 3.10, taken from Kaplan et al. (2020), shows the test performance of Transformer models compared to LSTM-based models as a function of model size and context length. Transformers outperform LSTMs with increasing context length.\n\n\n\n\n\n\nFig 3.10: Comparison of Transformer- and LSTM-performance based on Model size and context length. Taken from Kaplan et al. (2020)\n\n\n\nFurthermore, Kaplan et al. (2020) and Hoffmann et al. (2022) after them postulated performace power-laws (see also Figure 3.11) that suggest that the performance of a Transformer directly scales with the models size and data availability. Though the task of prediction of natural language poses a non-zero limit to the performance, it is suggested that this limit is not reached for any of the currently available models.9\n9 Incidentally, we might run out of data to train on before reaching that limit (Villalobos et al., 2024).\n\n\n\n\n\nFig 3.11: Performance power law for transformer models. Taken from Kaplan et al. (2020)\n\n\n\nThe advances made through leveraging transformer-based architectures for language modelling led to a family of general-purpose language models. Unlike the approaches before, these models were not trained for a specific task but rather on a general text base with the intention of allowing specific fine-tuning to adapt to a task. Classic examples of these early general-purpose natural language generating Transformer models are the Generative Pre-trained Transformer (the predecessor of ChatGPT you all know), first described in Radford et al. (2018), and the “Bidirectional Encoder Representations from Transformers” (BERT) architecture and training procedure, described by Devlin et al. (2019).\nThis general-purpose architecture is the base of modern LLMs as we know them today and most applications we will discuss in this course.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting started with (L)LMs</span>"
    ]
  },
  {
    "objectID": "content/getting_started_with_llms.html#choosing-open-source-models",
    "href": "content/getting_started_with_llms.html#choosing-open-source-models",
    "title": "Getting started with (L)LMs",
    "section": "Choosing open source models",
    "text": "Choosing open source models\nThe 2023 release of ChatGPT by OpenAI has sparked a lot of interest in large language models (LLMs) and their capabilities. This has also led to an increase in the number of available open-source LLMs. The selection of a model for your application is always a trade-off between performance, size, and computational requirements.\nAlthough Kaplan et al. (2020) showed a relationship between performance and model-size, the resources available will most probably limit you to smaller models. Additionally, a lot of tasks can be solved by smaller models if they are appropriately fine-tuned (Hsieh et al., 2023).\nA good idea when choosing an open source model is to start small and test whether the performace is sufficient for your use case. If not, you can always try a larger model later on.\nAdditionally, it is good practice to check the license of the model you want to use. Some models are only available under a non-commercial license, which means that you cannot use them for commercial purposes.\nThirdly, you should make sure that the model you choose is appropriate for your use case. For example, if you want to use a model for text generation, you should make sure that it was trained on a dataset that is similar to the data you will be using. If you want to use a model for translation, you should make sure that it was trained on a dataset that includes the languages you are interested in. A lot of usecases do already have benchmark datasets that can be used to pit models against each other and evaluate there appropriateness for a given use case based on a few key metrics.\nA good starting point for getting an overview about such metrics and benchmarks is Hugging Face. This platform has long cemented itself as the go-to place for getting access to open source models, but also provides a lot of resources for evaluating and comparing them. This page provides an overview of benchmarks, leaderboards and comparisons for a variety of tasks.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting started with (L)LMs</span>"
    ]
  },
  {
    "objectID": "content/getting_started_with_llms.html#basics-of-using-open-source-models",
    "href": "content/getting_started_with_llms.html#basics-of-using-open-source-models",
    "title": "Getting started with (L)LMs",
    "section": "Basics of using open source models",
    "text": "Basics of using open source models\n\n\n\n\n\n\n📝 Task\n\n\n\nNow it is your turn! In your project-groups, you will each have to build a small “Hello World”-style application that uses an open source model.\n\nChoose a small model using the sources we discussed before.\nEach group is to use one of the following frameworks\n\nHuggingface\nOllama\nLM-Studio from python\nLlama.cpp to load and use the model in your application.\n\nPresent your results and your experiences with the frameworks in a short presentation.\nSubmit your code and report on moodle.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting started with (L)LMs</span>"
    ]
  },
  {
    "objectID": "content/getting_started_with_llms.html#further-readings",
    "href": "content/getting_started_with_llms.html#further-readings",
    "title": "Getting started with (L)LMs",
    "section": "Further Readings",
    "text": "Further Readings\n\nThis quite high-level blog-article about foundational models by Heidloff (2023)\nThe Attention is all you need-paper (Vaswani et al., 2023) and the brilliant video discussing it by Umar Jamil (Vaswani et al., 2023)\nThis very good answer on stack exchange that explains the attention-concept ((https://stats.stackexchange.com/users/95569/dontloo), n.d.)",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting started with (L)LMs</span>"
    ]
  },
  {
    "objectID": "content/getting_started_with_llms.html#references",
    "href": "content/getting_started_with_llms.html#references",
    "title": "Getting started with (L)LMs",
    "section": "References",
    "text": "References\n\n\n\n\nAIAAIC - ChatGPT training emits 502 metric tons of carbon. (n.d.). https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chatgpt-training-emits-502-metric-tons-of-carbon.\n\n\nBahdanau, D. (2014). Neural machine translation by jointly learning to align and translate. arXiv Preprint arXiv:1409.0473. https://arxiv.org/abs/1409.0473\n\n\nBojanowski, P., Grave, E., Joulin, A., & Mikolov, T. (2017). Enriching Word Vectors with Subword Information (arXiv:1607.04606). arXiv. https://doi.org/10.48550/arXiv.1607.04606\n\n\nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (arXiv:1810.04805). arXiv. https://doi.org/10.48550/arXiv.1810.04805\n\n\nHeidloff, N. (2023). Foundation Models, Transformers, BERT and GPT. In Niklas Heidloff. https://heidloff.net/article/foundation-models-transformers-bert-and-gpt/.\n\n\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. de L., Hendricks, L. A., Welbl, J., Clark, A., Hennigan, T., Noland, E., Millican, K., Driessche, G. van den, Damoc, B., Guy, A., Osindero, S., Simonyan, K., Elsen, E., … Sifre, L. (2022). Training Compute-Optimal Large Language Models (arXiv:2203.15556). arXiv. https://doi.org/10.48550/arXiv.2203.15556\n\n\nHsieh, C.-Y., Li, C.-L., Yeh, C.-K., Nakhost, H., Fujii, Y., Ratner, A., Krishna, R., Lee, C.-Y., & Pfister, T. (2023). Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes (arXiv:2305.02301). arXiv. https://doi.org/10.48550/arXiv.2305.02301\n\n\n(https://stats.stackexchange.com/users/95569/dontloo), dontloo. (n.d.). What exactly are keys, queries, and values in attention mechanisms? Cross Validated.\n\n\nHussain, Z., Binz, M., Mata, R., & Wulff, D. U. (2024). A tutorial on open-source large language models for behavioral science. Behavior Research Methods, 56(8), 8214–8237. https://doi.org/10.3758/s13428-024-02455-8\n\n\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., & Amodei, D. (2020). Scaling Laws for Neural Language Models (arXiv:2001.08361). arXiv. https://doi.org/10.48550/arXiv.2001.08361\n\n\nMikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space (arXiv:1301.3781). arXiv. https://doi.org/10.48550/arXiv.1301.3781\n\n\nPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., & Zettlemoyer, L. (2018). Deep contextualized word representations (arXiv:1802.05365). arXiv. https://doi.org/10.48550/arXiv.1802.05365\n\n\nRadford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). Improving language understanding with unsupervised learning.\n\n\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2023). Attention Is All You Need (arXiv:1706.03762). arXiv. https://doi.org/10.48550/arXiv.1706.03762\n\n\nVillalobos, P., Ho, A., Sevilla, J., Besiroglu, T., Heim, L., & Hobbhahn, M. (2024, June). Position: Will we run out of data? Limits of LLM scaling based on human-generated data. Forty-First International Conference on Machine Learning.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting started with (L)LMs</span>"
    ]
  },
  {
    "objectID": "content/prompting.html",
    "href": "content/prompting.html",
    "title": "Prompting",
    "section": "",
    "text": "Instruct-tuned models\nPrompting describes the utilization of the ability of language models to use zero or few-shot instrutions to perform a task. This ability, which we briefly touched on when we were discussing the history of language models (i.e., the paper by Radford et al. (2019)), is one of the most important aspects of modern large language models.\nPrompting can be used for various tasks such as text generation, summarization, question answering, and many more.\nInstruct-tuned models are trained on a dataset (for an example, see Figure 4.1) that consists of instructions and their corresponding outputs. This is different from the pretraining phase of language models where they were trained on large amounts of text data without any specific task in mind. The goal of instruct-tuning is to make the model better at following instructions and generating more accurate and relevant outputs.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Prompting</span>"
    ]
  },
  {
    "objectID": "content/prompting.html#instruct-tuned-models",
    "href": "content/prompting.html#instruct-tuned-models",
    "title": "Prompting",
    "section": "",
    "text": "Fig 4.1: An example for a dataset that can be used for instruct-finetuning. This dataset can be found on huggingface\n\n\n\n\n\n\n\n\n\n📝 Task\n\n\n\nTest the difference between instruct and non-instruct-models.\nDo this by trying to get a gpt2-version (i.e., “QuantFactory/gpt2-xl-GGUF”) and a small Llama 3.2 Instruct-Model (i.e., “hugging-quants/Llama-3.2-1B-Instruct-Q8_0-GGUF” to write a small poem about the inception of the field of language modelling.\nUse LM-Studio to test this.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) A poem written by Llama 3.2 1B - a model with Instruct-Finetuning\n\n\n\n\n\n\n\n\n\n\n\n(b) A “poem” written by GPT2 - a model without Instruct-Finetuning\n\n\n\n\n\n\n\nFig 4.2: A poem and a “poem”\n\n\n\n\n\nShow answer",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Prompting</span>"
    ]
  },
  {
    "objectID": "content/prompting.html#prompting-strategies",
    "href": "content/prompting.html#prompting-strategies",
    "title": "Prompting",
    "section": "Prompting strategies",
    "text": "Prompting strategies\nThe results of a prompted call to a LM is highly dependent on the exact wording of the prompt. This is especially true for more complex tasks, where the model needs to perform multiple steps in order to solve the task. It is not for naught that the field of “prompt engineering” has emerged. There is a veritable plethora of resources available online that discuss different strategies for prompting LMs. It has to be said though, that the strategies that work and don’t work can vary greatly between models and tasks. A bit of general advice that holds true for nearly all models though, is to\n\ndefine the task in as many small steps as possible\nto be as literal and descriptive as possible and\nto provide examples if possible.\n\nSince the quality of results is so highly dependent on the chosen model, it is good practice to test candidate strategies against each other and therefore to define a target on which the quality of results can be evaluated. One example for such a target could be a benchmark dataset that contains multiple examples of the task at hand.\n\n\n\n\n\n\n\n📝 Task\n\n\n\n1. Test the above-mentioned prompting strategies on the MTOP Intent Dataset and evaluate the results against each other. The dataset contains instructions and labels indicating on which task the instruction was intended to prompt. Use a python script to call one of the following three models in LM-Studio for this:\n\nPhi 3.1 mini\nGemma 2 2B\nLlama 3.2 1B\n\nUse the F1-score implemented in scikit learn to evaluate your results.\n2. You do sometimes read very specific tips on how to improve your results. Here are three, that you can find from time to time:\n\nDo promise rewards (i.e., monetary tips) instead of threatening punishments\nDo formulate using affirmation (“Do the task”) instead of negating behaviours to be avoided (“Don’t do this mistake”)\nLet the model reason about the problem before giving an answer\n\nCheck these strategies on whether they improve your results. If your first instruction already results in near-perfect classification, brainstorm a difficult task that you can validate qualitatively. Let the model write a recipe or describe Kiel for example.\n3. Present your results\n3. Upload your code to moodle",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Prompting</span>"
    ]
  },
  {
    "objectID": "content/prompting.html#generation-of-synthetic-texts",
    "href": "content/prompting.html#generation-of-synthetic-texts",
    "title": "Prompting",
    "section": "Generation of synthetic texts",
    "text": "Generation of synthetic texts\nAs we discussed before, small models can perform on an acceptable level, if they are finetuned appropriately.\nA good way to do this is to use a larger model to generate synthetic data that you then use for training the smaller model. This approach has been used successfully in many applications, for example for improving graph-database queries (Zhong et al., 2024), for improving dataset search (Silva & Barbosa, 2024) or the generation of spreadsheet-formulas (Singh et al., 2024).\nSince even the largest LLMs are not perfect in general and might be even worse on some specific niche tasks, evidence suggests that a validation strategy for data generated in this way is beneficial (Kumar et al., 2024; Singh et al., 2024).\nStrategies to validate the synthetic data include:\n\nUsing a human annotator to label part of the data to test the models output\nForcing the model to answer in a structured way that is automatically testable (e.g., by using JSON)\nForcing the model to return 2 or more answers and checking for consistency\nCombining the two approaches above (i.e., forcing the model to return multiple structured outputs (JSON, XML, YAML, …) and checking for consistency)\nUsing a second LLM/different prompt to rate the answers\n\n\n\n\n\n\n\n📝 Task\n\n\n\nUsing your script for batch-testing different prompts, generate synthetic data for a emotion detection task based on Paul Ekman’s six basic emotions: anger, disgust, fear, happiness, sadness and surprise1.\nThe generated data should consist of a sentence and the emotion that is expressed in it. Start by generating two examples for each emotion. Validate these results and adapt them if necessary. Then use these examples to generate 100 samples for each emotion.\nUse one of the above mentioned (non-manual) strategies to validate the data you generated.\nUpload your results to Moodle.\n\n\n1 Though this nomenclature has fallen a bit out of fashion",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Prompting</span>"
    ]
  },
  {
    "objectID": "content/prompting.html#temperature",
    "href": "content/prompting.html#temperature",
    "title": "Prompting",
    "section": "Temperature",
    "text": "Temperature\nYou might have encountered eerily similar answers from the language model, especially in the last task. Talking of it - why does the model return different answers to the same prompt at all if we do use pretrained-models in the first place? Shouldn’t the utilization of the frozen weight-matrix result in the same answer, every time we run the model with the same input?\nYes, it should. And it does.\nRemember that a language model trained on language generation as we discussed in the first session ends in a softmax-layer that returns probabilities for each token in the vocabulary. The generation-pipeline does not just use the token with the highest probability though, but samples from this distribution. This means, that even if the input is identical, the output will be different every time you run the model.\nThe temperature parameter controls the steepness of the softmax-function and thus the randomness of the sampling process. A higher temperature value results in more random outputs, while a lower temperature value results in more “deterministic” outputs. The temperatur, indicated as a float between 0 and 12, is used to modulate the probabilities of the next token. This is done by adding a \\(\\frac{1}{Temp}\\) factor to the model-outputs before applying the softmax.\n2 Depending on the implementation, temperatures above 1 are also allowed. Temperatures above 1 are resultsing in strange behaviours - see Figure 4.3.This effectively changes the Sofmax-fomula from\n\\[\np_{Token} = \\frac{e^{z_{Token}}}{\\sum_{i=1}^k e^{z_{i}}}\n\\]\nto \\[\np_{Token}(Temp) = \\frac{e^{\\frac{z_{Token}}{Temp}}}{\\sum_{i=1}^k e ^{\\frac{z_{i}}{Temp}}}\n\\]\nWhere\n\n\\(z_{Token}\\) is the output for a given token\n\\(k\\) is the size of the vocabulary\n\\(Temp\\) is the temperature parameter (0 &lt; \\(Temp\\) &lt;= 1)\n\nThe effect of this temperature can be seen in Figure 4.3.\n\n\n\n\n\n\n\n\nFig 4.3: The effect of the temperature parameter on the softmax-output for a given input. The x-axis represents the temperature, the y-axis represents the token-position and the color represents the probability of the token.\n\n\n\n\n\nMost generation-frameworks do additionally provide a parameter called top_k or top_p. These parameters are used to limit the number of tokens that can be selected as the next token. This is done by sorting the probabilities in descending order and only considering the top k tokens or the top p percent of tokens.\nTemperature is the mayor setting to controll a LLMs “creativity” though.\n\n\n\n\n\n\n📝 Task\n\n\n\nUsing the script provided for generating snthetic data, test the effect of the temperature parameter on the output of the model.\n\nUse the same prompt and the same model\nRun the model with a temperature value of 0.1, 0.5, 1.0 and 2.0",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Prompting</span>"
    ]
  },
  {
    "objectID": "content/prompting.html#further-readings",
    "href": "content/prompting.html#further-readings",
    "title": "Prompting",
    "section": "Further Readings",
    "text": "Further Readings\n\nThis prompting-guide has some nice general advice\nOpenAI has its own set of tipps\ndeepset, the company behind Haystack, has a nice guide as well\nThis blog-article, again written by Heidloff (Heidloff, 2023)",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Prompting</span>"
    ]
  },
  {
    "objectID": "content/prompting.html#references",
    "href": "content/prompting.html#references",
    "title": "Prompting",
    "section": "References",
    "text": "References\n\n\n\n\nHeidloff, N. (2023). Fine-tuning small LLMs with Output from large LLMs. In Niklas Heidloff. https://heidloff.net/article/fine-tune-small-llm-with-big-llm/.\n\n\nKumar, B., Amar, J., Yang, E., Li, N., & Jia, Y. (2024). Selective Fine-tuning on LLM-labeled Data May Reduce Reliance on Human Annotation: A Case Study Using Schedule-of-Event Table Detection (arXiv:2405.06093). arXiv. https://doi.org/10.48550/arXiv.2405.06093\n\n\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language models are unsupervised multitask learners. OpenAI Blog, 1(8), 9.\n\n\nSilva, L., & Barbosa, L. (2024). Improving dense retrieval models with LLM augmented data for dataset search. Knowledge-Based Systems, 294, 111740. https://doi.org/10.1016/j.knosys.2024.111740\n\n\nSingh, U., Cambronero, J., Gulwani, S., Kanade, A., Khatry, A., Le, V., Singh, M., & Verbruggen, G. (2024). An Empirical Study of Validating Synthetic Data for Formula Generation (arXiv:2407.10657). arXiv. https://doi.org/10.48550/arXiv.2407.10657\n\n\nZhong, Z., Zhong, L., Sun, Z., Jin, Q., Qin, Z., & Zhang, X. (2024). SyntheT2C: Generating Synthetic Data for Fine-Tuning Large Language Models on the Text2Cypher Task (arXiv:2406.10710). arXiv. https://doi.org/10.48550/arXiv.2406.10710",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Prompting</span>"
    ]
  },
  {
    "objectID": "content/agent_basics.html",
    "href": "content/agent_basics.html",
    "title": "Agent basics",
    "section": "",
    "text": "What is an agent?\n“An AI agent is a system that uses an LLM to decide the control flow of an application.” (“What Is an AI Agent?” 2024)\nIn the context of large language models, agents are LLM-based systems that can solve complex tasks. Imagine asking a question like:\n“What were the key learnings from the Generative AI elective module in WiSe 24/25 at FH Kiel?”\nCould you just ask an LLM that question and expect a correct answer?\nIt is in theory possible, that an LLM could answer that directly, but only if it was trained on this information, that is, if a text describing the module exists, is accessible from the web and was used in training the model. However, usually we can not expect the LLM to have this knowledge.\nLet’s think for a moment how a human would answer that (one that did not attend the module). We would probably try to get a copy of the script, maybe we saved the script to our hard drive or other data storage. Maybe we could search the web for a description or text version of the module. Having obtained a copy of the script, we would probably read it. Then, we would try to distill the information hidden therein, to answer the question.\nSo, for our LLM to answer that question, it needs to be able to perform several tasks:\nThis is where agents come into play. Agents are LLM-based systems that can solve complex tasks by performing several subtasks in sequence, using an LLM to decide which subtask to perform next. In our example, the agent would first search the web for relevant documents, then read and understand them, summarize them and finally answer the question based on the summary.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Agent basics</span>"
    ]
  },
  {
    "objectID": "content/agent_basics.html#what-is-an-agent",
    "href": "content/agent_basics.html#what-is-an-agent",
    "title": "Agent basics",
    "section": "",
    "text": "Searching the web for relevant documents\nsearching in a local file storage or other database\nReading and understanding a document\nSummarizing the content of a document\nAnswering questions based on the summary of a document",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Agent basics</span>"
    ]
  },
  {
    "objectID": "content/agent_basics.html#agent-framework",
    "href": "content/agent_basics.html#agent-framework",
    "title": "Agent basics",
    "section": "Agent framework",
    "text": "Agent framework\n\n\n\nArchitecture of the agent framework (LLM Agents – Nextra, 2024)\n\n\nTo facilitate this, an agent system consists of several components:\n\nAgent: the agent core acting as coordinator\nPlanning: Assists the agent in breaking down the complex task into subtasks\nTools: functions that the agent can use to perform a specific task\nMemory: used to store information about previous interactions with the agent\n\nWe will describe each of them below.\n\nAgent\nThis is a general-purpose LLM, that functions as the brain and main decision-making component of an agent. It determines which tools to use and how to combine their results to solve complex tasks. The agent core uses the output of the previous tool as input for the next tool. It also uses an LLM to decide when to stop using tools and return a final answer. The behavior of the agent and the tools, it has at its disposal, is defined by a prompt template.\n\n\nPlanning\nPlanning is the process of breaking down a complex task into subtasks and deciding which tools to use for each subtask. The planning module is usually also an LLM, it can be one fine-tuned to this specific task or receive a specialized prompt. It uses techniques like chain-of-thought (CoT) prompting to generate a plan of action (Wei et al., 2023). CoT prompting is a technique that encourages the model to explain its reasoning step by step, making it easier for us to understand and evaluate its answers. Other strategies include Tree-of-Thoughts (Long, 2023), (Yao, Yu, et al., 2023), (Hulbert, 2023) or ReAct (Yao, Zhao, et al., 2023).  We will discuss these in more detail later.\n\n\nTools\nTools are functions that the agent can use to perform a specific task. They can be pre-defined or dynamically generated based on the user’s needs. Tools can be simple, such as a calculator, or complex, such as a web search engine. Tools can also be other agents, allowing for the creation of multi-agent systems. In our example, the tools would be a web search engine and a document reader. Other popular tools are a data store or a python interpreter.\n\n\nMemory\nMemory is used to store information about previous interactions with the agent. This allows the agent to remember past conversations and use this information in future interactions. Memory can be short-term, such as a conversation buffer, or long-term, such as a database. Memory can also be used to store the results of previous tool uses, allowing the agent to reuse them if necessary.\n\n\nChain-of-Thought prompting\nChain-of-Thought (CoT) prompting refers to the technique of giving the LLM hints in the user input on how to solve the problem step by step, similar to what we did above. In the original paper, this was used with few-shot prompting (giving the LLM examples in the prompt), see figure below. But it is also possible to use it with zero-shot prompting (i.e. without examples) by invoking the magical words “Let’s think step by step” (Kojima et al., 2023)1.\n1 Note that these informations get old fast. Newer LLMs may have this functionality build in already\n\n\nChain-of-Thought prompting illustrated (Wei et al., 2023)\n\n\n\n\nTree of Thoughts\nTree of Thoughts (ToT) is a generalization on CoT prompting. The papers on ToT are somewhat complex, so we will not discuss them in detail here. In short, LLMs are used to generate thoughts, that serve as intermediate steps towards the solution. The difference to CoT is basically, that several thoughts are generated at each step, creating a tree-like structure. This tree is then searched using breadth-first search or depth-first search until a solution is found. A simplified example is given by (Hulbert, 2023):\nImagine three different experts are answering this question.\nAll experts will write down 1 step of their thinking,\nthen share it with the group.\nThen all experts will go on to the next step, etc.\nIf any expert realises they're wrong at any point then they leave.\nThe question is...\n\n\nReAct\nReAct (short for Synergizing Reasoning and Acting) is a technique based on CoT, that updates its reasoning after each step of tool use. This allows the agent to react (pun intended) to unforeseen results during the step-by-step solution i.e. failed tool use. The agent can then follow a different chain of thoughts. This makes it very well suited to tool use. An illustration is given in the figure below.\n\n\n\nComparison of ReAct with other prompting techniques (Yao, Zhao, et al., 2023)",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Agent basics</span>"
    ]
  },
  {
    "objectID": "content/agent_basics.html#examples-of-agent-frameworks-llamaindex-langchain-haystack",
    "href": "content/agent_basics.html#examples-of-agent-frameworks-llamaindex-langchain-haystack",
    "title": "Agent basics",
    "section": "Examples of agent-frameworks (Llamaindex, LangChain & Haystack)",
    "text": "Examples of agent-frameworks (Llamaindex, LangChain & Haystack)\nThere are a lot of agent frameworks out there. In this module we will focus on three of them: LlamaIndex, LangChain and Haystack. They all have their own strengths and weaknesses, but they all share the same basic architecture as described above. We will describe each of them below.\n\nLlamaindex: LlamaIndex is a data framework for your LLM applications. It provides a central interface to connect your LLMs and your data. It also provides a set of tools to help you build your own applications, such as a document reader, a web search engine, a data store, etc. - LangChain: LangChain is a framework for developing applications powered by language models. It provides a set of tools to help you build your own applications, such as a document reader, a web search engine, a data store, etc. It also provides a set of agents that can use these tools to solve complex tasks.\nHaystack: Haystack is an open source NLP framework that enables you to build production-ready applications around LLMs and other models. It provides a set of tools to help you build your own applications, such as a document reader, a web search engine, a data store, etc. It also provides a set of agents that can use these tools to solve complex tasks.\n\n\n\n\n\n\n\n\n📝 Task\n\n\n\nNow it is your turn!\nEach group is to use one of the following frameworks to build a small demo agent:\n\nLlamaindex combine this approach with this notebook to make it work with LM Studio.\nLangchain\nHaystack\n(optional) another framework of your choice\n\n\nSet up a local LLM (e.g. using Ollama or LM Studio) to be used by the agent.\nChoose a small task for your agent, e.g. answering questions about a specific topic, summarizing a document, etc. (use the one in the respective tutorial)\nImplement the agent using one of the frameworks listed above.\nPresent your results and your experiences with the frameworks in a short presentation.\nSubmit your code and report on moodle.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Agent basics</span>"
    ]
  },
  {
    "objectID": "content/agent_basics.html#further-readings",
    "href": "content/agent_basics.html#further-readings",
    "title": "Agent basics",
    "section": "Further Readings",
    "text": "Further Readings\n\nThis paper compares different planning strategies\nIn addition to the websites listed above see also (“Introduction to LLM Agents,” 2023)",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Agent basics</span>"
    ]
  },
  {
    "objectID": "content/agent_basics.html#references",
    "href": "content/agent_basics.html#references",
    "title": "Agent basics",
    "section": "References",
    "text": "References\n\n\n\n\nHulbert, D. (2023). Using Tree-of-Thought Prompting to boost ChatGPT’s reasoning. https://github.com/dave1010/tree-of-thought-prompting.\n\n\nIntroduction to LLM Agents. (2023). In NVIDIA Technical Blog. https://developer.nvidia.com/blog/introduction-to-llm-agents/.\n\n\nKojima, T., Gu, S. S., Reid, M., Matsuo, Y., & Iwasawa, Y. (2023). Large Language Models are Zero-Shot Reasoners (arXiv:2205.11916). arXiv. https://doi.org/10.48550/arXiv.2205.11916\n\n\nLLM Agents – Nextra. (2024). https://www.promptingguide.ai/research/llm-agents.\n\n\nLong, J. (2023). Large Language Model Guided Tree-of-Thought (arXiv:2305.08291). arXiv. https://arxiv.org/abs/2305.08291\n\n\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., & Zhou, D. (2023). Chain-of-Thought Prompting Elicits Reasoning in Large Language Models (arXiv:2201.11903). arXiv. https://doi.org/10.48550/arXiv.2201.11903\n\n\nWhat is an AI agent? (2024). In LangChain Blog. https://blog.langchain.dev/what-is-an-agent/.\n\n\nYao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y., & Narasimhan, K. (2023). Tree of Thoughts: Deliberate Problem Solving with Large Language Models (arXiv:2305.10601). arXiv. https://arxiv.org/abs/2305.10601\n\n\nYao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., & Cao, Y. (2023). ReAct: Synergizing Reasoning and Acting in Language Models (arXiv:2210.03629). arXiv. https://doi.org/10.48550/arXiv.2210.03629",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Agent basics</span>"
    ]
  },
  {
    "objectID": "content/embeddings.html",
    "href": "content/embeddings.html",
    "title": "Embedding-based agent-systems",
    "section": "",
    "text": "Semantic embeddings and vector stores\nAll agents we discussed until here are using tools that allow them to use their generated inputs in some way. In most of the task we want to utilize agents, we do not only want to generate text but to also inform the generation based on some kind of existing knowledge base. Examples for these kinds of usecases include:\nThough most modern LLMs are increasingly capable in answering basic knowledge-questions, the more comples a topic or the more relevant the factual basis of an answer is, the more it is important to base generated answers on actual data.\nTo empower an agent too look up information during its thought-process, one has to build a tool that allows an agent to use natural language to retrieve information necessary for a task. The fundamental principle to do this are so-called semantic embeddings. These are pretty close to the concept we introduced when talking about the foundations of LLMs (see here) and can be understood as a way to map textual data into a vector space. The main idea is that semantically similar texts should have similar embeddings, i.e., they are close in the vector space. Close in this context is meant as having a reasonibly small distance between them. The go-to standard to measure this distance is the cosine similarity, which has proven usefull enough to be the standard for a range of semantic retrieval implementations (i.e., they are used in OpenAI tutorials and in Azure embedding-applications). The cosine similarity is defined as:\n\\[\n\\text{cosine\\_similarity}(u, v) = \\frac{u \\cdot v}{\\|u\\| \\|v\\|} = \\frac{\\sum_{i=1}^{n} u_i v_i}{\\sqrt{\\sum_{i=1}^{n} u_i^2} \\sqrt{\\sum_{i=1}^{n} v_i^2}}\n\\] The rationale here is that sequences with semantically similar contents should point to similar directions in the high dimensional vector space. See Figure 6.1 for an illustration of this and other common similarity concepts seen in semantic retrieval.\n(a) Illustration of “semantic embeddings” of different word.\n\n\n\n\n\n\n\n\n\n\n\n(b) Illustration of 4 common similarity concepts seen in semantic retrieval: cosine, euclidean, dot product and manhattan. dot product and cosine are taking the direction of the vector into account, while the cosine ignores the length of the vectors and the dot product does not. Manhattan and euclidean are both measuring the distance between two points in a vector space, but they do it differently. Euclidean is the straight line between two points, while manhattan is the sum of the absolute differences between the coordinates of the two points.\n\n\n\n\n\n\nFig 6.1: Illustration of common similarity metrics in semantic search.\nAs always, there is not the one solution to all problems though and the applicability of cosine similarity might not be optimal for your usecase (Goyal & Sharma, 2022; Steck et al., 2024).\nThough one could use any kind of (L)LM to calculate embeddings for this case1, it is advisable to use models specifically trained for this purpose. Reimers & Gurevych (2019) proposed Sentence-BERT which is a simple but effective approach to calculate semantic embeddings. SBERT and similar approaches are based on a (L)LM that was trained to predict missing words as we discussed before, resulting in a general representation of natural language. In the case of the original paper, they used (among others) the BERT model Devlin et al. (2019) mentioned before.\nThe authors then use this to embed a pair of sentences into one embedding-vector each2, for which some measure of semantic similarity is known. An example for a dataset containing such sentences is the Stanford Natural Language Inferenc(SNLI) corpus Bowman et al. (2015) which labels 550k pairs of sentences as either entailment, contradiction or neutral. Reimers & Gurevych (2019) then concated the both senteces embeddings and their element-wise difference into a single vector which is fed to a multiclass classifier, indicating in which category the sentences relationship falls. At inference, this classification head was removed and replaced as the cosine similarity as discussed above. The resulting network is highly effective in calculating semantic similarities between sentences.\nA look at the sbert-website shows that the module has somewhat grown and now does supply a series of learning paradigms that can be used to efficiently tune a model for your specific usecase3. As the library has grown, so has the sheer amount of pretrained embedding-models in some way based on this architecture that are hosted on huggingface. The MTEB-Leaderboard is a good strat to search for a model for your application. One utilization of this model-family, which has already been implicitly used in this script, is their very efficient ability to semantically search for documents. If a model is very good at finding similar sentences, it can also be very good to find documents that are very similar to a question.\nLook at the example illustrated in Figure 6.2. The question “why is the sky blue” embedded with the same model as our 5 documents stating some facts.\nWe can then calculate the cosine-similarity between these embeddings and return the document, that has the highest similarity to our question.\nThis approach of using a model to embed documents and questions into a vector space is the basis for the so-called Retrieval augmented generation.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Embedding-based agent-systems</span>"
    ]
  },
  {
    "objectID": "content/embeddings.html#semantic-embeddings-and-vector-stores",
    "href": "content/embeddings.html#semantic-embeddings-and-vector-stores",
    "title": "Embedding-based agent-systems",
    "section": "",
    "text": "1 And there are approaches to use LLMs to solve this taks i.e., Jiang et al. (2023)2 The original BERT-paper did this by adding a pooling layer before the task-header that extracted and weighed the context-dependend embedding of the first token. The SBERT paper tried different pooling-strategies and used a mean over each embedding dimension of the sequence.\n\n3 And this does not have to be expensive. Tunstall et al. (2022) have shown a highly efficient contrastive learning paradigm that limts the amount of necessary labels for a ridiuculously small amount of labels.\n\n\n\n\n\n\nFig 6.2: Illustration of the usage of embedding-based distances in retrieval.\n\n\n\n\n\n\n\n\n\n\n📝 Task\n\n\n\nInstall the sentence-transformer package and download the climate_fever-dataset.\nChoose one model from the MTEB-Leaderboard that you deem adequatly sized and appropriate for the task\nTest the different metrics for the first twenty claims of the dataset and a question you formulate.\nUse the similarity-implementations from sklearn.metrics.pairwise.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Embedding-based agent-systems</span>"
    ]
  },
  {
    "objectID": "content/embeddings.html#retrieval-augmented-generation",
    "href": "content/embeddings.html#retrieval-augmented-generation",
    "title": "Embedding-based agent-systems",
    "section": "Retrieval augmented generation",
    "text": "Retrieval augmented generation\nRetrieval augmented generation (RAG) is a framework that does pretty much do what it says on the tin. You use a retrieval model to find documents that are similar to your question and then either return these documents our feed them into a generative model, which then generates an answer based on these documents. This process can additionally be wrapped as a tool to be used by an agent, so that your existing agent can now also use external knowledge sources to answer questions.\nRetrieval does not have to be semantics-based in this context - all kinds of data sources and databases can be made accessible for a LLM - we will focus on a purely embbedding based approach here though.\nAlthough the small example in the last task was working, it is not really scalable. It was fine for a limited set of examples, if you want to realistically make a whole knowledge base searchable, you need to use an appropriate database system.\n\nVector databases\nA vector database is a database that stores vectors and allows for efficient similarity searches. As can be seen in the db-engines ranking there has been a surge of interest in this area recently, with many new players entering the market. From the plethora of vector databases, these three are examples that virtue a honorary mention:\n\nChroma - a in-memory database for small applications that is especially easy to get to run.\nElasticsearch - a well established database that is the go to system for open source search engines and has recently (and kind of naturally) also branched out into vector databases.\nQdrant - the product of a Berlin-based startup that focusses on stability and scalability. It can also run in memory, but does natively support hard drive storage.\n\nThe best way to use qdrant is to use docker to run it and the python sdk to interact with it. Since version 1.1.1, the sdk also allows to just run the client in memory.\n\n\n\n\n\n\n📝 Task\n\n\n\nInstall the qdrant-client python-sdk and fastembed.\nCreate a collection for the claims and one for the evidence in the climate_fever-dataset. Add the first 200 entries to each of these collections. Use qdrants fastembemd-integration to do this.\nTest the similarity search on a question you formulate.\n\n\n\n\nRAG\nThe last step to make this into a RAG pipeline is to use a generative model to answer the question based on the retrieved documents.\nThis means, that we do collect the relevant documents like we did before, still based on a natural language question, but instead of returning the hits we got from the index, we feed them into a LLM and ask it to generate an answer based on these documents. This is where the name retrieval augmented generation comes from - we use the retrieval step to augment the generative model with additional information. The diagram in Figure 6.3 illustrates this process.\n\n\n\n\n\n\nFig 6.3: Illustration of a RAG-system.\n\n\n\n\n\n\n\n\n\n📝 Task\n\n\n\nImplement a RAG pipeline for the climate_fever dataset using qdrant as vector database and a LLM of your choice for the summarization.\nTry to find a prompt that results in the LLM\n\nusing the information given\nnot inventing new information\nreferencing the source of the information it uses\n\nUpload your results until here (embedder, database and summarization) to moodle.\n\n\nMost agent frameworks provide integrations for a variety of vector databases.\nIn terms of llamaindex, there are not just one but two tutorials on how to get qdrant to integrate into your agent, one from qdrant for general integration and one from llamaindex.\nThe pipeline is pretty close to what we discussed until here, it just uses the llamaindex-typical wrapper classes. See Tip 6.1 for an example RAG-system implemented in Llamaindex.\n\n\n\n\n\n\nTip 6.1: Llamaindex Rag\n\n\n\n\n\nThe first thing in both the Llamaindex and the manual way of creating a retrieval pipeline is the setup of a vector database:\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import Distance, VectorParams, Batch\nDIMENSIONS = 384\nclient = QdrantClient(location=\":memory:\")\nTo store data and query the database, we have to load a embedding-model. As in the manual way of creating a retrieval pipeline discussed before, we can use a huggingface-SentenceTranformer model. But instead of using the SentenceTransformer class from the sentence_transformers library, we have to use the HuggingFaceEmbedding class from Llamaindex. This model is entered into the Llamaindex-Settings.\nfrom llama_index.core import Settings\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\nembed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L12-v2\")\nSettings.embed_model = embed_model\nThe next step is to wrap the vector-store into a Llamaindex-VectorStoreIndex. This index can be used to add our documents to the database.\nfrom llama_index.vector_stores.qdrant import QdrantVectorStore\n\nvector_store = QdrantVectorStore(client=client, collection_name=\"paper\")\nAs an example, we will add the “Attention is all you need” paper. This is how the head of our txt-file looks like:\n         Attention Is All You Need\narXiv:1706.03762v7 [cs.CL] 2 Aug 2023\n\n\n\n\n                                             Ashish Vaswani∗                Noam Shazeer∗               Niki Parmar∗             Jakob Uszkoreit∗\n                                              Google Brain                   Google Brain             Google Research            Google Research\n                                          avaswani@google.com             noam@google.com            nikip@google.com            usz@google.com\nSince we can not just dump the document at once, we will chunk it in sentences (more about that later). This can be done like this (ignore the parameters by now, we will look at them later):\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core import Document\n\nnode_parser = SentenceSplitter(chunk_size=100, chunk_overlap=20)\n\nnodes = node_parser.get_nodes_from_documents(\n    [Document(text=text)], show_progress=False\n)\nThese documents are then added to our database and transformed in an index llamaindex can use:\nfrom llama_index.core import VectorStoreIndex\n\nindex = VectorStoreIndex(\n    nodes=nodes,\n    vector_store=vector_store,\n)\nThis index can already be used to retrieve documents from the database (by converting it to a retriever).\nretriever = index.as_retriever(similarity_top_k=10)\nretriever.retrieve('What do the terms Key, Value and Query stand for in self-attention?')\n[NodeWithScore(node=TextNode(id_='04c12537-5f33-4d41-a4d4-df30d2aed6e4', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={&lt;NodeRelationship.SOURCE: '1'&gt;: RelatedNodeInfo(node_id='847f2be4-3799-41b5-80c0-b390298eba24', node_type=&lt;ObjectType.DOCUMENT: '4'&gt;, metadata={}, hash='74e64008cffed21d58edef5058f6cf6b3bc853bf936b83eefb70563168b73c5a'), &lt;NodeRelationship.PREVIOUS: '2'&gt;: RelatedNodeInfo(node_id='22d5c0dc-d921-4790-ac6e-4f6a6d5f336f', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='772c092906000e119c69ad2e5cb90148a6c8b113d54a20fb9d5984d6a9695ee8'), &lt;NodeRelationship.NEXT: '3'&gt;: RelatedNodeInfo(node_id='893d077f-a8ab-4a3f-9765-69ef72d46ec4', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='df269253fe4504ec666a0a40380f9399466c5bd366c7ce6c853ee45b31d4bc84')}, text='of the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n\\n3.2.1   Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk , and\\n                                    √ values of dimension dv .', mimetype='text/plain', start_char_idx=11715, end_char_idx=12088, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.588352239002419),\n NodeWithScore(node=TextNode(id_='c42d8e8c-24ac-447a-8058-d62d198ce9eb', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={&lt;NodeRelationship.SOURCE: '1'&gt;: RelatedNodeInfo(node_id='847f2be4-3799-41b5-80c0-b390298eba24', node_type=&lt;ObjectType.DOCUMENT: '4'&gt;, metadata={}, hash='74e64008cffed21d58edef5058f6cf6b3bc853bf936b83eefb70563168b73c5a'), &lt;NodeRelationship.PREVIOUS: '2'&gt;: RelatedNodeInfo(node_id='e961df5f-04be-4bf8-bba0-b30b346e6e3e', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='944203475caa494a68b2ca15140cea2278792db8546209bcc538388bf227b57d'), &lt;NodeRelationship.NEXT: '3'&gt;: RelatedNodeInfo(node_id='12962f1d-060f-49d3-9ff9-be2dceb23736', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='46773d9899458459b747af4980832a961621033663b11cb056304074633c0f14')}, text='Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].', mimetype='text/plain', start_char_idx=8003, end_char_idx=8396, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.5581949233902119),\n NodeWithScore(node=TextNode(id_='893d077f-a8ab-4a3f-9765-69ef72d46ec4', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={&lt;NodeRelationship.SOURCE: '1'&gt;: RelatedNodeInfo(node_id='847f2be4-3799-41b5-80c0-b390298eba24', node_type=&lt;ObjectType.DOCUMENT: '4'&gt;, metadata={}, hash='74e64008cffed21d58edef5058f6cf6b3bc853bf936b83eefb70563168b73c5a'), &lt;NodeRelationship.PREVIOUS: '2'&gt;: RelatedNodeInfo(node_id='04c12537-5f33-4d41-a4d4-df30d2aed6e4', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='4dc2893909c949675d444324e091b9dcae176eafe0faeb456e4f571f79863ac8'), &lt;NodeRelationship.NEXT: '3'&gt;: RelatedNodeInfo(node_id='e48f428a-1d0f-4830-8aca-82cbf4cd4b67', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='4a7481ff7440b3355d18a8f77fdbcf637903e138a37a44c74d4fd287baf610f2')}, text='We compute the dot products of the\\nquery with all keys, divide each by dk , and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V .', mimetype='text/plain', start_char_idx=12089, end_char_idx=12415, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.5557579023667499),\n NodeWithScore(node=TextNode(id_='0146f53a-f1b1-4d80-a333-26746920ab9d', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={&lt;NodeRelationship.SOURCE: '1'&gt;: RelatedNodeInfo(node_id='847f2be4-3799-41b5-80c0-b390298eba24', node_type=&lt;ObjectType.DOCUMENT: '4'&gt;, metadata={}, hash='74e64008cffed21d58edef5058f6cf6b3bc853bf936b83eefb70563168b73c5a'), &lt;NodeRelationship.PREVIOUS: '2'&gt;: RelatedNodeInfo(node_id='c0f333cd-8860-48e5-b177-649855617c5a', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='c5cea5e4a2c19b51c1912e3fbb06fd9f445f2ab46a888146c9540685c513a907'), &lt;NodeRelationship.NEXT: '3'&gt;: RelatedNodeInfo(node_id='5d433fd9-785b-4f25-b3b0-5cd206b0ca37', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='e52e557964f178c114303403bfab945ce6fc6bc18fbc723bc2c110071beaf965')}, text='• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\n           and queries come from the same place, in this case, the output of the previous layer in the\\n           encoder. Each position in the encoder can attend to all positions in the previous layer of the\\n           encoder.', mimetype='text/plain', start_char_idx=16021, end_char_idx=16345, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.5531707169222685),\n NodeWithScore(node=TextNode(id_='22d5c0dc-d921-4790-ac6e-4f6a6d5f336f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={&lt;NodeRelationship.SOURCE: '1'&gt;: RelatedNodeInfo(node_id='847f2be4-3799-41b5-80c0-b390298eba24', node_type=&lt;ObjectType.DOCUMENT: '4'&gt;, metadata={}, hash='74e64008cffed21d58edef5058f6cf6b3bc853bf936b83eefb70563168b73c5a'), &lt;NodeRelationship.PREVIOUS: '2'&gt;: RelatedNodeInfo(node_id='71788dae-10dc-4341-8ebd-250a8836bce5', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='f1c9e10879cdc5796376d70528c5ccd9d988818269ef633ea539e6d2df1922d1'), &lt;NodeRelationship.NEXT: '3'&gt;: RelatedNodeInfo(node_id='04c12537-5f33-4d41-a4d4-df30d2aed6e4', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='4dc2893909c949675d444324e091b9dcae176eafe0faeb456e4f571f79863ac8')}, text='3.2   Attention\\n\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n\\n\\n                                                  3\\n\\x0c           Scaled Dot-Product Attention                                  Multi-Head Attention\\n\\n\\n\\n\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.', mimetype='text/plain', start_char_idx=11208, end_char_idx=11712, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.5503383930857552),\n NodeWithScore(node=TextNode(id_='55481635-fcaa-4e90-9625-9b0c3bfa3109', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={&lt;NodeRelationship.SOURCE: '1'&gt;: RelatedNodeInfo(node_id='847f2be4-3799-41b5-80c0-b390298eba24', node_type=&lt;ObjectType.DOCUMENT: '4'&gt;, metadata={}, hash='74e64008cffed21d58edef5058f6cf6b3bc853bf936b83eefb70563168b73c5a'), &lt;NodeRelationship.PREVIOUS: '2'&gt;: RelatedNodeInfo(node_id='923d6eec-1ba9-4972-b457-47cc1cb5e5a7', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='534fa8133845bae34a1c58d14d5fe840710190a12c4951fa24b1acaaa4ed8e35'), &lt;NodeRelationship.NEXT: '3'&gt;: RelatedNodeInfo(node_id='ea0b511f-4179-4f64-8e5b-1cf5f6d76404', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='a958edeb1ca826ae9eb259fb9846f6fe7d822b9462583eea56914ae0383170e5')}, text='.                       .              .\\n                                                                                                                 &lt;EOS&gt;       &lt;EOS&gt;            &lt;EOS&gt;                 &lt;EOS&gt;\\n                                                                                                                  &lt;pad&gt;      &lt;pad&gt;             &lt;pad&gt;                &lt;pad&gt;\\n\\n\\n\\n\\n     Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\n     Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution.', mimetype='text/plain', start_char_idx=55980, end_char_idx=56574, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.46287885047540767),\n NodeWithScore(node=TextNode(id_='04b195bd-26e4-4d8c-afdc-780e96bdd345', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={&lt;NodeRelationship.SOURCE: '1'&gt;: RelatedNodeInfo(node_id='847f2be4-3799-41b5-80c0-b390298eba24', node_type=&lt;ObjectType.DOCUMENT: '4'&gt;, metadata={}, hash='74e64008cffed21d58edef5058f6cf6b3bc853bf936b83eefb70563168b73c5a'), &lt;NodeRelationship.PREVIOUS: '2'&gt;: RelatedNodeInfo(node_id='c28b6b26-7bbf-4682-9399-a7804be460ae', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='c3ad5697d4d156dd0b4c85c17741ee433c10899ddffbd3575904ce08cd6736de'), &lt;NodeRelationship.NEXT: '3'&gt;: RelatedNodeInfo(node_id='c0f333cd-8860-48e5-b177-649855617c5a', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='c5cea5e4a2c19b51c1912e3fbb06fd9f445f2ab46a888146c9540685c513a907')}, text='3.2.3    Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n\\n         • In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\n           and the memory keys and values come from the output of the encoder. This allows every\\n           position in the decoder to attend over all positions in the input sequence.', mimetype='text/plain', start_char_idx=15478, end_char_idx=15877, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.4550194901912972),\n NodeWithScore(node=TextNode(id_='d93b8e55-28cb-417e-838a-a22abf7cfbc9', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={&lt;NodeRelationship.SOURCE: '1'&gt;: RelatedNodeInfo(node_id='847f2be4-3799-41b5-80c0-b390298eba24', node_type=&lt;ObjectType.DOCUMENT: '4'&gt;, metadata={}, hash='74e64008cffed21d58edef5058f6cf6b3bc853bf936b83eefb70563168b73c5a'), &lt;NodeRelationship.PREVIOUS: '2'&gt;: RelatedNodeInfo(node_id='398e22c4-5cd8-42ed-ba1d-43f213413bc2', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='cd837bc3b60f4cff2ab7f296f85515886d65e8ca7c2a3fb9c7b10fb1c6904949'), &lt;NodeRelationship.NEXT: '3'&gt;: RelatedNodeInfo(node_id='e9ffed0b-00f1-4408-bd5d-512f5d05138d', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='d0057b6da67faef5766281c2cae5a165b6e5396059cd7c09222a6d9e77ca985c')}, text='On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv -dimensional\\n   4\\n     To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = di=1\\n                                                                        P k\\n                                                                              qi ki , has mean 0 and variance dk .', mimetype='text/plain', start_char_idx=14037, end_char_idx=14560, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.45141889186813816),\n NodeWithScore(node=TextNode(id_='158309a7-9a7a-47e6-ac58-1a4e98eee41b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={&lt;NodeRelationship.SOURCE: '1'&gt;: RelatedNodeInfo(node_id='847f2be4-3799-41b5-80c0-b390298eba24', node_type=&lt;ObjectType.DOCUMENT: '4'&gt;, metadata={}, hash='74e64008cffed21d58edef5058f6cf6b3bc853bf936b83eefb70563168b73c5a'), &lt;NodeRelationship.PREVIOUS: '2'&gt;: RelatedNodeInfo(node_id='e4e96748-8e42-4c45-a1b3-3e0b2a179475', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='162e546ee2aace8fdcf9330a044ed33bc46d32219ec57c876b93a1fad69425e7'), &lt;NodeRelationship.NEXT: '3'&gt;: RelatedNodeInfo(node_id='8229d93a-1fb8-492f-9227-2b13658180f7', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='e9431f13405886d724857fa8ba6e9d0bd84affbaf2d35beedeeda36e79d95de8')}, text='4     Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1 , ..., xn ) to another sequence of equal length (z1 , ..., zn ), with xi , zi ∈ Rd , such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.', mimetype='text/plain', start_char_idx=20488, end_char_idx=20939, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.4348473100243987),\n NodeWithScore(node=TextNode(id_='721c5981-90a9-4046-a757-4593a362ddf7', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={&lt;NodeRelationship.SOURCE: '1'&gt;: RelatedNodeInfo(node_id='847f2be4-3799-41b5-80c0-b390298eba24', node_type=&lt;ObjectType.DOCUMENT: '4'&gt;, metadata={}, hash='74e64008cffed21d58edef5058f6cf6b3bc853bf936b83eefb70563168b73c5a'), &lt;NodeRelationship.PREVIOUS: '2'&gt;: RelatedNodeInfo(node_id='a07f95e3-64fb-4637-ac18-4a928541df80', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='b11620062050474b2e5a6317e981c8ad07b227f032ebe169b1cb4f87c8994aa6'), &lt;NodeRelationship.NEXT: '3'&gt;: RelatedNodeInfo(node_id='165241f9-efb1-433a-896b-b6ea61168d3f', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='559f529f69207d17371f20407b6f1b4691910f9c8a90c9cefbb741e95fbf5de9')}, text='Operations\\n      Self-Attention                      O(n2 · d)             O(1)                O(1)\\n      Recurrent', mimetype='text/plain', start_char_idx=18618, end_char_idx=18733, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.4276254505797798)]\nThe retriever can then directly be use as a tool to answer questions about our documents:\nfrom llama_index.core.tools import BaseTool, FunctionTool\n\ndef find_references(question: str) -&gt; str:\n    \"\"\"Query a database containing the paper \"Attention is all you Need\" in parts.\n    This paper introduced the mechanism of self-attention to the NLP-literature.\n    Returns a collection of scored text-snippets that are relevant to your question.\"\"\"\n    return '\\n'.join([f'{round(n.score,2)} - {n.node.text}' for n in retriever.retrieve(question)])\n\n\nfind_references_tool = FunctionTool.from_defaults(fn=find_references)\nThis tool can then be added to an agent as we discussed before:\nfrom llama_index.core.agent import ReActAgent\n\nfrom llama_index.llms.lmstudio import LMStudio\n\n\nllm = LMStudio(model_name=\"llama-3.2-1b-instruct\",\n        base_url=\"http://localhost:1234/v1\",\n    temperature=0.5,\n    request_timeout=600)\n\n\nagent = ReActAgent.from_tools(tools=[find_references_tool],llm=llm, verbose=True)\n/home/brede/MEGA/Honorar/Generative AI/script/.venv/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in LMStudio has conflict with protected namespace \"model_\".\n\nYou may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n  warnings.warn(\nWhich can then be used to answer chat-requests:\nresponse = agent.chat(\"What is the meaning of Query, Key and Value in the context of self-attention?\")\nprint(str(response))\n&gt; Running step 062240ab-0d21-4fdb-a603-fb386970c32f. Step input: What is the meaning of Query, Key and Value in the context of self-attention?\nObservation: Error: Could not parse output. Please follow the thought-action-input format. Try again.\n&gt; Running step 2a291a80-5090-4373-945d-3a647ac2b758. Step input: None\nObservation: Error: Could not parse output. Please follow the thought-action-input format. Try again.\n&gt; Running step 908425d7-8f06-4830-8585-4ff312b43c45. Step input: None\nObservation: Error: Could not parse output. Please follow the thought-action-input format. Try again.\n&gt; Running step 543ab12f-e5e7-4a59-b103-b7fc7bd0a3fe. Step input: None\nObservation: Error: Could not parse output. Please follow the thought-action-input format. Try again.\n&gt; Running step 1b3cb4e3-e976-4420-a489-906b8f6c5776. Step input: None\nThought: Let's break down what Query, Key, and Value mean in the context of self-attention.\nAction: Use\nAction Input: {'input': \"What are the most relevant words for the sentence 'The quick brown fox jumps over the lazy dog'?\", 'num_beams': 5}\nObservation: Error: No such tool named `Use`.\n&gt; Running step 1101520e-54ff-42db-b327-d9d902acb957. Step input: None\nThought: I need to find a way to input the query and parameters into a tool.\nAction: Use\nAction Input: {'input': \"What are the most relevant words for the sentence 'The quick brown fox jumps over the lazy dog'?\", 'num_beams': 5}\nObservation: Error: No such tool named `Use`.\n&gt; Running step 47c5a9f6-5055-4f8c-9a3b-49f1db40abcb. Step input: None\nThought: I'm using a different tool to find references. Let me check if it supports finding relevant text snippets for the given query.\nAction: find_references\nAction Input: {'properties': AttributedDict([('question', \"What are the most relevant words for the sentence 'The quick brown fox jumps over the lazy dog'?\"), ('num_beams', 5)]), 'required': ['query', 'parameters']}\nObservation: Error: find_references() got an unexpected keyword argument 'properties'\n\nValueError: Reached max iterations.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)\nCell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----&gt; 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is the meaning of Query, Key and Value in the context of self-attention?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mstr\u001b[39m(response))\n\nFile \u001b[0;32m~/MEGA/Honorar/Generative AI/script/.venv/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:311\u001b[0m, in \u001b[0;36mDispatcher.span.&lt;locals&gt;.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m             _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--&gt; 311\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[1;32m    314\u001b[0m         new_future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(result)\n\nFile \u001b[0;32m~/MEGA/Honorar/Generative AI/script/.venv/lib/python3.10/site-packages/llama_index/core/callbacks/utils.py:41\u001b[0m, in \u001b[0;36mtrace_method.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m callback_manager \u001b[38;5;241m=\u001b[39m cast(CallbackManager, callback_manager)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m callback_manager\u001b[38;5;241m.\u001b[39mas_trace(trace_id):\n\u001b[0;32m---&gt; 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\nFile \u001b[0;32m~/MEGA/Honorar/Generative AI/script/.venv/lib/python3.10/site-packages/llama_index/core/agent/runner/base.py:647\u001b[0m, in \u001b[0;36mAgentRunner.chat\u001b[0;34m(self, message, chat_history, tool_choice)\u001b[0m\n\u001b[1;32m    642\u001b[0m     tool_choice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_tool_choice\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    644\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mAGENT_STEP,\n\u001b[1;32m    645\u001b[0m     payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mMESSAGES: [message]},\n\u001b[1;32m    646\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--&gt; 647\u001b[0m     chat_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchat_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchat_history\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatResponseMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWAIT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    653\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chat_response, AgentChatResponse)\n\u001b[1;32m    654\u001b[0m     e\u001b[38;5;241m.\u001b[39mon_end(payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mRESPONSE: chat_response})\n\nFile \u001b[0;32m~/MEGA/Honorar/Generative AI/script/.venv/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:311\u001b[0m, in \u001b[0;36mDispatcher.span.&lt;locals&gt;.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m             _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--&gt; 311\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[1;32m    314\u001b[0m         new_future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(result)\n\nFile \u001b[0;32m~/MEGA/Honorar/Generative AI/script/.venv/lib/python3.10/site-packages/llama_index/core/agent/runner/base.py:579\u001b[0m, in \u001b[0;36mAgentRunner._chat\u001b[0;34m(self, message, chat_history, tool_choice, mode)\u001b[0m\n\u001b[1;32m    576\u001b[0m dispatcher\u001b[38;5;241m.\u001b[39mevent(AgentChatWithStepStartEvent(user_msg\u001b[38;5;241m=\u001b[39mmessage))\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;66;03m# pass step queue in as argument, assume step executor is stateless\u001b[39;00m\n\u001b[0;32m--&gt; 579\u001b[0m     cur_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_choice\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cur_step_output\u001b[38;5;241m.\u001b[39mis_last:\n\u001b[1;32m    584\u001b[0m         result_output \u001b[38;5;241m=\u001b[39m cur_step_output\n\nFile \u001b[0;32m~/MEGA/Honorar/Generative AI/script/.venv/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:311\u001b[0m, in \u001b[0;36mDispatcher.span.&lt;locals&gt;.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m             _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--&gt; 311\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[1;32m    314\u001b[0m         new_future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(result)\n\nFile \u001b[0;32m~/MEGA/Honorar/Generative AI/script/.venv/lib/python3.10/site-packages/llama_index/core/agent/runner/base.py:412\u001b[0m, in \u001b[0;36mAgentRunner._run_step\u001b[0;34m(self, task_id, step, input, mode, **kwargs)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# TODO: figure out if you can dynamically swap in different step executors\u001b[39;00m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;66;03m# not clear when you would do that by theoretically possible\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m ChatResponseMode\u001b[38;5;241m.\u001b[39mWAIT:\n\u001b[0;32m--&gt; 412\u001b[0m     cur_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m ChatResponseMode\u001b[38;5;241m.\u001b[39mSTREAM:\n\u001b[1;32m    414\u001b[0m     cur_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_worker\u001b[38;5;241m.\u001b[39mstream_step(step, task, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\nFile \u001b[0;32m~/MEGA/Honorar/Generative AI/script/.venv/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:311\u001b[0m, in \u001b[0;36mDispatcher.span.&lt;locals&gt;.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m             _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--&gt; 311\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[1;32m    314\u001b[0m         new_future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(result)\n\nFile \u001b[0;32m~/MEGA/Honorar/Generative AI/script/.venv/lib/python3.10/site-packages/llama_index/core/callbacks/utils.py:41\u001b[0m, in \u001b[0;36mtrace_method.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m callback_manager \u001b[38;5;241m=\u001b[39m cast(CallbackManager, callback_manager)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m callback_manager\u001b[38;5;241m.\u001b[39mas_trace(trace_id):\n\u001b[0;32m---&gt; 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\nFile \u001b[0;32m~/MEGA/Honorar/Generative AI/script/.venv/lib/python3.10/site-packages/llama_index/core/agent/react/step.py:818\u001b[0m, in \u001b[0;36mReActAgentWorker.run_step\u001b[0;34m(self, step, task, **kwargs)\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;129m@trace_method\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_step\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, step: TaskStep, task: Task, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m&gt;\u001b[39m TaskStepOutput:\n\u001b[1;32m    817\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Run step.\"\"\"\u001b[39;00m\n\u001b[0;32m--&gt; 818\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\nFile \u001b[0;32m~/MEGA/Honorar/Generative AI/script/.venv/lib/python3.10/site-packages/llama_index/core/agent/react/step.py:576\u001b[0m, in \u001b[0;36mReActAgentWorker._run_step\u001b[0;34m(self, step, task)\u001b[0m\n\u001b[1;32m    572\u001b[0m reasoning_steps, is_done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_actions(\n\u001b[1;32m    573\u001b[0m     task, tools, output\u001b[38;5;241m=\u001b[39mchat_response\n\u001b[1;32m    574\u001b[0m )\n\u001b[1;32m    575\u001b[0m task\u001b[38;5;241m.\u001b[39mextra_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurrent_reasoning\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mextend(reasoning_steps)\n\u001b[0;32m--&gt; 576\u001b[0m agent_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextra_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcurrent_reasoning\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextra_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msources\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_done:\n\u001b[1;32m    580\u001b[0m     task\u001b[38;5;241m.\u001b[39mextra_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnew_memory\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mput(\n\u001b[1;32m    581\u001b[0m         ChatMessage(content\u001b[38;5;241m=\u001b[39magent_response\u001b[38;5;241m.\u001b[39mresponse, role\u001b[38;5;241m=\u001b[39mMessageRole\u001b[38;5;241m.\u001b[39mASSISTANT)\n\u001b[1;32m    582\u001b[0m     )\n\nFile \u001b[0;32m~/MEGA/Honorar/Generative AI/script/.venv/lib/python3.10/site-packages/llama_index/core/agent/react/step.py:437\u001b[0m, in \u001b[0;36mReActAgentWorker._get_response\u001b[0;34m(self, current_reasoning, sources)\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo reasoning steps were taken.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(current_reasoning) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_iterations:\n\u001b[0;32m--&gt; 437\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReached max iterations.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(current_reasoning[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], ResponseReasoningStep):\n\u001b[1;32m    440\u001b[0m     response_step \u001b[38;5;241m=\u001b[39m cast(ResponseReasoningStep, current_reasoning[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\n\u001b[0;31mValueError\u001b[0m: Reached max iterations.\nAs you can see, the model request ends up with errors. The model is not powerful enough to answer in the structured manner we need for the function-calling of the tool. To circumvent this, we can try a function-calling-finetuned model:\nWe can try to solve this issue by using a language model that is finetuned on function calling:\nfc_llm = LMStudio(model_name=\"phi-3-mini-4k-instruct-function-calling\",\n        base_url=\"http://localhost:1234/v1\",\n    temperature=0.2,\n    request_timeout=600)\n\nagent = ReActAgent.from_tools(tools=[find_references_tool],llm=fc_llm, verbose=True)\nresponse = agent.chat(\"What is the meaning of Query, Key and Value in the context of self-attention?\")\nprint(str(response))\n&gt; Running step 78c0a52b-55fa-4241-ade5-67c0b92b9bf3. Step input: What is the meaning of Query, Key and Value in the context of self-attention?\nThought: (Implicit) I can answer without any more tools!\nAnswer:  In the context of self-attention, \"Query\", \"Key\" and \"Value\" are terms used to describe different components of a neural network architecture. Here's what they mean:\n1. Query - The query component is used to retrieve information from memory banks during attention computation. It represents a set of learned parameters that enable the model to focus on specific parts of an input sequence when processing it. In other words, the query function defines how much importance we should give to each part of our input data while computing self-attention weights.\n2. Key - The key component is used to determine which parts of the input sequence are most relevant for a particular output location in the model's memory bank. It represents another set of learned parameters that help us identify important features in an input sequence during attention computation. In other words, the key function helps us decide what we should focus on when computing self-attention weights.\n3. Value - The value component is used to store the actual data corresponding to each memory bank location in a neural network architecture. It represents our stored knowledge or \"memory\" that can be retrieved later during attention computation. In other words, the value function holds all of the information we need to compute an output based on self-attention weights.\nIn summary, query, key and value are components of a neural network architecture used in self-attention that help us focus on specific parts of our input sequence, identify important features within it, and retrieve relevant stored knowledge/memory to compute outputs.\n In the context of self-attention, \"Query\", \"Key\" and \"Value\" are terms used to describe different components of a neural network architecture. Here's what they mean:\n1. Query - The query component is used to retrieve information from memory banks during attention computation. It represents a set of learned parameters that enable the model to focus on specific parts of an input sequence when processing it. In other words, the query function defines how much importance we should give to each part of our input data while computing self-attention weights.\n2. Key - The key component is used to determine which parts of the input sequence are most relevant for a particular output location in the model's memory bank. It represents another set of learned parameters that help us identify important features in an input sequence during attention computation. In other words, the key function helps us decide what we should focus on when computing self-attention weights.\n3. Value - The value component is used to store the actual data corresponding to each memory bank location in a neural network architecture. It represents our stored knowledge or \"memory\" that can be retrieved later during attention computation. In other words, the value function holds all of the information we need to compute an output based on self-attention weights.\nIn summary, query, key and value are components of a neural network architecture used in self-attention that help us focus on specific parts of our input sequence, identify important features within it, and retrieve relevant stored knowledge/memory to compute outputs.\nThis model does not run into an issue with the structured output, it does not try to use the tool anymore though.\nOne way to try to solve this issue is to adapt the agent-prompt:\nprint(agent.get_prompts()['agent_worker:system_prompt'].template)\nYou are designed to help with a variety of tasks, from answering questions to providing summaries to other types of analyses.\n\n## Tools\n\nYou have access to a wide variety of tools. You are responsible for using the tools in any sequence you deem appropriate to complete the task at hand.\nThis may require breaking the task into subtasks and using different tools to complete each subtask.\n\nYou have access to the following tools:\n{tool_desc}\n\n\n## Output Format\n\nPlease answer in the same language as the question and use the following format:\n\n```\nThought: The current language of the user is: (user's language). I need to use a tool to help me answer the question.\nAction: tool name (one of {tool_names}) if using a tool.\nAction Input: the input to the tool, in a JSON format representing the kwargs (e.g. {{\"input\": \"hello world\", \"num_beams\": 5}})\n```\n\nPlease ALWAYS start with a Thought.\n\nNEVER surround your response with markdown code markers. You may use code markers within your response if you need to.\n\nPlease use a valid JSON format for the Action Input. Do NOT do this {{'input': 'hello world', 'num_beams': 5}}.\n\nIf this format is used, the user will respond in the following format:\n\n```\nObservation: tool response\n```\n\nYou should keep repeating the above format till you have enough information to answer the question without using any more tools. At that point, you MUST respond in one of the following two formats:\n\n```\nThought: I can answer without using any more tools. I'll use the user's language to answer\nAnswer: [your answer here (In the same language as the user's question)]\n```\n\n```\nThought: I cannot answer the question with the provided tools.\nAnswer: [your answer here (In the same language as the user's question)]\n```\n\n## Current Conversation\n\nBelow is the current conversation consisting of interleaving human and assistant messages.\nThis we can adapt in the following way:\nfrom llama_index.core import PromptTemplate\nnew_agent_template_str = \"\"\"\nYou are designed to help answer questions based on a collection of paper-excerpts.\n\n## Tools\n\nYou have access to tools that allow you to query paper-content. You are responsible for using the tools in any sequence you deem appropriate to complete the task at hand.\nThis may require breaking the task into subtasks and using different tools to complete each subtask. Do not answer without tool-usage if a tool can be used to answer a question. Do try to find a text passage to back up your claims whenever possible. Do not answer without reference if the appropriate text is available in the tools you have access to.\n\nYou have access to the following tools:\n{tool_desc}\n\n\n## Output Format\n\nPlease answer in the same language as the question and use the following format:\n\n\\`\\`\\`\nThought: The current language of the user is: (user's language). I need to use a tool to help me answer the question.\nAction: tool name (one of {tool_names}) if using a tool.\nAction Input: the input to the tool, in a JSON format representing the kwargs (e.g. {{\"input\": \"hello world\", \"num_beams\": 5}})\n\\`\\`\\`\n\n\nPlease ALWAYS start with a Thought.\n\nNEVER surround your response with markdown code markers. You may use code markers within your response if you need to.\n...\n## Current Conversation\n\nBelow is the current conversation consisting of interleaving human and assistant messages.\n\"\"\"\nnew_agent_template = PromptTemplate(new_agent_template_str)\nagent.update_prompts(\n    {\"agent_worker:system_prompt\": new_agent_template}\n)\nWe can test this new prompt with the same question:\nresponse = agent.chat(\"What is the meaning of Query, Key and Value in the context of self-attention?\")\nprint(str(response))\n&gt; Running step d5fb46ea-de7a-4e8b-ace7-7ed3ae6a9706. Step input: What is the meaning of Query, Key and Value in the context of self-attention?\nThought: (Implicit) I can answer without any more tools!\nAnswer:  In the context of natural language processing (NLP), \"Query\", \"Key\" and \"Value\" are used as components for a type of neural network architecture called Transformer model. The Transformer model employs self-attention mechanism to improve its ability to process sequential data such as text or audio. \nHere's how these terms relate to the model:\n1. Query - A query is an input vector that represents the current state of a sequence being processed by the transformer network. It contains information about which words or tokens are currently being attended to, and helps guide the attention mechanism towards relevant parts of the input sequence.\n2. Key - The key component in a transformer model refers to a set of learned weights that help determine how much importance should be given to each word or token during self-attention computation. These keys are computed for all words or tokens in an input sequence and they form part of the attention mechanism used by the Transformer network.\n3. Value - The value component is responsible for storing information from a specific memory slot corresponding to a particular input token in the transformer model. It represents the output produced when we apply a transformation function on the query vector (which contains contextual information about the current word or token being processed) using learned weights, and then weighted-summed with the key vectors.\nIn summary, Query, Key and Value are components of a neural network architecture used in Transformer models for NLP that help us process sequential data such as text by guiding attention towards relevant parts of an input sequence, identifying important features within it, and computing outputs based on self-attention weights.\n In the context of natural language processing (NLP), \"Query\", \"Key\" and \"Value\" are used as components for a type of neural network architecture called Transformer model. The Transformer model employs self-attention mechanism to improve its ability to process sequential data such as text or audio. \nHere's how these terms relate to the model:\n1. Query - A query is an input vector that represents the current state of a sequence being processed by the transformer network. It contains information about which words or tokens are currently being attended to, and helps guide the attention mechanism towards relevant parts of the input sequence.\n2. Key - The key component in a transformer model refers to a set of learned weights that help determine how much importance should be given to each word or token during self-attention computation. These keys are computed for all words or tokens in an input sequence and they form part of the attention mechanism used by the Transformer network.\n3. Value - The value component is responsible for storing information from a specific memory slot corresponding to a particular input token in the transformer model. It represents the output produced when we apply a transformation function on the query vector (which contains contextual information about the current word or token being processed) using learned weights, and then weighted-summed with the key vectors.\nIn summary, Query, Key and Value are components of a neural network architecture used in Transformer models for NLP that help us process sequential data such as text by guiding attention towards relevant parts of an input sequence, identifying important features within it, and computing outputs based on self-attention weights.\nThe model still tries to answer without the tool.\nLet’s try to ask a more specific question:\nresponse = agent.chat(\"How does the paper 'Attention is all you need' define the term self attention?\")\nprint(str(response))\n&gt; Running step 3c1b3050-f4a0-4b46-9006-366161df0219. Step input: How does the paper 'Attention is all you need' define the term self attention?\nThought: (Implicit) I can answer without any more tools!\nAnswer:  In the paper \"Attention Is All You Need\", the authors present a novel Transformer model that relies heavily on an attention mechanism to improve its ability to process sequential data such as text or audio. The paper introduces several key concepts related to this mechanism, including the notion of \"self-attention\". \nSelf-attention is defined in the paper as follows: given a sequence of input tokens (or words), self-attention enables us to compute contextualized representations for each token by computing attention weights over all other tokens in the sequence. These attention weights reflect how much importance we should give to each token when computing our output representation. In particular, during training, these weights are learned based on the input data itself and can be adjusted dynamically as new inputs come in. The resulting contextualized representations produced by self-attention provide a rich source of information for downstream tasks like language modeling or machine translation.\n In the paper \"Attention Is All You Need\", the authors present a novel Transformer model that relies heavily on an attention mechanism to improve its ability to process sequential data such as text or audio. The paper introduces several key concepts related to this mechanism, including the notion of \"self-attention\". \nSelf-attention is defined in the paper as follows: given a sequence of input tokens (or words), self-attention enables us to compute contextualized representations for each token by computing attention weights over all other tokens in the sequence. These attention weights reflect how much importance we should give to each token when computing our output representation. In particular, during training, these weights are learned based on the input data itself and can be adjusted dynamically as new inputs come in. The resulting contextualized representations produced by self-attention provide a rich source of information for downstream tasks like language modeling or machine translation.\nStill no dice.\nOne solution to this problem is to just use a bigger model:\nllm = LMStudio(model_name=\"llama-3.2-3b-instruct\", #3 Billion instead of 1\n        base_url=\"http://localhost:1234/v1\",\n    temperature=0.2,\n    request_timeout=600)\n\n\nagent = ReActAgent.from_tools(tools=[find_references_tool],llm=llm, verbose=True)\n\nresponse = agent.chat(\"How does the paper 'Attention is all you need' define the term self attention?\")\nprint(str(response))\n&gt; Running step 9326aba5-48cf-40dd-8b85-b5da82554e5c. Step input: How does the paper 'Attention is all you need' define the term self attention?\nThought: The current language of the user is English. I need to use a tool to help me answer the question.\nAction: find_references\nAction Input: {'properties': AttributedDict([('question', AttributedDict([('title', 'self-attention definition'), ('type', 'string')]))]), 'required': ['question'], 'type': 'object'}\nObservation: Error: find_references() got an unexpected keyword argument 'properties'\n&gt; Running step b9bd6255-c348-473e-a031-2fd1e4e74cdf. Step input: None\nThought: The current language of the user is English. I need to use a tool to help me answer the question, but it seems like find_references doesn't support the properties argument.\nAction: find_references\nAction Input: {'question': \"How does the paper 'Attention is all you Need' define the term self attention?\"}\nObservation: 0.69 - Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n0.52 - .                       .              .\n                                                                                                                 &lt;EOS&gt;       &lt;EOS&gt;            &lt;EOS&gt;                 &lt;EOS&gt;\n                                                                                                                  &lt;pad&gt;      &lt;pad&gt;             &lt;pad&gt;                &lt;pad&gt;\n\n\n\n\n     Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\n     Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution.\n0.5 - &lt;EOS&gt;\n                                                                                                                                           &lt;pad&gt;\n                                                                                                                                                   &lt;pad&gt;\n                                                                                                                                                   &lt;pad&gt;\n                                                                                                                                                           &lt;pad&gt;\n                                                                                                                                                                   &lt;pad&gt;\n                                                                                                                                                                           &lt;pad&gt;\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\nthe verb ‘making’, completing the phrase ‘making...more difficult’.\n0.5 - Each layer has two\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\nthe two sub-layers, followed by layer normalization [1].\n0.49 - 4     Why Self-Attention\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\ntional layers commonly used for mapping one variable-length sequence of symbol representations\n(x1 , ..., xn ) to another sequence of equal length (z1 , ..., zn ), with xi , zi ∈ Rd , such as a hidden\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\nconsider three desiderata.\n0.47 - In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [17, 18] and [9].\n\n\n3   Model Architecture\n\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\nHere, the encoder maps an input sequence of symbol representations (x1 , ..., xn ) to a sequence\nof continuous representations z = (z1 , ..., zn ).\n0.45 - .                       .             .\n                                                                                                              &lt;EOS&gt;       &lt;EOS&gt;            &lt;EOS&gt;                &lt;EOS&gt;\n                                                                                                               &lt;pad&gt;      &lt;pad&gt;             &lt;pad&gt;               &lt;pad&gt;\n\n\n\n\n     sentence. We give two such examples above, from two different heads from the encoder self-attention\n     Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\n0.44 - Operations\n      Self-Attention                      O(n2 · d)             O(1)                O(1)\n      Recurrent\n0.43 - • The encoder contains self-attention layers. In a self-attention layer all of the keys, values\n           and queries come from the same place, in this case, the output of the previous layer in the\n           encoder. Each position in the encoder can attend to all positions in the previous layer of the\n           encoder.\n0.42 - As side benefit, self-attention could yield more interpretable models. We inspect attention distributions\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\nand semantic structure of the sentences.\n\n\n5     Training\n\nThis section describes the training regime for our models.\n&gt; Running step ac9c9225-596e-4c84-8e86-1518a4fd7d55. Step input: None\nThought: The current language of the user is English. I was able to retrieve relevant information about self-attention from the paper \"Attention is all you Need\". It seems that the authors define self-attention as an attention mechanism that relates different positions of a single sequence in order to compute a representation of the sequence.\nAnswer: Self-attention, also known as intra-attention, is an attention mechanism that computes a representation of a sequence by attending to different positions within the same sequence. It has been used successfully in various tasks such as reading comprehension, abstractive summarization, textual entailment, and learning task-independent sentence representations.\nSelf-attention, also known as intra-attention, is an attention mechanism that computes a representation of a sequence by attending to different positions within the same sequence. It has been used successfully in various tasks such as reading comprehension, abstractive summarization, textual entailment, and learning task-independent sentence representations.\nThis is not always feasible though.\nAnother way to use the retrieval-pipeline is to not give a weak model the opportunity to mess up the tool calling. This can be implemented by using a query-engine instead of the retriever. This directly wraps the retrieval in a LLM-Summarization-Module that only returns summaries.\nDoing this, we can use two separate models for each part of the task - one for the planning and answering and one for the structured summarization:\nquery_engine = index.as_query_engine(use_async=False, llm=fc_llm, verbose=True)\nresponse = query_engine.query(\"What is the meaning of Query, Key and Value in the context of self-attention?\")\nprint(str(response))\n In the context of self-attention, \"Query\" refers to the keys that are used to retrieve relevant information from a sequence. \"Key\" represents the values associated with each element in the sequence, which determine their importance or relevance. \"Value\" corresponds to the actual data being processed by the attention mechanism.\nFinally an answer we can work with!\n\n\n\n\n\n\n📝 Task\n\n\n\nBuild a llamaindex-application that allows you to chat with the climate_fever evidence.\n\n\n\n\n\n\n\nDocument chunking\nThe examples we looked at until now were all working with short text-snippets that comforably fit into the context window of a LLM. If you think about usual usecases for RAG-systems, this is not the most common case though. Usually, you will have a base of documents that can span multiple 1000’s of tokens and you want to be able to answer questions about these documents. Furthermore, you do not only want to know which document might be relevant, but ideally also which part of the document matches your question best.\nThis is where the process of doctument chunking or document splitting comes into play. There is a series of possible approaches to split a document, the most common, so called naive chunking method, is to use a structural element of the document though. This means that you parse the documents into sentences, paragraphs or pages and then use these as chunks that you individually embed and store in your vector database. To prevent loss of relevant context when splitting a document into chunks, it is additionally common to add some overlap between the chunks. This tries to solve the lost context problem, does however create reduncencies in the data.\nAn alternative approach is to use semantic chunking. This means that you split a document into chunks based on their meaning. Jina.ai explained in a blogpost (Late Chunking in Long-Context Embedding Models, 2024) their so called “late chunking” method. which iteratively runs the whole document through the attention head of the transformer to gain embeddings per token, and then averages these embeddings per naive chunk. This way, the chunks are still structure based but contain semantic information about the whole context. \nAnother approach to semantic chunking is described on the doc-pages of LlamaIndex. In their approach to semantic chunking, an adaptive splitting-rule is used, that splits the documents based on semantic similarity of sentences. This means that sentences that are semantically similar are grouped together into chunks.\n\n\n\n\n\n\n📝 Task\n\n\n\nImplement a document chunking strategy for a book of your choice from the project_gutenberg dataset.\nYou can use any approach you like, but you should explain your choice and why it is appropriate for this dataset.\n\n\n\n\nQuery Expansion/Transformation\nUntil now, we have based our retrieval on the assumption, that the question the user formulates is a good representation of their information need. This is not always the case though. Often, users do not know what they are looking for or they use synonyms or paraphrases that are not present in the documents. If the question is not formulated well, or if it is too specific, the system might not be able to find relevant documents. To improve the quality of the questions, we can use query expansion. This means that we take the original question and expand it with additional information to make it more specific and to increase the chances of finding relevant documents. This can be done in multiple ways, one common approach is to use a generative model to generate multiple queries based on the original question. Another approach is to use a keyword extraction algorithm to extract keywords from the question and then use these keywords to expand the query.\nThe most basic way to implement a query-expansion is to build a tool that instructs a LLM to give multiple alternate formulations of the original query. Though this will probably work, there are more refined methods.\nLlamaindex implements two more sophisticated approaches to transform queries:\n\nHypothetical Document Embeddings (HyDe): A LLM is instructed to generate a hypothetical document that answers the query. This document is then used to query the index\nMulti-Step Query Transformations: After a first execution of a (complex) query against an index, the answer is used to iteratively formulate follow-up questions that are then executed against the index.\n\n\n\n\n\n\n\n📝 Task\n\n\n\nImplement query expansion for the climate_fever dataset using llamaindex. This might be helpful.\nExperiment with different prompts and temperatures.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Embedding-based agent-systems</span>"
    ]
  },
  {
    "objectID": "content/embeddings.html#further-readings",
    "href": "content/embeddings.html#further-readings",
    "title": "Embedding-based agent-systems",
    "section": "Further Readings",
    "text": "Further Readings\n\nThis blogpost by DeepSet gives a good overview of the concept of RAG\nThis blogpost by qdrant about (their) vector store and its inner workings",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Embedding-based agent-systems</span>"
    ]
  },
  {
    "objectID": "content/embeddings.html#references",
    "href": "content/embeddings.html#references",
    "title": "Embedding-based agent-systems",
    "section": "References",
    "text": "References\n\n\n\n\nBowman, S. R., Angeli, G., Potts, C., & Manning, C. D. (2015). A large annotated corpus for learning natural language inference. In L. Màrquez, C. Callison-Burch, & J. Su (Eds.), Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (pp. 632–642). Association for Computational Linguistics. https://doi.org/10.18653/v1/D15-1075\n\n\nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (arXiv:1810.04805). arXiv. https://doi.org/10.48550/arXiv.1810.04805\n\n\nGoyal, K., & Sharma, M. (2022). Comparative Analysis of Different Vectorizing Techniques for Document Similarity using Cosine Similarity. 2022 Second International Conference on Advanced Technologies in Intelligent Control, Environment, Computing & Communication Engineering (ICATIECE), 1–5. https://doi.org/10.1109/ICATIECE56365.2022.10046766\n\n\nJiang, T., Huang, S., Luan, Z., Wang, D., & Zhuang, F. (2023). Scaling Sentence Embeddings with Large Language Models (arXiv:2307.16645). arXiv. https://doi.org/10.48550/arXiv.2307.16645\n\n\nLate Chunking in Long-Context Embedding Models. (2024). https://jina.ai/news/late-chunking-in-long-context-embedding-models.\n\n\nReimers, N., & Gurevych, I. (2019). Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks (arXiv:1908.10084). arXiv. https://doi.org/10.48550/arXiv.1908.10084\n\n\nSteck, H., Ekanadham, C., & Kallus, N. (2024). Is Cosine-Similarity of Embeddings Really About Similarity? Companion Proceedings of the ACM Web Conference 2024, 887–890. https://doi.org/10.1145/3589335.3651526\n\n\nTunstall, L., Reimers, N., Jo, U. E. S., Bates, L., Korat, D., Wasserblat, M., & Pereg, O. (2022). Efficient Few-Shot Learning Without Prompts (arXiv:2209.11055). arXiv. https://arxiv.org/abs/2209.11055",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Embedding-based agent-systems</span>"
    ]
  },
  {
    "objectID": "content/function_calling.html",
    "href": "content/function_calling.html",
    "title": "Function Calling",
    "section": "",
    "text": "Code generation and function calling\nFunction calling is a technique used in large language models (LLMs) and AI agents to enhance their capability to provide more accurate and relevant responses, especially when handling complex tasks or questions that require specialized knowledge or external data.\nWe already got to know function calling in chapter 3 of this course. There, we introduced agents, that already came with the ability to call predefined functions. In this chapter, we will go back to the basics of function calling using LLMs.\nThe basic idea of function calling is to use an LLM to generate valid, executable code from the user input. That is, the user’s input is sent to the LLM, together with a prompt, urging it to return structured output in a specific format. This output can then be taken and executed. For this to work properly, of course, the generated output must be valid code (in our case python code). There are two approaches for that:\nHere, we will focus on function calling. Still the challenge is to get the LLM to generate valid output. There are two main strategies to facilitate that:",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Function Calling</span>"
    ]
  },
  {
    "objectID": "content/function_calling.html#code-generation-and-function-calling",
    "href": "content/function_calling.html#code-generation-and-function-calling",
    "title": "Function Calling",
    "section": "",
    "text": "Code generation: Here, we ask the LLM to generate a complete python script from the user input. This approach has the advantage of being simple and straightforward, but it can be prone to errors if the LLM does not fully understand the task at hand or if it makes mistakes in its code generation. It can also pose a severe security issue because this approach hinges on running generated code on your machine.\nFunction calling: Here, we ask the LLM to generate a function call from the user input. This approach has the advantage of being more robust and accurate than code generation, as it is easier for the LLM to generate a correct function call than a complete python script. However, it requires that the functions that can be called are already defined and that they are properly documented.\n\n\n\nusing a large, generalized LLM (e.g. GPT-4) with good prompt engineering and\nusing a smaller model fine tuned to generate function calls.\n\n\nFunction definition\nThe first step in using function calling is to define the functions that the LLM can call. This is done by providing a JSON schema that describes the name of the function, its arguments and their types. The JSON schema should be provided to the LLM in the system prompt. Here is an example: 1\n1 Note, that this is not an executable implementation but just a description of the function for the LLM.{\n    \"name\": \"get_current_weather\",  \n    \"description\": \"Get the current weather in a given location\",  \n    \"arguments\": {    \n        \"location\": {\"type\": \"string\"},    \n        \"unit\": {\"type\": \"string\"}  \n        } \n}\n\n\nPrompting\nThe second step is to provide a good prompt. The prompt should make it clear to the LLM to only generate valid output and that it should follow the JSON schema. Here is an example of a prompt that can be used for function calling:\nYou are a helpful assistant that generates function calls based on user input. Only use the functions you have been provided with.\n\n{function definition as described above}\n\nUser: What's the weather like in Berlin?\n\nAssistant: {\n    \"name\": \"get_current_weather\",\n    \"arguments\": {\"location\": \"Berlin\", \"unit\": \"celsius\"}\n}\n\n\n\n\n\n\n📝 Task\n\n\n\nTry it!\n\nOpen a notebook and connect to a local LLM using LM Studio.\nDefine the function get_current_weather as shown above.\nWrite a prompt that asks the LLM to generate a function call based on user input.\nTest the prompt with an example input.\nDefine other functions and try other inputs and see if the LLM generates valid output.\nUpload to Moodle.\n\n\n\n\n\nChallenges, finetuned models and the influence of size\nThe main challenge is here to get the LLM to generate a valid answer. This is not always easy, as LLMs are not usually super safe coders 😃.\n\nThey can hallucinate functions or arguments that do not exist.\nThey can forget to call a function.\nThey can forget to provide all required arguments.\nThey can provide the wrong type for an argument.\nThey can provide invalid values for an argument.\n\nThere are several strategies to mitigate these issues:\n\nPrompt engineering: A good prompt can help to guide the LLM towards generating valid output. This is especially true for larger models, as they have a better understanding of the world and can therefore generate more accurate responses.\nFinetuning: Finetuning a model on a specific task can improve its performance on that task. This is especially useful for smaller models, as they are less likely to hallucinate functions or arguments that do not exist.\nSize: Larger models are better at generating valid output than smaller models. However, larger models are also more expensive to run and require more computational resources.\n\n\n\n\n\n\n\n📝 Task\n\n\n\nTest it! (we can do it together, if your hardware does not allow you to run the model.)\nAs above, but this time\n\nuse a very small model (e.g a small Llama model)\nuse a model finetuned for the task (you could try this one)\na larger model (a larger llama in this case)",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Function Calling</span>"
    ]
  },
  {
    "objectID": "content/function_calling.html#agents-recap",
    "href": "content/function_calling.html#agents-recap",
    "title": "Function Calling",
    "section": "Agents recap",
    "text": "Agents recap\nWe introduced agents already back in chapter 3. To give a quick recap, an agent is a wrapper layer, that takes the user input and pipes it to an LLM, together with a custom system prompt, that allows the LLM to answer the user request better. The agent has several modules at its disposal, the memory, some tools and a planning tool.\nThe memory function is what allows chat models to retain a memory of the past conversation with the user. This information is saved as plain text in the memory and given to the planning module (i.e. the LLM) along with the system prompt and the current user input.\nThe planning module then decides which tools to use, if any, to answer the user request. The output of the planning module is a response message containing one or several tool calls (or a final answer). The agent then executes the tool calls by first parsing the response, then executing the functions. Based on the tool outputs, a final answer is generated and sent back to the user.\n\nReact agents\nThere a several types of agent. Now, we want to fucus on the ReAct agent introduced by (Yao et al., 2023). The ReAct agent is a type of agent that uses the ReAct framework to solve complex tasks by reasoning in multiple steps. It is based on the idea of “thought-action-observation” loops. The LLM is given a task and it generates a thought, which is then used to decide on an action. The action is executed and the observation is fed back into the LLM. This process is repeated until the LLM decides that it has enough information to answer the question or if the maximum number of iterations is reached.\n\n\nLlamaindex\nLLamaindex is a framework that makes it easy to implement and use agents. In Llamaindex an agent consists a an agent runner and an agent_worker. Think of the agent runner as the agent core in the architecture schematics and the agent worker as the planning module. The tools are functions implemented in python that can be executed by the agent worker. Finally, the memory module consists of a simple text buffer, logging the conversation history between the user and the agent and between the agent and the tools.\n\n\n\n\n\n\n📝 Task\n\n\n\nLet’s have a look!\n\nOpen a notebook and connect it with a local LLM using LM Studio.\nDefine a function that can be called by the agent to get the current weather in a given location. (Implement it this time, it doesn’t need to work, just return random weather)\nInitialize a ReAct agent using LLamaindex (you can use this tutorial as the starting point) \nHave a look at the prompt, the agent gives to the LLM (you can find it using agent.get_prompts())\nDiscuss the prompt with the group! What does it do? How does it do it?\nAsk the agent to tell you the weather in a given location.\nWatch in LM Studio how the LLM called by the agent creates the thought process, function calls and the final response.\nTry to break the agent by asking stuff it cannot answer. Be creative. (On one occasion I just said “Hi” and it went into an infinite loop because it did not need a tool for that and there wasn’t a “none” tool 😜.)\nUpload to Moodle",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Function Calling</span>"
    ]
  },
  {
    "objectID": "content/function_calling.html#further-readings",
    "href": "content/function_calling.html#further-readings",
    "title": "Function Calling",
    "section": "Further Readings",
    "text": "Further Readings\nOn the Llamaindex website on the “examples” page you will find a lot of helpful material: examples, notebooks, recipes and more. I recommend to have a look at them! For our case, check the “agents” section. For an even more in-depth dive, go to the “workflows” part.\n\nIn this example you find an agent implementation that returns a step-by-step breakdown of its thought process.\nTo go even more low level then that see this example that will walk you through setting up a Workflow to construct a function calling agent from scratch.\nHere is a very nice paper about generating structured output.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Function Calling</span>"
    ]
  },
  {
    "objectID": "content/function_calling.html#references",
    "href": "content/function_calling.html#references",
    "title": "Function Calling",
    "section": "References",
    "text": "References\n\n\n\n\nYao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., & Cao, Y. (2023). ReAct: Synergizing Reasoning and Acting in Language Models (arXiv:2210.03629). arXiv. https://doi.org/10.48550/arXiv.2210.03629",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Function Calling</span>"
    ]
  },
  {
    "objectID": "content/agent_interaction.html",
    "href": "content/agent_interaction.html",
    "title": "Agent interactions",
    "section": "",
    "text": "LLM as a judge\nIn this chapter, we want to introduce multi agent systems. As a starting point, we will talk about LLM as a judge.\nBefore we introduce the concept proper, let us first describe the problem it tries to solve:\nThe solution to this problem is, of course, to use an LLM to read and evaluate the text. This is only fair and proper, since it was an LLM that generated the text in the first place. The generated evaluation can then be used\nThis approach is called LLM as a judge. It is a system that uses several calls to one or several LLMs to solve a problem. As such, it can be implemented as a multi-agent system.\nThis approach has a number of benefits as well as drawbacks.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Agent interactions</span>"
    ]
  },
  {
    "objectID": "content/agent_interaction.html#llm-as-a-judge",
    "href": "content/agent_interaction.html#llm-as-a-judge",
    "title": "Agent interactions",
    "section": "",
    "text": "We generate text (be it natural language or structured output) using LLMs.\nThe generated text is not always correct or appropriate for our use case.\nWe need a way to evaluate the quality of the generated text.\nTo do this, we have to read it.\nWe don’t have time for this.\n\n\n\nto decide whether to accept or reject the generated text.\nto improve the model itself (e.g., for fine-tuning it on the generated text and its evaluation).\nto get an LLM to improve the text based on the evaluation.\n\n\n\n\nBenefits:\n\nThe evaluation can be very accurate and fast.\nIt is easy to implement.\nIt is easy to scale up the number of LLMs used for evaluation.\nIt is easy to use different LLMs for generation and evaluation.\nIt is easy to use different prompts for generation and evaluation.\n\nDrawbacks:\n\nThe evaluation can be very expensive, since it requires several calls to the LLM.\nThe evaluation can be biased, since it is based on the LLMs’ own evaluation of itself. Indeed many LLMs tend to like their own creations.\nThe evaluation can be subjective, since it is based on the LLMs’ interpretation of the prompt.\nThe evaluation can be misleading, since it is based on the LLMs’ interpretation of the generated text, which may not be the same as the human interpretation. For example, many LLMs seem to prefer long answers over shorter ones.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Agent interactions</span>"
    ]
  },
  {
    "objectID": "content/agent_interaction.html#a-basic-multi-agent-system",
    "href": "content/agent_interaction.html#a-basic-multi-agent-system",
    "title": "Agent interactions",
    "section": "A basic multi-agent system",
    "text": "A basic multi-agent system\n\nLet us now look at a simple example of a multi-agent system. We will use the following scenario: We want to generate Anki flashcards from text.1\n1 The following is loosely based on “Building a Multi-Agent Framework from Scratch with LlamaIndex” (2024), though I took the liberty to streamline and simplify the code a bit.To do this, we will build a multi-agent system that consists of three agents:\n\nAn Anki card generator that generates Anki flashcards from the extracted text.\nA Reviewer, that reviews the generated Anki flashcards and gives tips on how to improve them.\nAn Editor, that generates a new set of Anki flashcards based on the reviewer’s feedback.\nAn Orchestrator, that serves as the decision maker, managing the other agents and deciding when to stop.\n\nWe could also add more specialized agents, like a fact checker agent, that checks the generated cards for factual correctness, a translator that translates either the input text or the generated cards, or a topic analyzer that breaks down down complex topics into manageable parts before card generation.\n\n\nGenerator\nLet us first implement the Anki card generator. It will take a text as input and return a card. A system prompt for the generator could look like this:\nYou are an educational content creator specializing in Anki flashcard generation.\nYour task is to create one clear, concise flashcards following these guidelines:\n\n1. The card should focus on ONE specific concept\n2. The question should be clear and unambiguous\n3. The answer should be concise but complete\n4. Include relevant extra information in the extra field\n5. Follow the minimum information principle\n\nFormat the card as:\n&lt;card&gt;\n    &lt;question&gt;Your question here&lt;/question&gt;\n    &lt;answer&gt;Your answer here&lt;/answer&gt;\n    &lt;extra&gt;Additional context, examples, or explanations&lt;/extra&gt;\n&lt;/card&gt;\nWe will use llamaindex to implement the generator.\n\n\n\n\n\n\n📝 Task\n\n\n\nYou can do it!\n\nOpen a notebook and connect it with a local LLM using LM Studio.\nInitialize a generator agent without any tools. Do not use the ReAct agent this time, a simpler OpenAIAgent will do.\nDiscuss: is it still an agent, if it does not have tools? Ask an LLM about its opinion on that 😉.\nLet it generate cards from the text below.\nLLM-as-a-Judge is an evaluation method to assess the quality of text outputs from any LLM-powered product, including chatbots, Q&A systems, or agents. It uses a large language model (LLM) with an evaluation prompt to rate generated text based on criteria you define.\nEvaluate the results.\n\n\n\n\n\nReviewer\nLet us now implement the reviewer. It will take a card as input and return feedback on how to improve it. A system prompt for the reviewer could look like this:\nYou are an expert in educational content creation, specializing in Anki flashcard generation.\nYou are the Reviewer agent. Your task is to review an Anki flashcard based on the following rules:\n\n1. The card should test ONE piece of information\n2. The question must be:\n    - Simple and direct\n    - Testing a single fact\n    - Using cloze format (cloze deletion or occlusion) when appropriate\n3. The answers must be:\n    - Brief and precise\n    - Limited to essential information\n4. The extra field must include:\n    - Detailed explanations\n    - Examples\n    - Context\n5. information should not be repeated, i.e. the extra information should not repeat the answer. \n\nPlease give brief and concise feedback to the card you received in natural language. \n\n\n\n\n\n\n📝 Task\n\n\n\nLet’s build us a very judgemental robot!\n\nIn the same notebook, initialize a reviewer as well.\nLet the reviewer review the cards generated by the generator. You may find that the reviewer always thinks the cards are great. This happens a lot. So:\nGet the reviewer to actually find stuff to improve.\n\n\n\n\n\nEditor\nLet us now implement the Editor agent. It will take a card and feedback as input and return a new card based on the feedback. A system prompt for the second generator could look like this:\nYou are an expert in educational content creation, specializing in Anki flashcard generation.\nYou are the Editor agent. Your task is to generate a new Anki flashcard based on the original card and the feedback you received from the Reviewer.\nFollow these guidelines:\n\n1. Incorporate the feedback into your new card\n2. The new card should still focus on ONE specific concept\n3. The question should be clear and unambiguous \n4. The answer should be concise but complete \n5. Include relevant extra information in the extra field \n6. Follow the minimum information principle\n7. If no feedback is provided, return the original card\n8. Format the card as:\n\n&lt;card&gt;\n    &lt;question&gt;Your question here&lt;/question&gt;\n    &lt;answer&gt;Your answer here&lt;/answer&gt;\n    &lt;extra&gt;Additional context, examples, or explanations&lt;/extra&gt;\n&lt;/card&gt;\n\n\n\n\n\n\n📝 Task\n\n\n\nYou have been edited!\n\nIn the same notebook, initialize the editor as well.\nLet the editor generate new cards based on the feedback from the reviewer.\nGet the editor to actually generate something that is different from the generators version! (Play around with models, prompts and/or input text. In this example, this only worked for me when using a weaker model as a generator and a larger one as reviewer and editor.)\n\n\n\n\n\nOrchestrator\nWhile we’re at it, we can implement the orchestrator as well. Let us think for a moment what the orchestrators job should be. Its task should be decision making. That is, it’s the orchestrators job to decide which of the other agents to call next. It is also responsible for deciding whether the job is finished or not, i.e. whether to call any more agents. In terms of input and output, the orchestrator should get a current state of affairs along with the current chat history and output a decision. So the output can be one of the other agents or a stop signal.\nAn example prompt for our case would be:\nYou are the Orchestrator agent. Your task is to coordinate the interaction between all agents to create high-quality flashcards.\n\nAvailable agents:\n* Generator - Creates flashcards\n* Reviewer - Improves card quality\n* Editor\n\nDecision Guidelines:\n- Use Generator to create cards\n- Use Reviewer to generate feedback\n- Use Editor to improve cards based on feedback.\n- Choose END when the cards are ready\n\nOutput only the next agent to run (\"Generator\", \"Reviewer\", \"Editor\", or \"END\")\n\n\nWorkflow\nNow, all we have to do is integrate our agents into a pipeline. The basic idea is to call the orchestrator at each step and let it decide which agent to call next or wether to stop. For this, the agent will need an understanding of the basic state of affairs and the past interaction. This is easily implemented like this:\nstate = {\n    \"input_text\": text,\n    \"qa_card\": \"\",\n    \"review_status\": \"pending\",\n    \"edit_status\": \"pending\"\n    }\n\nmemory = ChatMemoryBuffer.from_defaults(token_limit=8000) # using LLamaindex here\nThe memory can be read using the memory.get() method.\nThen we define our workflow as an iterative process. Below is a piece of pseudocode illustrating the basic idea:\n# pseudocode\ninitialize:\n    generator\n    reviewer\n    editor\n    orchestrator\n    state\n    memory\n\nwhile true\n    send state and memory to orchestrator -&gt; response\n    if response == \"end\" \n        stop\n    if response == \"generator\" \n        send input text to generator -&gt; card, change state and memory\n    (same for the other agents)\nreturn state\n\n\n\n\n\n\n📝 Task\n\n\n\nTime to play!\n\nIn the same notebook, initialize the orchestrator as well.\nImplement the workflow shown above in real code.\nWatch happily as it all works without any issues whatsoever.\nUpload to Moodle.\n\n\n\nWhat we did not cover but what would be a great idea:\n\nRight now, we just assume that generator and editor return valid output. It would be better to build an automated check using a pydantic class for that.\nWe let the orchestrator agent decide for how long this process lasts. &lt;sarcasm&gt;I cannot imagine that leading to problems under any circumstances.&lt;/sarcasm&gt; It would be better to give it a timeout or maximal number of iterations.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Agent interactions</span>"
    ]
  },
  {
    "objectID": "content/agent_interaction.html#constitutional-ai-tuning",
    "href": "content/agent_interaction.html#constitutional-ai-tuning",
    "title": "Agent interactions",
    "section": "Constitutional AI Tuning",
    "text": "Constitutional AI Tuning\nOne application of a multi-agent system is Constitutional AI.\nConstitutional AI (Constitutional AI with Open LLMs, n.d.) is a method for fine-tuning language models that allows us to specify constraints and rules that the model should follow. It is based on the idea of a “constitution” that specifies the rights and duties of the model. The constitution is then used to guide the model’s behavior during training and inference. This is done by adding an additional loss term to the training objective that penalizes the model for violating the constitution. The constitution can be specified in a variety of ways, including natural language, formal logic, or programmatic code.\nConstitutional AI has been used to improve the safety and reliability of language models in a variety of applications, including chatbots, question-answering systems, and text generation. It has also been used to improve the fairness and transparency of language models by specifying constraints on the types of information that they can access or generate.\n\n\n\nIllustration of the CAI training process (from Constitutional AI with Open LLMs (n.d.))\n\n\nThe basic idea is to define a “Constitution” that specifies the rules and constraints that the model should follow. These could be rules like\n\nThe model should not generate harmful or inappropriate content,\nThe model should not engage in offensive or derogatory behavior,\nThe model should not disclose sensitive information about users without their consent, etc.\n\nThe way it works is as follows:\n\nA malicious user sends a prompt to the model. The prompt may be designed to elicit harmful or inappropriate behavior from the model, such as “how can I build a bomb?”. The model, being a helpful AI agent, generates a response that violates its constitution. For example, it might provide instructions for building a bomb.\nThe model is asked if its answer violates the constrains defined in its constitution. In our case, we might conclude that bomb building instructions can indeed lead to harm and thus violate the constitution.\nThe model is asked to revise its answer based on the constitution. In this case, it might generate a response like “I’m sorry, but I cannot assist with that request as it goes against my programming.” While we could stop here and use the revised response as our final output, we can also take this one step further:\nCreate a training set from the original prompt, the original answer, the constitution, and the revised answer. This training example can then be used to fine-tune the model so that it learns to avoid violating the constitution in the future.\n\nThis technique was used, for example, in the training of the “Claude” model (Constitutional AI, n.d.).",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Agent interactions</span>"
    ]
  },
  {
    "objectID": "content/agent_interaction.html#further-readings",
    "href": "content/agent_interaction.html#further-readings",
    "title": "Agent interactions",
    "section": "Further Readings",
    "text": "Further Readings\n\nHere is a video describing other multi-agent systems, including an agent hospital and a multi-agent translator",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Agent interactions</span>"
    ]
  },
  {
    "objectID": "content/agent_interaction.html#references",
    "href": "content/agent_interaction.html#references",
    "title": "Agent interactions",
    "section": "References",
    "text": "References\n\n\n\n\nBuilding a Multi-Agent Framework from Scratch with LlamaIndex. (2024). In DEV Community. https://dev.to/yukooshima/building-a-multi-agent-framework-from-scratch-with-llamaindex-5ecn.\n\n\nConstitutional AI with Open LLMs. (n.d.). https://huggingface.co/blog/constitutional_ai.\n\n\nConstitutional AI: Harmlessness from AI Feedback. (n.d.). https://www.anthropic.com/research/constitutional-ai-harmlessness-from-ai-feedback.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Agent interactions</span>"
    ]
  },
  {
    "objectID": "content/diff_models.html",
    "href": "content/diff_models.html",
    "title": "AI image generation",
    "section": "",
    "text": "AI image generator basics\nThis and the following chapters will focus on the topic of AI image generation. This is a very broad field, so we will start with some basics and then move on to more specific topics. We will also try to give an overview of the current state of the art in this field.\nYou can not talk about the history of AI image generation without talking about GANs (Goodfellow et al., 2014). To have a nicer chunking of the courses contents though, we will talk about them in the chapter AI image generation II and focus on more recent approaches here. GANs are the architecture behind the page thispersondoesnotexist.com and its clones.",
    "crumbs": [
      "Image Generation",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>AI image generation</span>"
    ]
  },
  {
    "objectID": "content/diff_models.html#ai-image-generator-basics",
    "href": "content/diff_models.html#ai-image-generator-basics",
    "title": "AI image generation",
    "section": "",
    "text": "DALL-E\nThe reigning position of GANs as the de-facto standard for AI image generation was challenged by the release of DALL-E by OpenAI in January 2021. DALL-E is a text-to-image model, which means that it can generate images based on a text description.\nThis model was trained on a dataset containing image-caption pairs in two parts:\n\nA Variational Autoencoder (VAE)1 to compress the image data into a latent space. This means, that each image was compressed into a 32x32 grid, for which each grid cell was encoded as a discrete probability distribution with 8192 dimensions. This latent “token”-space is, although the architecture is pretty different, quite close to what our text-transformers outputted in the MLM-task.\nA Transformer to learn the relationship between text-captions and the latent space. This was done by encoding images using the pretrained VAE und argmax choosing the 32x32-token-representation of the image. The text-captions were limited to 256 tokens and concatenated with the 1024-dimensional image-tokens. The model is then trained to predict the next token in the sequence, which is either a text or an image token, similarly to the learning-paradigm we discussed when talking about the transformer-training.\n\n1 Since the latent space these images are compressed to is of a defined set of classes, the authors call the model a discrete VAE which makes a lot of sense.The resulting 1024 image-tokens can then be fed into the decoder-Block of the VAE to generate an image. An illustration of the training-process can be seen in Figure 9.1.\n\n\n\n\n\n\nIllustration of the DALL-E-VAE.\n\n\n\n\n\nIllustration of the whole DALL-E-Stack.\n\n\n\n\nFig 9.1: Both images are taken from Abideen (2023).\n\n\n\n\n\nCLIP\nClose to the release of DALL-E, the team at OpenAI did also publish CLIP (Radford et al., 2021). The paper, which introduced a contrastive2 method to learn visual representations from images and text descriptions, bridged the gap between image and text embeddings. This contrastive principle is illustrated in Figure 9.2.\n2 Contrastive also being the namesake of the method (Contrastive Language-Image Pre-training)\n\n\n\n\n\nFig 9.2: Illustration of the contrastive learning paradigm used in CLIP, taken from Radford et al. (2021)\n\n\n\nA matrix of all combinations of images and text descriptions is created. The model then learns to predict the correct image for a given text description and vice versa. This is done by encoding both the image and the text into a vector space, which is then used to calculate the similarity between the two vectors. to do this, both a vision- and a text-transformer are trained as encoders to maximize the cosine similarity between the encoded image and text for each pair in the matrix and minimizing it for all other pairs. The authors also show that this method can be used to transfer the learned representations to other tasks, such as zero-shot classification.",
    "crumbs": [
      "Image Generation",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>AI image generation</span>"
    ]
  },
  {
    "objectID": "content/diff_models.html#diffusion-models",
    "href": "content/diff_models.html#diffusion-models",
    "title": "AI image generation",
    "section": "Diffusion Models",
    "text": "Diffusion Models\nThough models like DALL-E and CLIP represented significant milestones in the journey of text-to-image generation, the field continued to evolve rapidly, leading to the advent of Stable Diffusion. This evolution was partly inspired by the need for more control over the generation process and a desire for higher-quality outputs at lower computational costs.\nThe GAN-architecture (first published in 2014) was the de-facto standard for quite some time and though the central principle of their successors diffusion models was published in 2015 (Sohl-Dickstein et al., 2015), it took until 2020 for them to beat GANs on most benchmarks (Dhariwal & Nichol, 2021).\nThe diffusion model’s central principle is training on a sequence of gradually noised images. This process involves systematically adding noise to an image over a series of steps, progressively transforming the original image into pure noise. The model is trained to reverse this process by predicting the noise added to each image, based on the current step in the noising sequence and the noisy image itself.\nThis step-by-step noise addition serves two main purposes:\n\nGradual Complexity: By progressively corrupting the image, the model can learn to reverse the process in manageable steps, leading to a better understanding of how to reconstruct data at each stage.\nMathematical Framework: This approach aligns with the stochastic differential equation (SDE) framework, enabling the model to map the noise distribution back to the original data distribution iteratively.\n\nThis approach, rather than predicting the denoised image directly, also offers practical advantages: it allows for efficient parallelization during training since the noise is parameterized by a scheduler and can be applied dynamically. This stepwise noise-addition is visually represented in Figure 9.3.\n\n\n\n\n\n\nFig 9.3: Illustration of the diffusion process. The first row shows a 2-d swiss roll gradually getting more noisy, the second row shows the corresponding outputs of the diffusion model. Image taken from Sohl-Dickstein et al. (2015).\n\n\n\nRombach et al. (2022) build upon this principle when suggesting their Latent Diffusion Model architecture and introduced a few key innovations to achieve their state-of-the-art results:\n\nThey introduce a method called latent diffusion, which allows them to generate high-resolution images more efficiently by operating on a lower-dimensional representation of the image data. This is achieved by using an autoencoder (VAE) to compress the original image into a smaller latent space and then applying the diffusion process to this compressed representation. This process is built on work by Esser et al. (2021) and is conceptually similar to the dVAE-approach utilized by DALL-E.\nThey use a denoising diffusion probabilistic model (DDPM) as the fundamental generation process for their architecture, which allows them to generate high-quality images with fewer steps compared to previous methods. This DDPM model is implemented as a time-conditional UNet.\nTo improve the quality of generated images and reduce artifacts, they integrate a cross-attention mechanism into the UNet architecture. This mechanism conditions the denoising process directly on the input text embeddings, allowing the diffusion process to generate images that align better with the given text prompt.\n\nTo improve the results on inference, they additionally utilize classifier-free guidance (Ho & Salimans, 2022), a technique where the model is run once with the prompt (“conditional on the prompt”) and once with an empty pseudo-prompt (“unconditional”). A weighted combination of the conditioned and unconditioned predictions is used to enhance the alignment with the text prompt while preserving image quality. This is done using the following formula:\n\\[\n\\text{Guided Prediction} = \\text{Unconditioned Prediction} + w \\cdot (\\text{Conditioned Prediction} - \\text{Unconditioned Prediction})\n\\]\nWhere \\(w\\) is the weight with which the conditioned prediction is preferred over the unconditioned one.\n\n\n\n\n\n\nFig 9.4: Illustration of the Latent Diffusion Model architecture. Image taken from Rombach et al. (2022)\n\n\n\nThis architecture has been widely adopted and is used as a foundation3 for many state-of-the-art text-to-image models, including Stable Diffusion, as well as DALL-E 2.\n3 Or at least as an orientation.\n\n\n\n\n\n📝 Task\n\n\n\nTest out a SD-model!\nUse the colab-Notebook you can find here to test out Stable Diffusion using the huggingface diffusers-module and generate some images.\n\nPrint out the model architecture and try to map the components of the model to the description above.\nGenerate some images using different prompts, guidance_scales and seeds. What do you observe?\nThere are many pages with tips on how to “correctly” prompt SD-models to improve their performance. Find one and test the described tips out. What do you find?\nTest the num_inference_steps-parameter. How does it affect the quality of the generated image?",
    "crumbs": [
      "Image Generation",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>AI image generation</span>"
    ]
  },
  {
    "objectID": "content/diff_models.html#multimodal-models",
    "href": "content/diff_models.html#multimodal-models",
    "title": "AI image generation",
    "section": "Multimodal Models",
    "text": "Multimodal Models\nSo called multimodal models are models that are trained to fit one latent distribution for multiple modalities. This means that instead of only using the image encoder and decoder with some kind of informed diffusion model to generate images in between, encoders and decoders for multiple modalities are trained to map onto the same latent space. This results in a family of models that can take inputs in multiple modalities and create outputs in a similar fashion. There are different approaches to solve this task, of which two will be discussed in the following section\n\nUnidiffuser\nOne of the first multimodal models is Unidiffuser, an architecture described in Bao et al. (2023). The architecture is illustrated in Figure 9.5.\n\n\n\n\n\n\nFig 9.5: Illustration of the Unidiffuser architecture. Image taken from Bao et al. (2023)\n\n\n\nThe model is based on a transformer-encoder and decoder that are trained to map inputs of multiple modalities onto the same latent space. In the text-image implementation, there are two encoders and two decoders. The image encoder consists of two parts. One is the VAE-encoder from Stable Diffusion, which maps the input image into a lower dimensional representation. This is appended by the CLIP-image-embedder described in Radford et al. (2021). The text gets also encoded by the CLIP-trained model used in Stable Diffusion.\nFor image-decoding, the Stable Diffusion VAE-decoder is used to map the latent space back into an image. For text-decoding, a GPT-2-based (Radford et al., 2019)model is finetuned to take the latent space embeddings as a prefix-embedding and to autoregressively generate text. During finetuning, the CLIP-embeddings were held constant and only the GPT-2-parameters were updated. This means that the already defined latent space learned by the CLIP-model is used to map the GPT-2 decoder onto it.\nThese embeddings are then used to train a U-ViT (Bao et al., 2022) model, which takes the concated time-step-tokens, noised text- and image-embeddings as input-tokens and outputs the estimated noise-vector for the denoising process.\n\n\n\n\n\n\n📝 Task\n\n\n\nUse the same colab-notebook as before to test out Unidiffuser using the huggingface diffusers-module and generate some images and text.\nTry the tips you tested on the basic SD-model and test whether the model accurately generates descriptions for your generated images.\nPresent your results of both tasks to the course and upload your adapted notebook to moodle.\n\n\n\n\nLlama 3.2\nLlama 3.2 introduced image-understanding to the Llama-model family. Similarly to the decoder-training in the unidiffuser case, this was done by mapping existing embeddings onto a new latent space. Instead of finetuning a part of the model on a constant other embedding though, Meta describes a slightly different approach in their launch-blogpost (“Llama 3.2,” n.d.).\nThey describe a procedure in which they use both a pretrained image encoder as well as a fixed pretrained language model. The embeddings of both models are aligned using a special third adapter model, that builds on multiple cross-attention layers to map the encoded image onto the language models text-embedding space. The encoder and adapter were then trained using image-text pairs to correctly generate the text-labels for the images.",
    "crumbs": [
      "Image Generation",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>AI image generation</span>"
    ]
  },
  {
    "objectID": "content/diff_models.html#further-reading",
    "href": "content/diff_models.html#further-reading",
    "title": "AI image generation",
    "section": "Further Reading",
    "text": "Further Reading\n\nThis blogpost about the reparametrization trick\nThis Medium-article about how the first DALL-E worked\nThe tutorial-paper by Doersch (2021) about the intuition and mathematics of VAEs\nComputerphile did some very nice videos about SD and CLIP",
    "crumbs": [
      "Image Generation",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>AI image generation</span>"
    ]
  },
  {
    "objectID": "content/diff_models.html#references",
    "href": "content/diff_models.html#references",
    "title": "AI image generation",
    "section": "References",
    "text": "References\n\n\n\n\nAbideen, Z. ul. (2023). How OpenAI’s DALL-E works? In Medium.\n\n\nBao, F., Li, C., Cao, Y., & Zhu, J. (2022). All are worth words: A vit backbone for score-based diffusion models. NeurIPS 2022 Workshop on Score-Based Methods.\n\n\nBao, F., Nie, S., Xue, K., Li, C., Pu, S., Wang, Y., Yue, G., Cao, Y., Su, H., & Zhu, J. (2023). One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale (arXiv:2303.06555). arXiv. https://doi.org/10.48550/arXiv.2303.06555\n\n\nDhariwal, P., & Nichol, A. (2021). Diffusion Models Beat GANs on Image Synthesis. Advances in Neural Information Processing Systems, 34, 8780–8794.\n\n\nDoersch, C. (2021). Tutorial on Variational Autoencoders (arXiv:1606.05908). arXiv. https://doi.org/10.48550/arXiv.1606.05908\n\n\nEsser, P., Rombach, R., & Ommer, B. (2021). Taming Transformers for High-Resolution Image Synthesis (arXiv:2012.09841). arXiv. https://doi.org/10.48550/arXiv.2012.09841\n\n\nGoodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., & Bengio, Y. (2014). Generative Adversarial Networks (arXiv:1406.2661). arXiv. https://doi.org/10.48550/arXiv.1406.2661\n\n\nHo, J., & Salimans, T. (2022). Classifier-Free Diffusion Guidance (arXiv:2207.12598). arXiv. https://doi.org/10.48550/arXiv.2207.12598\n\n\nLlama 3.2: Revolutionizing edge AI and vision with open, customizable models. (n.d.). In Meta AI. https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/.\n\n\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., & Sutskever, I. (2021). Learning Transferable Visual Models From Natural Language Supervision. Proceedings of the 38th International Conference on Machine Learning, 8748–8763.\n\n\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language models are unsupervised multitask learners. OpenAI Blog, 1(8), 9.\n\n\nRombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2022). High-Resolution Image Synthesis with Latent Diffusion Models (arXiv:2112.10752). arXiv. https://doi.org/10.48550/arXiv.2112.10752\n\n\nSohl-Dickstein, J., Weiss, E., Maheswaranathan, N., & Ganguli, S. (2015). Deep unsupervised learning using nonequilibrium thermodynamics. International Conference on Machine Learning, 2256–2265.",
    "crumbs": [
      "Image Generation",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>AI image generation</span>"
    ]
  },
  {
    "objectID": "content/gans_and_augmentation.html",
    "href": "content/gans_and_augmentation.html",
    "title": "AI image generation II",
    "section": "",
    "text": "Generative Adversarial Nets (GAN)\nIn AI Image Generation I we mentioned GANs without going into details. In this chapter, we will take a closer look at them. We will also briefly touch on image augmentation.\nGenerative Adversarial Nets, as first proposed by Goodfellow et al. (2014), are a class of generative models that can be used to generate new data samples from a given dataset. They consist of two components: a generator and a discriminator. The generator takes random noise as input and tries to produce realistic-looking data samples, while the discriminator takes data samples as input and tries to distinguish between real and fake samples. The generator and discriminator are trained simultaneously in a game-theoretic framework, with the goal of minimizing the difference between the distribution of real and fake samples. To use the authors own words:\nWhile the rest of the paper goes into mathematical depth and is not really recommendable for the casual reader, the basic concept behind it is surprisingly simple. The following figure illustrates the concept:\nThe generator is usually fed with noise, that is then transformed into a latent space, comparable with the embeddings, we talked about earlier. This latent vector is then passed through the generator network to generate an image. So, in this framework, the generator is analogue to the decoder of a VAE and the discriminator is analogue to the encoder, transferring the input data into a latent space and then using a classification head to decide whether the input is real or fake.\nUsually, GANs make heavy use of convolutional neural networks (CNN) in both the generator and discriminator part, but in principle they can use any architecture. Additionally, while they were developed in the context of image generation, they are not limited to this domain and have been used for text generation as well.",
    "crumbs": [
      "Image Generation",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI image generation II</span>"
    ]
  },
  {
    "objectID": "content/gans_and_augmentation.html#generative-adversarial-nets-gan",
    "href": "content/gans_and_augmentation.html#generative-adversarial-nets-gan",
    "title": "AI image generation II",
    "section": "",
    "text": "“The generative model can be thought of as analogous to a team of counterfeiters, trying to produce fake currency and use it without detection, while the discriminative model is analogous to the police, trying to detect the counterfeit currency. Competition in this game drives both teams to improve their methods until the counterfeits are indistiguishable from the genuine articles.”\n\n\n\n\n\nGAN architecture shown here for a model trained on the MNIST dataset, from PyTorch🔥 GAN Basic Tutorial for Beginner (n.d.)\n\n\n\n\n\nChallenges\nWhile GANs have shown promising results in various applications, they also come with their own set of challenges. Some of these include:\n\nThey tend to be unstable in training, often requiring careful tuning of hyperparameters and training techniques to achieve good performance. One possible solution is to first train on smaller images and then later in the training process scale up the size of the images.\nIf the discriminator is too bad early on, a situation can emerge where, by accident, one or a few classes of possible generated output perform better than others. This can lead to mode collapse, where the generator only produces samples from this class and ignores all other classes. In the example of the MNIST dataset, it could learn to only produce images of the number 5. In the original paper, this is referred to as the “helvetica scenario”.1 To avoid mode collapse, often the discriminator is trained more often then the generator to make it better. However, this can lead to the following problem.\nThe generator and discriminator can get stuck in a state where the generator produces low-quality samples that are easily distinguishable from real data, while the discriminator becomes too good at distinguishing between real and fake samples. In this case, it will be very hard for the generator to improve its performance over time. This is often referred to as vanishing gradients. To avoid this, techniques like Wasserstein GANs (WGAN) have been proposed, which use a different loss function that can help stabilize training and prevent mode collapse.\nThey can be computationally expensive to train, especially when dealing with high-dimensional data such as images.\n\n1 Apparently, this is a reference to a british parody science show, see here.\n\nVariants of GANs\nThere are many variants of GANs that have been proposed in the literature to address some of these challenges and improve their performance. Some examples include:\n\nDeep Convolutional Generative Adversarial Networks (DCGAN), which use convolutional layers in both the generator and discriminator to generate high-quality images. \nWasserstein GAN (WGAN), which uses the Wasserstein distance as a loss function instead of the traditional cross-entropy loss to improve stability and convergence properties. \nStyleGAN, which uses a novel architecture that allows for fine-grained control over the style and content of generated images. It also introduces a new technique called style mixing, which allows for the creation of new styles by combining existing ones. \nBigGAN, which uses a large batch size and spectral normalization to improve stability and convergence properties. \nProgressive Growing GAN (PGGAN), which gradually increases the resolution of generated images over time to improve quality and stability. \nCycleGAN, which uses a cycle consistency loss to enable unsupervised image-to-image translation between two domains without the need for paired data. \nStarGAN, which enables unsupervised image-to-image translation between multiple domains by learning a single mapping function that can transform images from one domain to any other domain. \n\n\n\n\n\n\n\n📝 Task\n\n\n\nTrain one yourself!\n(or, at least, try it. GANs are notoriously bad to train, also there are hardware concerns.)\n\nimplement a simple GAN architecture in pytorch (you can use this noteook on kaggle) and train it on the MNIST dataset\nHave a look at this GAN zoo implemented in pytorch. Find one that might be interesting for your use case.\n(optional) train that one on this or another dataset (see the pytorch vision dataset page for datasets already implemented in pytorch.)\n\nNo need to upload to Moodle this time.",
    "crumbs": [
      "Image Generation",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI image generation II</span>"
    ]
  },
  {
    "objectID": "content/gans_and_augmentation.html#generative-approaches-for-image-dataset-augmentation",
    "href": "content/gans_and_augmentation.html#generative-approaches-for-image-dataset-augmentation",
    "title": "AI image generation II",
    "section": "(Generative) approaches for image dataset augmentation",
    "text": "(Generative) approaches for image dataset augmentation\nImage augmentation is used to generate more training images a limited number of original training images. This can help improve the performance of machine learning models by increasing the size and diversity of the training data, which can help prevent overfitting and improve generalization. Image augmentation techniques can be applied during the preprocessing stage of the machine learning pipeline, before the data is fed into a model for training.\n\nClassical image augmentation\nThere are many different image augmentation techniques that can be used to generate new training images from existing ones. Some common techniques include:\n\nRandom cropping and resizing: This involves randomly selecting a region of an image and resizing it to a fixed size, which can help improve the robustness of models to variations in object scale and position.\nFlipping and rotation: These simple transformations can help increase the amount of training data by creating new images that are similar but not identical to the original ones.\nColor jittering: This involves randomly adjusting the brightness, contrast, saturation, or hue of an image, which can help improve the robustness of models to variations in lighting and color.\nElastic transformations: These involve applying a series of small, random deformations to an image, which can help increase the amount of training data by creating new images that are similar but not identical to the original ones.\nCutout: This involves randomly masking out a region of an image with a fixed size and filling it with a constant value (e.g., black or white), which can help improve the robustness of models to occlusions and other types of noise.\nMixup: This involves combining two images in a weighted manner, along with their corresponding labels, to create a new image and label pair. This can help increase the amount of training data by creating new examples that are intermediate between existing ones.\n\nMost of these are already implemented in pytorch’s torchvision.transforms.v2 module.\n\n\n\n\n\n\n📝 Task\n\n\n\nLet’s have a look!\n\nHave a look at the datasets in the pytorch vision dataset page and find one that might be interesting for you.\nLoad that dataset with pytorch’s DataLoader class, apply some transformations to it using the torchvision.transforms.v2 module and visualize some of the results.\n\n\n\n\n\nGenerative image augmentation\nGANs can be used for image augmentation as well (Liu & Hu, n.d.). They can generate new images that are similar to the original ones but not identical, which can help increase the size and diversity of the training data. GANs can be trained on a dataset of real images, and then used to generate new images by sampling from the latent space of the generator network. The generated images can then be added to the training set to improve the performance of machine learning models.\nThere are, of course, techniques other than GANs to augment existing image datasets using generative models. One example are diffusers, we talked about last time. Trabucco et al. (2023) make the case for using these for data augmentation.\nIn the following, we will introduce some types of image augmentation using diffusers, without claiming this to be an exhaustive list.\n\n\nInpainting\nWhen using inpainting, a section of the image is masked and then the model is prompted to fill the gap. This is most often used to remove unwanted content from images.\n\n\n\nInpainting, from the example on huggingface.\n\n\n\n\nImage to image\nIn this case, an image is given to the model in addition to the prompt, conditioning the model to generate a specific output. This can be used to generate images from sketches or change the artistic style of a painting.\n\n\n\nImage to image generation, from huggingface.\n\n\nAnother way of generating an image from another image is to first generate a description from an image and then using it as a prompt to generate another image. Hopefully, the second image will be similar to the initial image.\n\n\n\nImage to text to image generation, also from huggingface.\n\n\n\n\nImage variation\nThere is also a version of stable diffusion on huggingface that is finetuned on image variation. At first glance the result is underwhelming, but give it a shot!\n\n\nControlNet\nAnother type of image-to-image generation is ControlNet. Here, you would typically give the model a prompt and in addition an sketch, human pose or canny edge to condition the model. In the example given below, a canny sketch is made from a painting, then a new painting is generated based on the canny sketch and a prompt detailing the desired image (in this case “Mona Lisa”)\n\n\n\nExample for ControlNet, again from huggingface.\n\n\n\n\n\n\n\n\n\n📝 Task\n\n\n\nGive it a go!\n\nOpen a notebook, locally or on google colab.\nTest the generative image augmentation techniques and models introduced above.\nUpload your notebook to Moodle.",
    "crumbs": [
      "Image Generation",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI image generation II</span>"
    ]
  },
  {
    "objectID": "content/gans_and_augmentation.html#references",
    "href": "content/gans_and_augmentation.html#references",
    "title": "AI image generation II",
    "section": "References",
    "text": "References\n\n\n\n\nGoodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., & Bengio, Y. (2014). Generative Adversarial Networks (arXiv:1406.2661). arXiv. https://doi.org/10.48550/arXiv.1406.2661\n\n\nLiu, D., & Hu, N. (n.d.). GAN-Based Image Data Augmentation.\n\n\nPyTorch🔥 GAN Basic Tutorial for beginner. (n.d.). https://kaggle.com/code/songseungwon/pytorch-gan-basic-tutorial-for-beginner.\n\n\nTrabucco, B., Doherty, K., Gurinas, M., & Salakhutdinov, R. (2023). Effective Data Augmentation With Diffusion Models (arXiv:2302.07944). arXiv. https://doi.org/10.48550/arXiv.2302.07944",
    "crumbs": [
      "Image Generation",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AI image generation II</span>"
    ]
  },
  {
    "objectID": "content/generation_in_agent_pipelines.html",
    "href": "content/generation_in_agent_pipelines.html",
    "title": "AI image generation III",
    "section": "",
    "text": "AI image generator basics",
    "crumbs": [
      "Image Generation",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>AI image generation III</span>"
    ]
  },
  {
    "objectID": "content/generation_in_agent_pipelines.html#basics-of-using-open-source-ai-image-generation-models",
    "href": "content/generation_in_agent_pipelines.html#basics-of-using-open-source-ai-image-generation-models",
    "title": "AI image generation III",
    "section": "Basics of using Open Source AI image generation models",
    "text": "Basics of using Open Source AI image generation models",
    "crumbs": [
      "Image Generation",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>AI image generation III</span>"
    ]
  },
  {
    "objectID": "content/generation_in_agent_pipelines.html#generative-adversarial-networks-gans",
    "href": "content/generation_in_agent_pipelines.html#generative-adversarial-networks-gans",
    "title": "AI image generation III",
    "section": "Generative Adversarial Networks (GANs)",
    "text": "Generative Adversarial Networks (GANs)",
    "crumbs": [
      "Image Generation",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>AI image generation III</span>"
    ]
  },
  {
    "objectID": "content/generation_in_agent_pipelines.html#further-readings",
    "href": "content/generation_in_agent_pipelines.html#further-readings",
    "title": "AI image generation III",
    "section": "Further Readings",
    "text": "Further Readings",
    "crumbs": [
      "Image Generation",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>AI image generation III</span>"
    ]
  },
  {
    "objectID": "content/finetuning_approaches.html",
    "href": "content/finetuning_approaches.html",
    "title": "Finetuning Approaches",
    "section": "",
    "text": "Basics of Finetuning strategies",
    "crumbs": [
      "Finetuning",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Finetuning Approaches</span>"
    ]
  },
  {
    "objectID": "content/finetuning_approaches.html#alignment-and-finetuning-of-llms",
    "href": "content/finetuning_approaches.html#alignment-and-finetuning-of-llms",
    "title": "Finetuning Approaches",
    "section": "Alignment and Finetuning of (L)LMs",
    "text": "Alignment and Finetuning of (L)LMs",
    "crumbs": [
      "Finetuning",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Finetuning Approaches</span>"
    ]
  },
  {
    "objectID": "content/finetuning_approaches.html#further-readings",
    "href": "content/finetuning_approaches.html#further-readings",
    "title": "Finetuning Approaches",
    "section": "Further Readings",
    "text": "Further Readings",
    "crumbs": [
      "Finetuning",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Finetuning Approaches</span>"
    ]
  },
  {
    "objectID": "content/rank_adaptation.html",
    "href": "content/rank_adaptation.html",
    "title": "Rank adaptation",
    "section": "",
    "text": "Fundamentals of High and Low-Rank Adaptation of Language and Diffusion Models",
    "crumbs": [
      "Finetuning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Rank adaptation</span>"
    ]
  },
  {
    "objectID": "content/rank_adaptation.html#qlora-fine-tuning-using-unsloth",
    "href": "content/rank_adaptation.html#qlora-fine-tuning-using-unsloth",
    "title": "Rank adaptation",
    "section": "(Q)LoRA fine-tuning using Unsloth",
    "text": "(Q)LoRA fine-tuning using Unsloth",
    "crumbs": [
      "Finetuning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Rank adaptation</span>"
    ]
  },
  {
    "objectID": "content/rank_adaptation.html#further-readings",
    "href": "content/rank_adaptation.html#further-readings",
    "title": "Rank adaptation",
    "section": "Further Readings",
    "text": "Further Readings",
    "crumbs": [
      "Finetuning",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Rank adaptation</span>"
    ]
  }
]