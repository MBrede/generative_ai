[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Generative AI",
    "section": "",
    "text": "Introduction\nThis script serves as an introduction to Generative AI and was developed for the elective module ‚ÄúGenerative AI,‚Äù offered to master‚Äôs students of the ‚ÄúData Science‚Äù program at the University of Applied Sciences Kiel. Built using quarto, this resource is designed to provide an accessible overview of key topics and applications in this rapidly evolving field.\nWhile not an exhaustive guide to Generative AI, the script highlights foundational concepts, modern applications, and practical techniques that empower students to engage with and explore the possibilities of these transformative technologies.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#contents-and-learning-objectives",
    "href": "index.html#contents-and-learning-objectives",
    "title": "Generative AI",
    "section": "Contents and learning objectives",
    "text": "Contents and learning objectives\nContents listed in the module database entry:\nOpen Source Language Models\n\nOverview of model lists\nOllama\nGeneration of synthetic text as training sets\n\nAgent and LLM-Pipeline Systems\n\nLlamaindex, LangChain & smolagents\nFunction calling\nLLM-based pipelines\n\nEmbeddings and Vector Stores\n\nSemantic Search\nRetrieval-augmented generation\nRecommendations\n\nAI Image Generators\n\nGenerative Adversarial Networks (GANs)\nVariational Autoencoders / Diffusion Models\nGenerative approaches for image dataset augmentation\n\nFine-Tuning of LLMs and Diffusion Models\n\nExamples: LoRA, QLoRA, MoRA\n\n\nLearning objectives listed in the module database entry:\nStudents\n\nknow the fundamentals of generative AI systems.\nknow various modern applications of generative AI systems.\nknow the theoretical foundations and practical applications of generative AI systems.\n\nStudents\n\nare able to explain and apply various open-source language models.\nare able to implement and utilize agent systems and their functionalities.\nare able to understand and use embeddings and vector stores for semantic search and recommendations.\nare able to explain and practically apply different methods for image generation.\nare able to fine-tune large language models (LLMs) and diffusion models for specific tasks.\n\nStudents\n\nare able to successfully organize teamwork for generative AI projects.\nare able to report and present team solutions for practical project tasks.\nare able to interpret and communicate the approaches in technical and functional terms.\n\nStudents\n\nare able to work professionally in the field of generative AI systems.\nare able to give and accept professional feedback to different topics of generative AI systems.\nare able to select relevant scientific literature about generative AI systems.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Generative AI",
    "section": "Schedule:",
    "text": "Schedule:\n\nCourse schedule\n\n\n\n\n\n\n\n\nNumber:\nDate:\nTitle:\nTopics:\n\n\n\n\n1\n10.11.\nGetting started with (L)LMs\nLanguage Model Basics\n\n\n\n\n\nChoosing open source models\n\n\n\n\n\nBasics of using open source models (Huggingface, Ollama, LLM-Studio, Llama.cpp, ‚Ä¶)\n\n\n2\n12.11.\nPrompting\nPrompting strategies\n\n\n\n\n\nGeneration of synthetic texts\n\n\n3\n17.11.\nFunction Calling\nCode generation and function calling\n\n\n\n\n\nMCP\n\n\n4\n19.11.\nEmbedding-based retrieval systems\nSemantic embeddings and vector stores\n\n\n\n\n\nRetrieval augmented and interleaved generation\n\n\n5\n24.11.\nAgent basics\nFundamentals of agents\n\n\n\n\n\nExamples of agent-frameworks (Llamaindex, LangChain & smolagents)\n\n\n6\n26.11.\nLLM-pipelines\n\n\n\n7\n1.12.\nLLM-based automated Data Analysis\n\n\n\n8\n3.12.\nAI image generation I\nAI image generator basics\n\n\n\n\n\nDiffusion Models and Variational Autoencoders\n\n\n\n\n\nMultimodal models\n\n\n9\n8.12.\nAI image generation II\nUsing Open Source AI image generation models\n\n\n\n\n\nAI image generators in agent systems\n\n\n10\n10.12.\nFinetuning Basics\nBasics of Finetuning strategies\n\n\n\n\nRank adaptation\nFundamentals of High and Low-Rank Adaptation of Language and Diffusion Models\n\n\n\n\n\n(Q)LoRA fine-tuning using Unsloth\n\n\n11\n15.12.\nAlignment\nCentral principles of Model-Alignment\n\n\n\n\n\nReinforcement Learning from Human Feedback (RLHF)\n\n\n12\n17.12.\nProject presentations\n\n\n\n\n16.01.\nProject submission on moodle",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "content/orga.html",
    "href": "content/orga.html",
    "title": "Organizational Details",
    "section": "",
    "text": "Planned Class Structure\nEach class meeting will follow this structure:\nStudents will be divided into teams of three at the start of the course, with projects culminating in a final presentation to the class. The project grade will count towards your final course grade.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Organizational Details</span>"
    ]
  },
  {
    "objectID": "content/orga.html#planned-class-structure",
    "href": "content/orga.html#planned-class-structure",
    "title": "Organizational Details",
    "section": "",
    "text": "Instructional Session: We‚Äôll introduce new concepts and techniques.\nPractice Exercise: Students will apply these concepts through an exercise.\nProject Worktime: Students will work on their team projects.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Organizational Details</span>"
    ]
  },
  {
    "objectID": "content/project_details.html",
    "href": "content/project_details.html",
    "title": "Project Details",
    "section": "",
    "text": "A note about using LLMs for your Project\nProjects should allow students to apply what they‚Äôve learned throughout the course. They must implement an LLM-based system that includes at least two of the following features:\nThe project should also include function-calling-based interface (‚Äúa tool‚Äù) to an AI image generator.\nStudents are free to choose their project topic, as long as it fits within the course scope and is approved by the instructor. All projects must be implemented in Python.\nThe active participation on the course will be taken into account before grading. This means that all tasks asking the students to upload their results to moodle should be completed. If more than one of the required tasks is missing, the student will not be graded.\nThe projects are to be presented in the last session of the course. The students of each group need to take part in this session. The presentation will become part of the overall grade. The presentation can but does not have to be prepared in PPT, any other mode of presentation (including a live-demo based on a nice notebook) is fine.\nThe project will then be graded based on these contents in addition to the following criteria:\nExample Project Ideas:\nLet‚Äôs start with a small psychological demonstration.\nLook at these anagrams:\n\\[\n\\begin{array}{ccc}\n\\text{edbbal} & \\rightarrow & \\text{dabble} \\\\\n\\text{eaeslg} & \\rightarrow & \\text{eagles} \\\\\n\\text{fcbair} & \\rightarrow & \\text{fabric} \\\\\n\\text{elsmod} & \\rightarrow & \\text{models} \\\\\n\\text{actysh} & \\rightarrow & \\text{yachts}\n\\end{array}\n\\]\nHow long would you take to solve such an anagram?",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Project Details</span>"
    ]
  },
  {
    "objectID": "content/project_details.html#a-note-about-using-llms-for-your-project",
    "href": "content/project_details.html#a-note-about-using-llms-for-your-project",
    "title": "Project Details",
    "section": "",
    "text": "A: 30 sec\nB: &gt; 30 sec, &lt; 1 min\nC: &gt; 1 min, &lt; 1:30 min\nD: &gt; 1:30 min, &lt; 2 min\nE: &gt; 2 min\n\n\n\n\n\\[\n\\begin{array}{ccc}\n\\text{piemls} & \\rightarrow & \\text{???}\n\\end{array}\n\\]\n\n\nPlease take your time to think about how long you will take before clicking here to unveiling the anagram you are to solve.\n\n\n\n\n\nI will not show you the solution, though I assure you that it is quite simple.\nThe main point of this demo is to illustrate the psychological bias often called overconfidence. This effect takes place when you underestimate the effort a reaching a solution takes when you are directly presented with the solution.\nIn terms of using Gen AI to solve tasks, findings in the same vain can be found in Stadler et al. (2024), who ran a study in which students were asked to research nanoparticles in sunscreen either using search engines or ChatGPT 3.5.\nTheir\n\nResults indicated that students using LLMs experienced significantly lower cognitive load. However, despite this reduction, these students demonstrated lower-quality reasoning and argumentation in their final recommendations compared to those who used traditional search engines.\n\nand they argue further that\n\n[‚Ä¶] while LLMs can decrease the cognitive burden associated with information gathering during a learning task, they may not promote deeper engagement with content necessary for high-quality learning per se.\n\nGiving a lecture about Gen AI and expecting the students to not use seems rather pointless, but we will use the presentation at the end of the semester to test if you do indeed understand your solution to test the depth of your engagement with the lecture‚Äôs contents.\n\n\nShow solution\n\n\n\n\n\n\nStadler, M., Bannert, M., & Sailer, M. (2024). Cognitive ease at a cost: LLMs reduce mental effort but compromise depth in student scientific inquiry. Computers in Human Behavior, 160, 108386. https://doi.org/10.1016/j.chb.2024.108386",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Project Details</span>"
    ]
  },
  {
    "objectID": "content/getting_started_with_llms.html",
    "href": "content/getting_started_with_llms.html",
    "title": "Getting started with (L)LMs",
    "section": "",
    "text": "Language Model Basics\nThis chapter provides a brief introduction to the history and function of modern language models, focusing on their practical use in text generation tasks. It will then give a short introduction on how to utilize pretrained language models for your own applications.\nLanguage models have diverse applications, including speech recognition, machine translation, text generation, and question answering. While we‚Äôll concentrate on text generation for this course, understanding the general concept of language models is crucial. Given language‚Äôs inherent complexity and ambiguity, a fundamental challenge in NLP is creating structured representations that can be employed downstream. This section will first explore the evolution of these representations before introducing the transformer architecture, which forms the foundation of most modern language models.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Getting started with (L)LMs</span>"
    ]
  },
  {
    "objectID": "content/getting_started_with_llms.html#language-model-basics",
    "href": "content/getting_started_with_llms.html#language-model-basics",
    "title": "Getting started with (L)LMs",
    "section": "",
    "text": "A short history of natural language processing\n\n\n\n\n\n\nFig¬†3.1: BOW-representation of sentences.\n\n\n\nThe Bag Of Words (BOW) method represents text data by counting the frequency of each word in a given document or corpus. It treats all words as independent and ignores their order, making it suitable for tasks like text classification, for which it was traditionally the gold-standard. However, BOW has limitations when it comes to capturing semantic relationships between words and gets utterly useless if confronted with words not represented in the corpus. Additionally, it does not take into account the order of words in a sentence, which can be crucial for understanding its meaning. For example, the sentences ‚ÄúThe cat is on the mat‚Äù and ‚ÄúThe mat is on the cat‚Äù have different meanings despite having the same set of words.\n\n\n\n\n\n\nFig¬†3.2: CBOW-representation of corpus.\n\n\n\nThe Continuous Bag Of Words (CBOW) method extends traditional BOW by representing words as dense vectors in a continuous space. CBOW predicts a target word based on its context, learning meaningful word representations from large amounts of text data.\n\n\n\n\n\n\nFig¬†3.3: Shallow Model using CBOW-Method to predict missing word.\n\n\n\nfastText (Bojanowski et al., 2017), an open-source library developed by Facebook, builds upon the CBOW method and introduces significant improvements. It incorporates subword information and employs hierarchical softmax for efficient training on large-scale datasets. Even with limited data, fastText can learn meaningful word representations. fastText and its predecessor Word2Vec are considered precursors to modern language models due to their introduction of Embeddings, which laid the foundation for many modern NLP methods. Figure¬†3.3 illustrates this fastText-architecture1 Figure¬†3.3 additionally illustrates quite nicely the learning paradigm that modern language models still use - the so called masked language modelling (MLM). This paradigm presents the language model with the semi-supervised task of predicting a masked (i.e., missing) token from a presented sequence of tokenized text. Token means one of the word(-parts) that the model has represented in its vocabulary. This prediction is represented as probabilities of all possible tokens.\n1¬†Well, kind of. One of the major advantages of fasttext was the introduction of subword information which were left out of this illustration to save on space. This meant that uncommon words that were either absent or far and few between in the training corpus could be represented by common syllables. The display like it is here is far closer to fasttext‚Äôs spiritual predecessor word2vec (Mikolov et al., 2013).\n\n\n\n\n\nFig¬†3.4: Model using CBOW-Method to predict missing word.\n\n\n\nLanguage Model Embeddings are learned by predicting the next/missing token in a sequence. The utilisation of word-parts instead of whole words as tokens was another invention introduced by fastText (Bojanowski et al., 2017), that allowed the model to generalize to new, unknown words when moving to inference. Embeddings are the representation the model learns to map the context-tokens to a multiclass classification of the missing token in the space of all possible tokens. These embeddings capture semantic and syntactic relationships between words, enabling them to understand context effectively. Since these embeddings represent the conditional probability distribution that language models learn to comprehend natural language, they can be reused by other models for tasks such as text classification or text retrieval. But more on this later.\nStill, these models did not really solve the inherent issue of the order of words in a sentence. The input of models of this generation still used a dummyfied version of the corpus to represent context, which loses a lot of information.\n\n\n\n\n\n\nFig¬†3.5: Illustration of a simple RNN-model, (exaggeratingly) illustrating the issue of the model ‚Äúforgetting‚Äù parts of the input when processing long sequences.\n\n\n\nTraditionally, this was approached by feeding these embeddings into Recurrent Neural Networks (RNNs). These models could learn to keep track of sequential dependencies in text data and improve the understanding of context. However, RNNs suffered from their architecture‚Äôs inherent inability to retain information over long sequences. Simple RNN- cells2 iterate through a sequence and use both their last output and the next sequence element as input to predict the next output. This makes it hard for them to learn long-term dependencies, since they have to compress all information into one vector (Figure¬†3.5).\n\n2¬†And pretty much all of the more complex variantsLong Short-Term Memory (LSTM) networks addressed this issue by introducing a mechanism called ‚Äúgates‚Äù that allowed information to flow through the network selectively and more efficiently, but were, as the RNNs before, notoriously slow in training since only one word could be processed at a time. Additionally, a single LSTM is still only able to process the input sequence from left to right, which is not ideal for inputs that contain ambiguous words that need context after them to fully understand their meaning. Take the following part of a sentence:\n\nThe plant was growing\n\nThe word plant get‚Äôs wildly differing meanings, depending on how the sentence continues:\n\nThe plant was growing rapidly in the sunny corner of the garden.\n\n\nThe plant was growing to accommodate more machinery for production.\n\nA model that only processes the input sequence from left to right would just not be able to understand the meaning of ‚Äúplant‚Äù in this context.\nThe ELMo model (Peters et al., 2018), which stands for Embeddings from Language Models, is an extension of LSTMs that improved contextual word representations. ELMo uses bidirectional LSTM layers to capture both past and future context, enabling it to understand the meaning of words in their surrounding context. This resulted in ELMo outperforming other models of its era on a variety of natural language processing tasks. Still as each of the LSTM-Layer were only able to process one part of the sequence at a time, it was still unfortunately slow in training and inference. Its performance additionally decreased with the length of the input sequence since LSTM-cells have a better information retention than RNNs but are still not able to keep track of dependencies over long sequences.\n\n\nAttention is all you need\nIn their transformative paper ‚ÄúAttention is all you need‚Äù, Vaswani et al. (2023) described the transformer architecture.\nAs the paper‚Äôs title neatly suggests, the major breakthrough presented in this paper was the introduction of the so-called self-attention mechanism. This mechanism allows the model to ‚Äúfocus‚Äù on different parts of the input to a) determine the appropriate context for each word and b) to improve its performance on differing tasks by allowing the model to filter unnecessary information.\n\nSelf-Attention Mechanism\nThe self-attention mechanism relies on three components: Query (Q), Key (K), and Value (V), inspired by concepts in information retrieval. Imagine you search for a specific term in a library (query), match it against the catalogue (key), and use this information about the catalogue to update your personal priority of search terms (value).\nIn practice, for each word in a sentence, the model calculates:\n\nRelevance Scores: Compare each Query vector (Q) with every Key vector (K) in the sequence using the dot product. These scores measure how much focus one word should have on another.\nAttention Weights: Normalize the scores using a softmax function to ensure they sum to 1, distributing focus proportionally across all words.\nWeighted Sum: Multiply each Value vector (V) by its corresponding attention weight to compute the final representation.\n\n\n\nCalculating Attention\nFor a sequence of words, the attention scores are computed as: \\[\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n\\]\nwhere:\n\n\\(Q\\) represents the query matrix.\n\\(K\\) is the key matrix.\n\\(V\\) is the value matrix.\n\\(d_k\\) is the dimensionality of the key vectors, ensuring scale invariance.\n\nLet‚Äôs first illustrate this concept with a practical example (not specifically from the context of NLP) to later circle back to its application in the transformer architecture.\nWe look at a retrieval task in which we query in a domain that has 5 attributes describing the items in it. The aforementioned ‚Äúlookup‚Äù is then implemented by calculating the dot product between the query and the transposed keys resulting in a vector of weights for each input-aspect.\nAs a simplification, we assume that all aspects can be described in binary terms. A hypothetical 1x5 query matrix (Q) represents the aspects we are querying in a 5-dimensional space, while a transposed 1x5 key matrix (K) represents the aspects of the search space. The dot product between these matrices results in a scalar that reflects the alignment or similarity between the query and the key, effectively indicating how many aspects of the query align with the search space.\n\n\n\n\n\n\n\n\n\nIf we now add a series of items we want to query for to our matrix \\(K\\), the result will be a vector representing the amount of matches, each item has with our query:\n\n\n\n\n\n\n\n\n\nThe result is a vector of scores that indicate the matches of the query per key. This principle does obviously also work for more than one query by adding more rows to our Query matrix \\(Q\\). This does result in a matrix, in which each row indicates the amount of matching keys for each query:\n\n\n\n\n\n\n\n\n\nInstead of binary indicators, the \\(Q\\) and \\(K\\) matrices in the attention mechanism are filled with floats. This does still result in the same kind of matched-key-result, although the results are now more like degrees of relevance instead of absolute matches:\n\\[\nQ \\times K^T =\n\\]\n\n\n\n\n\n\n\n\n\nAs you can already see in this small example, the values of individual cells can get relatively high compared to the rest of the matrix. As you remember - we want to use this product to rank our values. If these numbers are too large, it might lead to numerical instability or incorrect results. To address this issue, we will scale down the dot-product by dividing it with \\(\\sqrt{d_n}\\), where \\(d_n\\) is the dimension of the aspect space (in our case 5).\n\\[\n\\frac{Q \\times K^T}{\\sqrt{d_n}} =\n\\]\n\n\n\n\n\n\n\n\n\nSince we want to use this matrix for filtering our dataset, we would prefer the weights to sum up to one. To achieve that, we will apply a softmax function on each row of the matrix (remember that the rows currently represent the key-weighted aspects for each query). The resulting matrix with scaled weights for each aspect is then multiplied with the value-matrix that contains one datapoint in each row, described by 5 aspects along the columns.\n\\[\n\\text{softmax}(\\frac{Q \\times K^T}{\\sqrt{d_n}}) \\times V =\n\\]\n\n\n\n\n\n\n\n\n\nThe result is now an attention matrix in the sense that it tells us the importance of each value‚Äôs aspect for our query. In the specific example, the forth value seems to be the most important aspect for our third query. The crucial advantage is, that all aspects of all queries can be simultaneously compared with all aspects of all values without the necessity of sequential processing.\nThough this general idea of weighting aspects in the sense of self-attention3 to process a sequence without disadvantages of the distances of the items was used before (Bahdanau, 2014), the major contribution of the paper was the complete reliance on this mechanism without the need of LSTM/RNN parts. That their suggested architecture works is in part due to the utilisation of multiple self-attention layers, each learning its own weights for \\(Q\\), \\(K\\) and \\(V\\). This allows the model to learn more complex patterns and dependencies between words in a sentence. You can think of it as allowing the model to focus on different parts of the input sequence at different stages of processing. The outputs of the multiple heads are then concatenated and linearly transformed into the final output representation using a series of fully connected feed-forward layers.\n3¬†self in the sense of the model weighting its own embeddings, queries, keys and valuesThis small example is already pretty close to the general attention-mechanism described by Vaswani et al. (2023) (see also Figure¬†3.6), though the actual language model learns its own weights for \\(Q\\), \\(K\\) and \\(V\\).\n\n\n\n\n\n\nFig¬†3.6: Multi-headed attention as depicted in Vaswani et al. (2023)\n\n\n\nInstead of 5x5 matrices, the attenion mechanism as described in the paper implements \\(d_n \\times d_c\\)4 matrices, where \\(d_n\\) is the dimension of the embedding space5 and \\(d_c\\) is the size of the context window. In the original paper, Vaswani et al. (2023) implement the context-window as the same size as the embedding space (i.e., \\(d_n = d_c\\)). In Figure¬†3.7 you can see a brilliant illustration of the multiheaded-attention mechanism at work.\n4¬†\\(\\frac{d_n}{h} \\times \\frac{d_c}{h}\\) actually, the paper used feed-forward layers to reduce the dimensionality of each attention header to reduce the computational cost.5¬†I.e., the dimensionality used to represent each word‚Äôs meaning. In the previous toy-example illustrating the concept of embeddings (Figure¬†3.4), this would be the width of the hidden layer (8). In the case of transformers, this is usually 512 or 1024. These embeddings are learned during training and are a simple transformation of the one-hot vectors returned by the models tokenizer.\n\n\n\n\n\nFig¬†3.7: Illustration of the multi-headed attention mechanism. Taken from Hussain et al. (2024)\n\n\n\nThe implementation of the multi-headed attention mechanism allowed to solve all major issues of the language modelling approaches of the previous generation6. It firstly allows the input of a whole text-sequence at once, rendering the training and inference far speedier then the recursive approaches. Furthermore, the multi-head attention mechanism allows the model to focus on different parts of the input sequence simultaneously, enabling it to capture more complex relationships between words and improve its understanding of context without losing information about long-term dependencies. This mechanism also implicitly solves the bidirectionality-issue since each word can be taken into account when processing every other word in the sequence.\n6¬†Well, kind of. Transformers are far superior language models due to their ability to parallely process long sequences without issues with stretched context - these advantages come at a price though. GPT-3s training is estimated to have emitted around 502 metric tons of carbon (AIAAIC - ChatGPT training emits 502 metric tons of carbon, n.d.). The computational cost of the architecture as described here does additionally scale quadratically with context window size.The description until now omitted one final but key detail - we only spoke about the weight matrices \\(Q\\), \\(K\\) and \\(V\\). Each of these weight matrices are actually the product of the learned weights and the input vectors. In other words, each of the three matrices is calculated as follows:\n\\[\n\\begin{array}{lcl}\n    Q  &=& XW_Q \\\\\n    K  &=& XW_k \\\\\n    V  &=& XW_v\n\\end{array}\n\\]\nwhere \\(W_{Q, k, v}\\) are the learned weight matrices and \\(X\\) is the input matrix. This input matrix consists of a) the learned embeddings of the tokenized input-parts and b) the added, so called positional encoding.7\n7¬†While we are talking about omitted details, the whole architecture implements its layers as residual layers. This means that the output of each layer is added to the input of the layer before, before it is passed on to the next layer. But this detail is irrelevant for our understanding of the central mechanism.The positional encoding is a vector that encodes the position of each token in the input sequence. It is added to the embedding of each token to provide the model with information about the order of the tokens in the sequence. The positional encoding is calculated as follows:\n\\[\n\\begin{array}{lcl}\nPE_{(pos, 2i)} &=& sin(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}) \\\\\nPE_{(pos, 2i+1)} &=& cos(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})\n\\end{array}\n\\]\nWhere \\(i\\) is the dimension and \\(pos\\) is the position. Those 2 formulas are not the most intuitive, what they do is to add a unique offset to each embedding though, that allows the model to infer and weigh the token‚Äôs positions in the matrix on it‚Äôs own. Figure¬†3.8 illustrates the pattern this specific combination of sin and cos creates for each sequence-position and embedding-dimension.\n\n\n\n\n\n\n\n\nFig¬†3.8: The positional encoding for 50 dimensions and 512 embedding-dimensions. The x-axis represents the position and the y-axis represents the dimension. The color represents the value of the encoding.\n\n\n\n\n\nThese parts alltogether are all building-blocks of the basic transformer architecture. As you can see in Figure¬†3.9, all parts depicted by Vaswani et al. (2023) are parts we have discussed until now.\n\n\n\n\n\n\nFig¬†3.9: The transformer architecture as depicted in Vaswani et al. (2023)\n\n\n\nThe Encoder half uses the embedding -&gt; encoding -&gt; multi-headed-attention -&gt; feed-forward structure to create a semantic representation of the sequence. The Decoder half uses the same structure, but with an additional masked multi-head attention layer to prevent the model from looking at future tokens. This is necessary because we want to generate a sequence token by token.\nThe architecture described by Vaswani et al. (2023) implements both an encoder and a decoder half. In practice, many modern language models do not use this full architecture. Instead, they implement either encoder-only or decoder-only variants, each optimized for different tasks.\nEncoder-only models like BERT (Devlin et al., 2019) use bidirectional attention across the entire input sequence. Since they can attend to all tokens simultaneously, they excel at tasks requiring deep understanding of complete contexts - such as text classification, named entity recognition, or semantic similarity. These models cannot generate text sequentially since they process all tokens at once.\nDecoder-only models like GPT (Radford et al., 2018) use masked attention to prevent tokens from attending to future positions. This unidirectional constraint makes them naturally suited for autoregressive generation, where each token is predicted based only on preceding context. While this limits their ability to leverage future context, it enables them to generate coherent text token by token. Encoder-decoder models retain both components and are primarily used for sequence-to-sequence tasks like translation or summarization, where the encoder processes the input and the decoder generates the output.\nThe choice between these architectures represents a fundamental trade-off between understanding and generation capabilities. For an interactive visualization of a decoder-only architecture in action, this visualization by Brendan Bycroft provides an excellent walkthrough of how tokens flow through each layer during inference.\nFigure¬†3.10, taken from Kaplan et al. (2020), shows the test performance of Transformer models compared to LSTM-based models as a function of model size and context length. Transformers outperform LSTMs with increasing context length.\n\n\n\n\n\n\nFig¬†3.10: Comparison of Transformer- and LSTM-performance based on Model size and context length. Taken from Kaplan et al. (2020)\n\n\n\nFurthermore, Kaplan et al. (2020) and Hoffmann et al. (2022) after them postulated performace power-laws (see also Figure¬†3.11) that suggest that the performance of a Transformer directly scales with the models size and data availability. Though the task of prediction of natural language poses a non-zero limit to the performance, it is suggested that this limit is not reached for any of the currently available models.8\n8¬†Incidentally, we might run out of data to train on before reaching that limit (Villalobos et al., 2024).\n\n\n\n\n\nFig¬†3.11: Performance power law for transformer models. Taken from Kaplan et al. (2020)\n\n\n\nThe advances made through leveraging transformer-based architectures for language modelling led to a family of general-purpose language models. Unlike the approaches before, these models were not trained for a specific task but rather on a general text base with the intention of allowing specific fine-tuning to adapt to a task. Classic examples of these early general-purpose natural language generating Transformer models are the Generative Pre-trained Transformer (the predecessor of ChatGPT you all know), first described in Radford et al. (2018), and the ‚ÄúBidirectional Encoder Representations from Transformers‚Äù (BERT) architecture and training procedure, described by Devlin et al. (2019).\nThis general-purpose architecture is the base of modern LLMs as we know them today and most applications we will discuss in this course.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Getting started with (L)LMs</span>"
    ]
  },
  {
    "objectID": "content/getting_started_with_llms.html#choosing-open-source-models",
    "href": "content/getting_started_with_llms.html#choosing-open-source-models",
    "title": "Getting started with (L)LMs",
    "section": "Choosing open source models",
    "text": "Choosing open source models\nThe 2023 release of ChatGPT by OpenAI has sparked a lot of interest in large language models (LLMs) and their capabilities. This has also led to an increase in the number of available open-source LLMs. The selection of a model for your application is always a trade-off between performance, size, and computational requirements.\nAlthough Kaplan et al. (2020) showed a relationship between performance and model-size, the resources available will most probably limit you to smaller models. Additionally, a lot of tasks can be solved by smaller models if they are appropriately fine-tuned (Hsieh et al., 2023).\nA good idea when choosing an open source model is to start small and test whether the performace is sufficient for your use case. If not, you can always try a larger model later on.\nAdditionally, it is good practice to check the license of the model you want to use. Some models are only available under a non-commercial license, which means that you cannot use them for commercial purposes.\nThirdly, you should make sure that the model you choose is appropriate for your use case. For example, if you want to use a model for text generation, you should make sure that it was trained on a dataset that is similar to the data you will be using. If you want to use a model for translation, you should make sure that it was trained on a dataset that includes the languages you are interested in. A lot of usecases do already have benchmark datasets that can be used to pit models against each other and evaluate there appropriateness for a given use case based on a few key metrics.\nA good starting point for getting an overview about such metrics and benchmarks is Hugging Face. This platform has long cemented itself as the go-to place for getting access to open source models, but also provides a lot of resources for evaluating and comparing them. This page provides an overview of benchmarks, leaderboards and comparisons for a variety of tasks.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Getting started with (L)LMs</span>"
    ]
  },
  {
    "objectID": "content/getting_started_with_llms.html#basics-of-using-open-source-models",
    "href": "content/getting_started_with_llms.html#basics-of-using-open-source-models",
    "title": "Getting started with (L)LMs",
    "section": "Basics of using open source models",
    "text": "Basics of using open source models\n\n\n\n\n\n\nNoteüìù Task\n\n\n\nNow it is your turn! In your project-groups, you will each have to build a small application that uses an open source model to generate code.\n\nChoose a small model (&lt; 3B parameters) using the sources we discussed before.\nEach group is to use one of the following frameworks from python to load and use the model:\n\nLM-Studio\nOllama\nHuggingface\nVLLM\n\n\nAll APIs above use the de facto standard of the OpenAI API scheme. This scheme presents multiple POST-endpoints, of which we will mostly use the chat-completion.\nYour task is to prompt the model to generate a Python program that prints ‚ÄúHello World‚Äù to the console. Use the following prompt:\nWrite a Python program that prints \"Hello World\" to the console. \nProvide only the code without any explanations.\nYou can either directly call the API using python requests:\nimport os\nimport requests\nimport json\n\nresponse = requests.post(\"http://&lt;your API endpoint&gt;:&lt;port&gt;/v1/chat/completions\",\njson = {\n    \"model\": \"qwen2.5-coder-1.5b\",\n    \"messages\":[\n        {\n            \"role\": \"user\",\n            \"content\": \"Write a Python program that prints \\\"Hello World\\\" to the console. Provide only the code without any explanations.\"\n        }\n    ]})\n\nresult = json.loads(response.content.decode())[\"choices\"][0][\"message\"][\"content\"]\nprint(result)\nOr you can call it using the wrapper in the OpenAI-python-module:\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key='lm-studio',  \n    base_url=\"http://&lt;your API endpoint&gt;:&lt;port&gt;/v1\"\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Write a Python program that prints \\\"Hello World\\\" to the console. Provide only the code without any explanations.\"\n        }\n    ],\n    model=\"qwen2.5-coder-1.5b\",\n)\n\nprint(chat_completion.choices[0].message.content)\n\nTest whether the generated code executes correctly.\nPresent your results and your experiences with the frameworks to the course. Cover the following aspects:\n\nModel selection rationale\nSetup process and challenges\nGenerated code quality\nFramework usability\n\nSubmit your code and a brief report less than a page on moodle.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Getting started with (L)LMs</span>"
    ]
  },
  {
    "objectID": "content/getting_started_with_llms.html#further-readings",
    "href": "content/getting_started_with_llms.html#further-readings",
    "title": "Getting started with (L)LMs",
    "section": "Further Readings",
    "text": "Further Readings\n\nThis quite high-level blog-article about foundational models by Heidloff (2023)\nThe Attention is all you need-paper (Vaswani et al., 2023) and the brilliant video discussing it by Umar Jamil (Umar Jamil, 2023)\nThis very good answer on stack exchange that explains the attention-concept ((https://stats.stackexchange.com/users/95569/dontloo), n.d.)",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Getting started with (L)LMs</span>"
    ]
  },
  {
    "objectID": "content/getting_started_with_llms.html#references",
    "href": "content/getting_started_with_llms.html#references",
    "title": "Getting started with (L)LMs",
    "section": "References",
    "text": "References\n\n\n\n\nAIAAIC - ChatGPT training emits 502 metric tons of carbon. (n.d.). https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chatgpt-training-emits-502-metric-tons-of-carbon.\n\n\nBahdanau, D. (2014). Neural machine translation by jointly learning to align and translate. arXiv Preprint arXiv:1409.0473. https://arxiv.org/abs/1409.0473\n\n\nBojanowski, P., Grave, E., Joulin, A., & Mikolov, T. (2017). Enriching Word Vectors with Subword Information (arXiv:1607.04606). arXiv. https://doi.org/10.48550/arXiv.1607.04606\n\n\nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (arXiv:1810.04805). arXiv. https://doi.org/10.48550/arXiv.1810.04805\n\n\nHeidloff, N. (2023). Foundation Models, Transformers, BERT and GPT. In Niklas Heidloff. https://heidloff.net/article/foundation-models-transformers-bert-and-gpt/.\n\n\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. de L., Hendricks, L. A., Welbl, J., Clark, A., Hennigan, T., Noland, E., Millican, K., Driessche, G. van den, Damoc, B., Guy, A., Osindero, S., Simonyan, K., Elsen, E., ‚Ä¶ Sifre, L. (2022). Training Compute-Optimal Large Language Models (arXiv:2203.15556). arXiv. https://doi.org/10.48550/arXiv.2203.15556\n\n\nHsieh, C.-Y., Li, C.-L., Yeh, C.-K., Nakhost, H., Fujii, Y., Ratner, A., Krishna, R., Lee, C.-Y., & Pfister, T. (2023). Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes (arXiv:2305.02301). arXiv. https://doi.org/10.48550/arXiv.2305.02301\n\n\n(https://stats.stackexchange.com/users/95569/dontloo), dontloo. (n.d.). What exactly are keys, queries, and values in attention mechanisms? Cross Validated.\n\n\nHussain, Z., Binz, M., Mata, R., & Wulff, D. U. (2024). A tutorial on open-source large language models for behavioral science. Behavior Research Methods, 56(8), 8214‚Äì8237. https://doi.org/10.3758/s13428-024-02455-8\n\n\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., & Amodei, D. (2020). Scaling Laws for Neural Language Models (arXiv:2001.08361). arXiv. https://doi.org/10.48550/arXiv.2001.08361\n\n\nMikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space (arXiv:1301.3781). arXiv. https://doi.org/10.48550/arXiv.1301.3781\n\n\nPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., & Zettlemoyer, L. (2018). Deep contextualized word representations (arXiv:1802.05365). arXiv. https://doi.org/10.48550/arXiv.1802.05365\n\n\nRadford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). Improving language understanding with unsupervised learning.\n\n\nUmar Jamil. (2023). Attention is all you need (Transformer) - Model explanation (including math), Inference and Training.\n\n\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2023). Attention Is All You Need (arXiv:1706.03762). arXiv. https://doi.org/10.48550/arXiv.1706.03762\n\n\nVillalobos, P., Ho, A., Sevilla, J., Besiroglu, T., Heim, L., & Hobbhahn, M. (2024, June). Position: Will we run out of data? Limits of LLM scaling based on human-generated data. Forty-First International Conference on Machine Learning.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Getting started with (L)LMs</span>"
    ]
  },
  {
    "objectID": "content/prompting.html",
    "href": "content/prompting.html",
    "title": "Prompting",
    "section": "",
    "text": "Instruct-tuned models\nPrompting describes the utilization of the ability of language models to use zero or few-shot instrutions to perform a task. This ability, which we briefly touched on when we were discussing the history of language models (i.e., the paper by Radford et al. (2019)), is one of the most important aspects of modern large language models.\nPrompting can be used for various tasks such as text generation, summarization, question answering, and many more.\nInstruct-tuned models are trained on a dataset (for an example, see Figure¬†4.1) that consists of instructions and their corresponding outputs, seperated by special tokens. This is different from the pretraining phase of language models where they are trained on large amounts of text data without any specific task in mind. The goal of instruct-tuning is to make the model better at following instructions and generating more accurate and relevant outputs.\nThese finetuning-datasets are formatted into a specific structure, usually in the form of a chat template. As you can see in the following quite simple example from the SmolLM2-Huggingface-repo, the messages are separated by special tokens and divided into system-message and messages indicated by the relevant role:\nThere are usually three types of messages used in the context of instruction tuning and the usage of instruct models:",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Prompting</span>"
    ]
  },
  {
    "objectID": "content/prompting.html#sec-instruct",
    "href": "content/prompting.html#sec-instruct",
    "title": "Prompting",
    "section": "",
    "text": "Fig¬†4.1: An example for a dataset that can be used for instruct-finetuning. This dataset can be found on huggingface\n\n\n\n\n{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '&lt;|im_start|&gt;system\nYou are a helpful AI assistant named SmolLM, trained by Hugging Face&lt;|im_end|&gt;\n' }}{% endif %}{{'&lt;|im_start|&gt;' + message['role'] + '\n' + message['content'] + '&lt;|im_end|&gt;' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '&lt;|im_start|&gt;assistant\n' }}{% endif %}\n\n\nSystem prompts which tell the model its general role and behavior.\nUser prompts, which contain the actual instructions or questions for the model to respond to.\nAssistant prompts, which are the responses generated by the model based on the user‚Äôs input, or, in terms of the training phase, the answers that the model should learn to generate.\n\n\n\n\n\n\n\nNoteüìù Task\n\n\n\nTest the difference between instruct and non-instruct-models.\nDo this by trying to get a gpt2-version (i.e., ‚ÄúQuantFactory/gpt2-xl-GGUF‚Äù) and a small Instruct-Model (i.e., ‚ÄúQwen/Qwen3-0.6B‚Äù to write a small poem about the inception of the field of language modelling.\nUse LM-Studio to test this. Do also play around with the system prompt and note the effect of changing it.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) A poem written by Qwen3 0.6B - a model with Instruct-Finetuning\n\n\n\n\n\n\n\n\n\n\n\n(b) A ‚Äúpoem‚Äù written by GPT2 - a model without Instruct-Finetuning\n\n\n\n\n\n\n\nFig¬†4.2: A poem and a ‚Äúpoem‚Äù\n\n\n\n\n\nShow answer",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Prompting</span>"
    ]
  },
  {
    "objectID": "content/prompting.html#prompting-strategies",
    "href": "content/prompting.html#prompting-strategies",
    "title": "Prompting",
    "section": "Prompting strategies",
    "text": "Prompting strategies\nThe results of a prompted call to a LM is highly dependent on the exact wording of the prompt. This is especially true for more complex tasks, where the model needs to perform multiple steps in order to solve the task. It is not for naught that the field of ‚Äúprompt engineering‚Äù has emerged. There is a veritable plethora of resources available online that discuss different strategies for prompting LMs. It has to be said though, that the strategies that work and don‚Äôt work can vary greatly between models and tasks. A bit of general advice that holds true for nearly all models though, is to\n\ndefine the task in as many small steps as possible\nto be as literal and descriptive as possible and\nto provide examples if possible.\n\nSince the quality of results is so highly dependent on the chosen model, it is good practice to test candidate strategies against each other and therefore to define a target on which the quality of results can be evaluated. One example for such a target could be a benchmark dataset that contains multiple examples of the task at hand.\n\n\n\n\n\n\nNoteüìù Task\n\n\n\n1. Test the above-mentioned prompting strategies on the MTOP Intent Dataset and evaluate the results against each other. The dataset contains instructions and labels indicating on which task the instruction was intended to prompt. Use a python script to call one of the following three models in LM-Studio for this:\n\nPhi 4 mini\nQwen3 0.6B\nLlama 3.3 1B\n\nUse the F1-score implemented in scikit learn to evaluate your results.\nSince the dataset has a whole series of labels, use the following python-snippet (or your own approach) to extract only examples using the ‚ÄúGET_MESSAGE‚Äù-label:\nimport json\ndata = []\nwith open('data/de_test.jsonl', 'r') as f:\n    for line in f:\n        data.append(json.loads(line))\n\npossible_labels = list(set([entry['label_text'] for entry in data]))\n\ntexts_to_classify = [\n    {'example': entry['text'],\n     'label': 'GET_MESSAGE' if entry['label_text'] == 'GET_MESSAGE' \n              else 'OTHER'} for entry in data\n]\n2. You do sometimes read very specific tips on how to improve your results. Here are three, that you can find from time to time:\n\nDo promise rewards (i.e., monetary tips) instead of threatening punishments\nDo formulate using affirmation (‚ÄúDo the task‚Äù) instead of negating behaviours to be avoided (‚ÄúDon‚Äôt do this mistake‚Äù)\nLet the model reason about the problem before giving an answer\n\nCheck these strategies on whether they improve your results. If your first instruction already results in near-perfect classification, brainstorm a difficult task that you can validate qualitatively. Let the model write a recipe or describe Kiel for example.\n3. Present your results\n3. Upload your code to moodle",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Prompting</span>"
    ]
  },
  {
    "objectID": "content/prompting.html#generation-of-synthetic-texts",
    "href": "content/prompting.html#generation-of-synthetic-texts",
    "title": "Prompting",
    "section": "Generation of synthetic texts",
    "text": "Generation of synthetic texts\nAs we discussed before, small models can perform on an acceptable level, if they are finetuned appropriately.\nA good way to do this is to use a larger model to generate synthetic data that you then use for training the smaller model. This approach, sometimes called ‚Äúdistillation‚Äù (Xu et al., 2024) has been used successfully in many applications, for example for improving graph-database queries (Zhong et al., 2024), for improving dataset search (Silva & Barbosa, 2024) or the generation of spreadsheet-formulas (Singh et al., 2024).\nSince even the largest LLMs are not perfect in general and might be even worse on some specific niche tasks, evidence suggests that a validation strategy for data generated in this way is beneficial (Kumar et al., 2024; Singh et al., 2024).\nStrategies to validate the synthetic data include:\n\nUsing a human annotator to label part of the data to test the models output\nForcing the model to answer in a structured way that is automatically testable (e.g., by using JSON - see Tip¬†4.1 for an example on how to generate structured output using an API that follows the OpenAI-API-scheme.)\nForcing the model to return 2 or more answers and checking for consistency\nCombining the two approaches above (i.e., forcing the model to return multiple structured outputs (JSON, XML, YAML, ‚Ä¶) and checking for consistency)\nUsing a second LLM/different prompt to rate the answers\n\n\n\n\n\n\n\nTip¬†4.1: Structured Output\n\n\n\n\n\nThere are ways on forcing a language model to only generate output conforming to a specific format. We have already seen one in the examples of models being instruct-tuned to conform to a given chat-template. Another often used method is to use regular expressions to set the probabilities of the next token to be zero if it would not conform to a given (JSON-)scheme. The nicest way to use this feature in python is to define a pydantic-dataclass that defines the possible output formats and describes the expected field content to the model:\nimport os\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\n\nclass ClassificationResult(BaseModel):\n    description: str = Field(description=\"The description of the classification result.\")\n    label: str = Field(pattern=r'^(GET_MESSAGE|OTHER)$', description=\"The classified label of the text.\")\n\n\n\nclient = OpenAI(\n    api_key='lm-studio',  \n    base_url=\"http://localhost:1234/v1\"\n)\n\ntest_dict = {\"example\": \"This is a test\", \"example2\": \"This is another test\"}\ndef classify_one_sample(sample):\n    chat_completion = client.beta.chat.completions.parse(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"Classify the label of the following text: '{sample}' as 'GET_MESSAGE', 'OTHER'. /no_think {json_string}\"\n            }\n        ],\n        model=\"qwen3-0.6B\",\n        response_format=ClassificationResult\n    )\n    return chat_completion.choices[0].message.content\n\nclassify_one_sample(\"Gib mir die Nachricht von Peter aus!\")\nThe descriptions are the internally added as context to the prompt and the output is limited to the given data-range. For more possibilities on setting constraints see the pydantic-docs, for a more detailed explanation on how the token limitations work, see this article by outlined.\n\n\n\n\n\n\n\n\n\nNoteüìù Task\n\n\n\nUsing your script for batch-testing different prompts, generate synthetic data for a emotion detection task based on Paul Ekman‚Äôs six basic emotions: anger, disgust, fear, happiness, sadness and surprise1.\nThe generated data should consist of a sentence and the emotion that is expressed in it. Start by generating two examples for each emotion. Validate these results and adapt them if necessary. Then use these examples to generate 10 samples for each emotion.\nUse one of the above mentioned (non-manual) strategies to validate the data you generated.\nUpload your results to Moodle.\n\n\n1¬†Though this nomenclature has fallen a bit out of fashion",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Prompting</span>"
    ]
  },
  {
    "objectID": "content/prompting.html#temperature",
    "href": "content/prompting.html#temperature",
    "title": "Prompting",
    "section": "Temperature",
    "text": "Temperature\nYou might have encountered eerily similar answers from the language model, especially in the last task. Talking of it - why does the model return different answers to the same prompt at all if we do use pretrained-models in the first place? Shouldn‚Äôt the utilization of the frozen weight-matrix result in the same answer, every time we run the model with the same input?\nYes, it should. And it does.\nRemember that a language model trained on language generation as we discussed in the first session ends in a softmax-layer that returns probabilities for each token in the vocabulary. The generation-pipeline does not just use the token with the highest probability though, but samples from this distribution. This means, that even if the input is identical, the output will be different every time you run the model.\nThe temperature parameter controls the steepness of the softmax-function and thus the randomness of the sampling process. A higher temperature value results in more random outputs, while a lower temperature value results in more ‚Äúdeterministic‚Äù outputs. The temperatur, indicated as a float between 0 and 12, is used to modulate the probabilities of the next token. This is done by adding a \\(\\frac{1}{Temp}\\) factor to the model-outputs before applying the softmax.\n2¬†Depending on the implementation, temperatures above 1 are also allowed. Temperatures above 1 are resultsing in strange behaviours - see Figure¬†4.3.This effectively changes the Sofmax-fomula from\n\\[\np_{Token} = \\frac{e^{z_{Token}}}{\\sum_{i=1}^k e^{z_{i}}}\n\\]\nto \\[\np_{Token}(Temp) = \\frac{e^{\\frac{z_{Token}}{Temp}}}{\\sum_{i=1}^k e ^{\\frac{z_{i}}{Temp}}}\n\\]\nWhere\n\n\\(z_{Token}\\) is the output for a given token\n\\(k\\) is the size of the vocabulary\n\\(Temp\\) is the temperature parameter (0 &lt; \\(Temp\\) &lt;= 1)\n\nThe effect of this temperature can be seen in Figure¬†4.3.\n\n\n\n\n\n\n\n\nFig¬†4.3: The effect of the temperature parameter on the softmax-output for a given input. The x-axis represents the temperature, the y-axis represents the token-position and the color represents the probability of the token.\n\n\n\n\n\nMost generation-frameworks do additionally provide a parameter called top_k or top_p. These parameters are used to limit the number of tokens that can be selected as the next token. This is done by sorting the probabilities in descending order and only considering the top k tokens or the top p percent of tokens.\nTemperature is the mayor setting to control a LLMs ‚Äúcreativity‚Äù though.\n\n\n\n\n\n\nNoteüìù Task\n\n\n\nUsing the script provided for generating synthetic data, test the effect of the temperature parameter on the output of the model.\n\nUse the same prompt and the same model\nRun the model with a temperature value of 0.1, 0.5, 1.0 and 2.0",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Prompting</span>"
    ]
  },
  {
    "objectID": "content/prompting.html#understanding-and-mitigating-hallucinations",
    "href": "content/prompting.html#understanding-and-mitigating-hallucinations",
    "title": "Prompting",
    "section": "Understanding and Mitigating Hallucinations",
    "text": "Understanding and Mitigating Hallucinations\nWhile temperature controls the randomness of model outputs, a more fundamental challenge emerges when language models generate plausible yet incorrect information - a phenomenon known as hallucination.\n\nWhat are Hallucinations?\nHallucinations occur when language models produce confident, plausible-sounding outputs that are factually incorrect or unsupported by their training data. These aren‚Äôt random errors - they‚Äôre systematic failures that arise from the statistical nature of language modeling itself.\nConsider this example: When asked ‚ÄúWhat is Adam Tauman Kalai‚Äôs birthday?‚Äù, state-of-the-art models confidently produce different incorrect dates across multiple attempts, even when explicitly asked to respond only if they know the answer (Kalai et al., 2025).\n\n\nWhy Hallucinations Occur: A Statistical Perspective\nKalai et al. (2025) demonstrate that hallucinations emerge from the fundamental objective of language model training. They show that generating valid outputs is inherently harder than classifying output validity - a task where errors are well-understood in machine learning.\nThe key insight: even with perfect training data, the cross-entropy objective used in pretraining naturally leads to errors on certain types of facts. Specifically:\nArbitrary Facts: Information without learnable patterns (like birthdays of obscure individuals) will be hallucinated at rates approximately equal to the fraction of such facts appearing exactly once in training data. If 20% of birthday facts appear only once, expect ~20% hallucination rate on birthdays.\nPoor Models: When model architectures cannot adequately represent certain patterns, systematic errors emerge. For example, models using only token-based representations struggle with character-level tasks like counting letters in ‚ÄúDEEPSEEK‚Äù.\n\n\nHallucinations as Compression Failures\nChlon et al. (2025) provide a complementary information-theoretic perspective. They show that transformers minimize expected conditional description length over input orderings rather than the permutation-invariant description length. This makes them ‚ÄúBayesian in expectation, not in realization.‚Äù\nTheir framework introduces practical metrics for predicting hallucinations:\n\nInformation Sufficiency Ratio (ISR): The ratio of available information to required information for a target reliability threshold\nBits-to-Trust (B2T): The amount of information needed to achieve a specific confidence level\n\nA key finding: hallucinations decrease by approximately 0.13 per additional nat of information, making the phenomenon quantitatively predictable rather than mysterious.\n\n\nWhy Hallucinations Persist After Training\nBeyond pretraining, Kalai et al. (2025) argue that post-training and evaluation procedures actively reinforce hallucinations. Most benchmarks use binary grading (correct/incorrect) with no credit for expressing uncertainty. This creates an ‚Äúepidemic‚Äù of penalizing honest uncertainty - models that guess when unsure outperform those that appropriately abstain.\nConsider two models:\n\nModel A: Accurately signals uncertainty, never hallucinates\nModel B: Same as A but guesses instead of expressing uncertainty\n\nModel B will outperform A on most current benchmarks, despite being less trustworthy.\n\n\n\n\n\n\nNoteüìù Task\n\n\n\nTest hallucination behavior on a small model using LM-Studio:\n\nUse a small model (Qwen3-0.6B or similar) to answer 10 factual questions about rare entities\nFor each question, generate 3 responses with the same temperature\nDocument:\n\nHow often does the model give confident but incorrect answers?\nHow often does it appropriately express uncertainty?\nHow does response consistency relate to likely correctness?\n\nNow modify the prompt to explicitly encourage uncertainty expression (e.g., ‚ÄúOnly answer if you‚Äôre very confident, otherwise say you don‚Äôt know‚Äù)\nCompare the results\n\nUpload your observations to Moodle.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Prompting</span>"
    ]
  },
  {
    "objectID": "content/prompting.html#further-readings",
    "href": "content/prompting.html#further-readings",
    "title": "Prompting",
    "section": "Further Readings",
    "text": "Further Readings\n\nThis prompting-guide has some nice general advice\nOpenAI has its own set of tipps\ndeepset, the company behind Haystack, has a nice guide as well\nThis blog-article, again written by Heidloff (Heidloff, 2023)",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Prompting</span>"
    ]
  },
  {
    "objectID": "content/prompting.html#references",
    "href": "content/prompting.html#references",
    "title": "Prompting",
    "section": "References",
    "text": "References\n\n\n\n\nChlon, L., Karim, A., & Chlon, M. (2025). Predictable Compression Failures: Why Language Models Actually Hallucinate (arXiv:2509.11208). arXiv. https://doi.org/10.48550/arXiv.2509.11208\n\n\nHeidloff, N. (2023). Fine-tuning small LLMs with Output from large LLMs. In Niklas Heidloff. https://heidloff.net/article/fine-tune-small-llm-with-big-llm/.\n\n\nKalai, A. T., Nachum, O., Vempala, S. S., & Zhang, E. (2025). Why Language Models Hallucinate (arXiv:2509.04664). arXiv. https://doi.org/10.48550/arXiv.2509.04664\n\n\nKumar, B., Amar, J., Yang, E., Li, N., & Jia, Y. (2024). Selective Fine-tuning on LLM-labeled Data May Reduce Reliance on Human Annotation: A Case Study Using Schedule-of-Event Table Detection (arXiv:2405.06093). arXiv. https://doi.org/10.48550/arXiv.2405.06093\n\n\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language models are unsupervised multitask learners. OpenAI Blog, 1(8), 9.\n\n\nSilva, L., & Barbosa, L. (2024). Improving dense retrieval models with LLM augmented data for dataset search. Knowledge-Based Systems, 294, 111740. https://doi.org/10.1016/j.knosys.2024.111740\n\n\nSingh, U., Cambronero, J., Gulwani, S., Kanade, A., Khatry, A., Le, V., Singh, M., & Verbruggen, G. (2024). An Empirical Study of Validating Synthetic Data for Formula Generation (arXiv:2407.10657). arXiv. https://doi.org/10.48550/arXiv.2407.10657\n\n\nXu, X., Li, M., Tao, C., Shen, T., Cheng, R., Li, J., Xu, C., Tao, D., & Zhou, T. (2024). A Survey on Knowledge Distillation of Large Language Models (arXiv:2402.13116). arXiv. https://doi.org/10.48550/arXiv.2402.13116\n\n\nZhong, Z., Zhong, L., Sun, Z., Jin, Q., Qin, Z., & Zhang, X. (2024). SyntheT2C: Generating Synthetic Data for Fine-Tuning Large Language Models on the Text2Cypher Task (arXiv:2406.10710). arXiv. https://doi.org/10.48550/arXiv.2406.10710",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Prompting</span>"
    ]
  },
  {
    "objectID": "content/function_calling.html",
    "href": "content/function_calling.html",
    "title": "Function Calling",
    "section": "",
    "text": "Motivation\nSometimes you want your AI assistant to answer more complex tasks then the LLM can handle out of the box. For example, you might want to ask questions regarding documents or datasets on your hard drive, or ask about recent events. This is a hard task for an LLM, because, if it was not trained on this information, i.e.¬†the information wasn‚Äôt part of its training data, it won‚Äôt be able to answer these questions correctly.\nTo solve this problem, we need to give the LLM some tools it can use to access this additional information. This is where function calling comes into play. Function calling allows the LLM to call a function with specific parameters to get the required information. The function then returns the requested data or performs the necessary task, and the LLM can continue generating its response based on that information.\nA basic workflow may look like this:\nNote: As stated above, the functions need to be predefined. In theory, we could just give the LLM its task and let it generate code to be executed. This is, however not the best idea for reasons of security.\nThere are two main ways to implement function calling:\nStill the challenge is to get the LLM to generate valid output. There are two main strategies to facilitate that:",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Function Calling</span>"
    ]
  },
  {
    "objectID": "content/function_calling.html#motivation",
    "href": "content/function_calling.html#motivation",
    "title": "Function Calling",
    "section": "",
    "text": "Noteüìù Task\n\n\n\nTry it!\n\nOpen a notebook and connect to a local LLM using LM Studio.\nAsk the LLM about the current weather in your location.\n\n(I mean, sure, you could just look out the window, but we are developers here, we don‚Äôt have windows!)\n\n\n\n\n\nThe user inputs a question or task.\nThe LLM determines if it needs additional information or assistance from external tools.\nIf needed, the LLM calls a function with specific parameters to retrieve the required data or perform the necessary task. This means the LLM generates an answer containing either some executable code or a JSON object containing the name of the function and its parameters. These functions have to be defined in advance.\nThe LLM response is then scanned for these elements (code or JSON object) and these are executed if possible.\nThe response is then fed back into the LLM (usually as user input) for further processing.\nThe LLM uses the returned information to continue generating its response.\n\n\n\n\n\n\nStructured output: Here, the LLM is tasked to generate a tool call in the form of a JSON object containing the name of the function and its parameters.\nCode generation: Here, we ask the LLM to generate the function calls in the form of executable python code. Usually, we still want to restrict the LLM to use only predefined functions. Nevertheless, this can pose a severe security issue because this approach hinges on running generated code on your machine.  \n\n\n\n\nusing a large, generalized LLM (e.g.¬†GPT-4) with good prompt engineering and\nusing a smaller model fine tuned to generate function calls.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Function Calling</span>"
    ]
  },
  {
    "objectID": "content/function_calling.html#structured-output",
    "href": "content/function_calling.html#structured-output",
    "title": "Function Calling",
    "section": "Structured output",
    "text": "Structured output\n\nThe traditional way1 of doing function calling is to generate a JSON object containing the name of the function and its parameters. Until recently, all major agent frameworks (more on agents next time) used this approach. Here, the LLM response is scanned for a JSON object. Function name and arguments are extracted and the function is executed, if possible.\n1¬†In this context, traditional means: people have been using it for more than a year. \nFunction definition\nThe first step in using function calling is to define the functions that the LLM can call. This is done by providing a JSON schema that describes the name of the function, its arguments and their types. The JSON schema should be provided to the LLM in the system prompt. Here is an example: 2\n2¬†Note, that this is not an executable implementation but just a description of the function for the LLM.{\n    \"name\": \"get_current_weather\",  \n    \"description\": \"Get the current weather in a given location\",  \n    \"arguments\": {    \n        \"location\": {\"type\": \"string\"},    \n        \"unit\": {\"type\": \"string\"}  \n        } \n}\nFunction name and description should be as clear as possible to make it easier for the LLM to decide which function to use and how to properly use it. Argument names and types should be as precise as possible to avoid ambiguity in the function call.\n\n\nPrompting\nThe second step is to provide a good prompt. The prompt should make it clear to the LLM to only generate valid output and that it should follow the JSON schema. Here is an example of a prompt that can be used for function calling:\nYou are a helpful assistant that generates function calls based on user input. Only use the functions you have been provided with.\n\n{function definition as described above}\n\nUser: What's the weather like in Berlin?\n\nAssistant: {\n    \"name\": \"get_current_weather\",\n    \"arguments\": {\"location\": \"Berlin\", \"unit\": \"celsius\"}\n}\nAnother way of forcing the LLM to output structured format is to use pydantic classes as described last time.\n\n\n\n\n\n\nNoteüìù Task\n\n\n\nTry it!\n\nOpen a notebook and connect to a local LLM using LM Studio.\nDefine the function get_current_weather as shown above.\nWrite a prompt that asks the LLM to generate a function call based on user input. Use prompt engineering as shown above or pydantic classes as shown last time.\nTest the prompt with an example input.\nDefine other functions and try other inputs and see if the LLM generates valid output.\n\n\n\n\n\nChallenges, finetuned models and the influence of size\nThe main challenge is here to get the LLM to generate a valid answer. This is not always easy, as LLMs are not usually super safe coders üòÉ.\n\nThey can hallucinate functions or arguments that do not exist.\nThey can forget to call a function.\nThey can forget to provide all required arguments.\nThey can provide the wrong type for an argument.\nThey can provide invalid values for an argument.\n\nThere are several strategies to mitigate these issues:\n\nPrompt engineering: A good prompt can help to guide the LLM towards generating valid output. This is especially true for larger models, as they have a better understanding of the world and can therefore generate more accurate responses.\nFinetuning: Finetuning a model on a specific task can improve its performance on that task. This is especially useful for smaller models, as they are less likely to hallucinate functions or arguments that do not exist.\nSize: Larger models are better at generating valid output than smaller models. However, larger models are also more expensive to run and require more computational resources.\n\n\n\n\n\n\n\nNoteüìù Task\n\n\n\nTest it! (we can do it together, if your hardware does not allow you to run the model.)\nAs above, but this time\n\n\nuse a very small model (e.g a small Llama model)\nuse a model finetuned for the task (you could try this one)\na larger model (a larger llama in this case)",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Function Calling</span>"
    ]
  },
  {
    "objectID": "content/function_calling.html#code-generation",
    "href": "content/function_calling.html#code-generation",
    "title": "Function Calling",
    "section": "Code Generation",
    "text": "Code Generation\n\nThe exception mentioned above is the smolagents framework. Here, the default mode is code generation, but JSON mode is also supported. (We will get to know agents and the smolagents framework next time.) When using this approach, the function definition and description will be given to the LLM as python code. Additionally, the LLM is expected to generate the function call also as valid python code. As with structured output, function name and description should be as clear as possible. Typing might also help.\n\n\n\n\n\n\nNoteüìù Task\n\n\n\nTry it!\n\nIn your notebook, define the weather function (and/or some other function of your choice) in python code.\nWrite an appropriate prompt that makes it clear that you expect python code calling the defined function(s).\nTest your prompt with an example input.\n\n\n\nAs mentioned above (several times already), giving clear names and descriptions for functions, parameters, etc., will help the model generate more accurate code snippets. (PRO TIP: it will help your human coworkers as well in understanding your code.) Here, you have the opportunity to see the consequences in action in a save environment without angering fellow humans or yourself later on!\n\n\n\n\n\n\nNoteüìù Task\n\n\n\nTry it!\n\nIn your notebook, write a well written python function using clear names, description text and typing hints. You can use the one you wrote earlier, because of course you wrote clean code!\nTest the function with your prompt and example inputs.\nNow write a badly written python function, without clear names, descriptions or typing hints. Test it with your example inputs too. Are the results better or worse? Why do you think that‚Äôs happening?\nUpload your notebook to Moodle.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Function Calling</span>"
    ]
  },
  {
    "objectID": "content/function_calling.html#further-readings",
    "href": "content/function_calling.html#further-readings",
    "title": "Function Calling",
    "section": "Further Readings",
    "text": "Further Readings\n\n\nHere is a very nice paper about generating structured output.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Function Calling</span>"
    ]
  },
  {
    "objectID": "content/agent_basics.html",
    "href": "content/agent_basics.html",
    "title": "Agent basics",
    "section": "",
    "text": "What is an agent?\n‚ÄúAn AI agent is a system that uses an LLM to decide the control flow of an application.‚Äù (‚ÄúWhat Is an AI Agent?‚Äù 2024)\nAn agent (from latin agere, to act) is defined by its ability to make decisions based on the circumstances to achieve a predefined goal. In the context of large language models, agents are LLM-based systems where the control flow of an application is decided by the LLM based on its understanding of the situation at hand. Often, it also means that the agent has access to the external world i.e.¬†it has agency. In Practice, this means that the LLM decides what do do next, which tool to use (if any), which information to retrieve, etc. based on its understanding of the situation. A nice Example of such an agentic system is Vending-Bench (Backlund & Petersson, 2025), where an AI agent is tasked with managing a vending machine. The agent is responsible for deciding what products to stock, when to restock, which prices to set, etc. based on its understanding of the market demand, competition, and other factors.\nBut let‚Äôs keep it simpler for now. Let‚Äôs say you want to know:\n‚ÄúWhat were the key learnings from the Generative AI elective module in SoSe 25 at FH Kiel?‚Äù\nCould you just ask an LLM that question and expect a correct answer?\nAs stated earlier, it is in theory possible, but in reality usually not. So we need a more complex approach.\nLet‚Äôs think for a moment how a human would answer that (one that did not attend the module). We would probably try to get a copy of the script, maybe we saved the script to our hard drive or other data storage. Maybe we could search the web for a description or text version of the module. Having obtained a copy of the script, we would probably read it. Then, we would try to distill the information hidden therein, to answer the question.\nSo, for our LLM to answer that question, it needs to be able to perform several tasks:\nThis is where agents come into play. Agents are LLM-based systems that can solve complex tasks by performing several subtasks in sequence, using an LLM to decide which subtask to perform next. In our example, the agent would first search the web for relevant documents, then read and understand them, summarize them and finally answer the question based on the summary.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Agent basics</span>"
    ]
  },
  {
    "objectID": "content/agent_basics.html#what-is-an-agent",
    "href": "content/agent_basics.html#what-is-an-agent",
    "title": "Agent basics",
    "section": "",
    "text": "Vending-Bench architecture (Backlund & Petersson, 2025)\n\n\n\n\n\n\n\n\n\n\n\nSearching the web or a local file storage for relevant documents\nReading and understanding a document\nSummarizing the content of a document\nAnswering questions based on the summary of a document",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Agent basics</span>"
    ]
  },
  {
    "objectID": "content/agent_basics.html#agent-framework",
    "href": "content/agent_basics.html#agent-framework",
    "title": "Agent basics",
    "section": "Agent framework",
    "text": "Agent framework\n\n\n\nArchitecture of the agent framework (LLM Agents ‚Äì Nextra, 2024)\n\n\nTo facilitate this, an agent system consists of several components:\n\nAgent: the agent core acting as coordinator\nPlanning: Assists the agent in breaking down the complex task into subtasks\nTools: functions that the agent can use to perform a specific task\nMemory: used to store information about previous interactions with the agent\n\nWe will describe each of them below.\n\nAgent\nThis is a general-purpose LLM, that functions as the brain and main decision-making component of an agent. It determines which tools to use and how to combine their results to solve complex tasks. The agent core uses the output of the previous tool as input for the next tool. It also uses an LLM to decide when to stop using tools and return a final answer. The behavior of the agent and the tools, it has at its disposal, is defined by a prompt template.\n\n\nPlanning\nPlanning is the process of breaking down a complex task into subtasks and deciding which tools to use for each subtask. The planning module is not easily separable from the agent core. It is more like the setup that defines how the LLM should approach the problem (e.g.¬†multi-step thought process), rather than an independent component.\n\n \n\n\nTools\nTools are functions that the agent can use to perform a specific task. They can be pre-defined or dynamically generated based on the user‚Äôs needs. Tools can be simple, such as a calculator, or complex, such as a web search engine. Tools can also be other agents, allowing for the creation of multi-agent systems. In our example, the tools would be a web search engine and a document reader. Other popular tools are a data store or a python interpreter.\n\n\nMemory\nMemory is used to store information about previous interactions with the agent. This allows the agent to remember past conversations and use this information in future interactions. Memory can be short-term, such as a conversation buffer, or long-term, such as a database. Memory can also be used to store the results of previous tool uses, allowing the agent to reuse them if necessary.\n\n\n\nMulti-step agents\nThe de-facto standard for multi-step agents is ReAct introduced by Yao et al. (Yao et al., 2023). ReAct is short for Synergizing Reasoning and Acting. In this approach, the agent updates its reasoning after each step of tool use. Basically, it is running a while loop:\nFirst, the agent is initialized with a system prompt, tools and a task to solve. Then it goes through these steps:\n\nThe agent reasons about the task and selects a tool to use based on its reasoning.\nThe tool is executed. Both the tool usage and the result are recorded to memory.\nBased on the output from the tool, the agent updates its reasoning and may select another tool if needed.\nRepeat until the goal is achieved or deemed unachievable.\nThe final answer is generated based on the memory.\n\nOr, in code structure (from the smolagents site):\nmemory = [user_defined_task]\nwhile llm_should_continue(memory): # this loop is the multi-step part\n    action = llm_get_next_action(memory) # this is the tool-calling part\n    observations = execute_action(action)\n    memory += [action, observations] # this is the memory update part\nreturn llm_generate_answer(memory) # this is the final answer generation part",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Agent basics</span>"
    ]
  },
  {
    "objectID": "content/agent_basics.html#examples-of-agent-frameworks",
    "href": "content/agent_basics.html#examples-of-agent-frameworks",
    "title": "Agent basics",
    "section": "Examples of agent-frameworks",
    "text": "Examples of agent-frameworks\nThere are a lot of agent frameworks out there. In this module we will focus on two of them: LlamaIndex and smolagents. They all have their own strengths and weaknesses, but they all share the same basic architecture as described above. We will describe each of them below.\n\nLlamaindex: LlamaIndex is a data framework for building LLM applications. LLamaindex is easy to use out of the box. The documentation consists mainly of examples and tutorials that guide the user through the process of building an agent application. These can of course be combined or varied in many ways to create more complex applications.\nsmolagents: Smolagents is an agent framework for LLMs developed by the huggingface team. It is designed to be lightweight and easy to use. Smolagents is quite new but the documentation is quite good and the community is very active.\n\nWe will also give honorary mentions to two other frameworks, which are also very popular in the LLM community.\n\nLangChain: LangChain is a popular framework. It is designed to be modular and easy to use. It is designed for developers who want to build applications with LLMs but do not necessarily have a lot of time to spend on building the infrastructure.\nHaystack: Haystack is an open source NLP framework that enables you to build production-ready applications around LLMs and other models. The company behind Haystack, Deepset, is based in Germany and has a strong focus on open source. It is a good choice for production environments where you need high performance and scalability.\n\n\n\n\n\n\n\n\nNoteüìù Task\n\n\n\nNow it is your turn!\nEach group is to use one of the following frameworks to build a small demo agent:\n\nLlamaindex You can use the ReAct example as a starting point.\nSmolagents\n\n\nSet up a local LLM (e.g.¬†using Ollama or LM Studio) to be used by the agent.\nChoose a small task for your agent, e.g.¬†answering questions about a specific topic, summarizing a document, etc. (use the one in the respective tutorial)\nImplement the agent using one of the frameworks listed above.\nPresent your results and your experiences with the frameworks in a short presentation. (presenting the notebook is fine.)",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Agent basics</span>"
    ]
  },
  {
    "objectID": "content/agent_basics.html#summary-and-recap",
    "href": "content/agent_basics.html#summary-and-recap",
    "title": "Agent basics",
    "section": "Summary and recap",
    "text": "Summary and recap\n\nAgents\nAn agent is a wrapper layer, that takes the user input and pipes it to an LLM, together with a custom system prompt, that allows the LLM to answer the user request better. The agent has several modules at its disposal, the memory, some tools and a planning tool.\nThe memory function is what allows chat models to retain a memory of the past conversation with the user. This information is saved as plain text in the memory and given to the planning module (i.e.¬†the LLM) along with the system prompt and the current user input.\nThe planning module then decides which tools to use, if any, to answer the user request. The output of the planning module is a response message containing one or several tool calls (or a final answer). The agent then executes the tool calls by first parsing the response, then executing the functions. Based on the tool outputs, a final answer is generated and sent back to the user.\n\n\nReact agents\nThere a several types of agent, of which the ReAct agent is most often used. It is a type of agent that uses the ReAct framework to solve complex tasks by reasoning in multiple steps. It is based on the idea of ‚Äúthought-action-observation‚Äù loops. The LLM is given a task and it generates a thought, which is then used to decide on an action. The action is executed and the observation is fed back into the LLM. This process is repeated until the LLM decides that it has enough information to answer the question or if the maximum number of iterations is reached.\n\n\nWhy agents?\nAgents make decisions that control the flow of an application. The question is: when is this helpful? If the workflow is really simple, you basically don‚Äôt need an agent. Just run the LLM calls in a pipeline. Actually, most of the examples you find in tutorials will be better solved with a pipeline. We will use the example from the smolagents introduction to illustrate that.\nLet‚Äôs take an example: say you‚Äôre making an app that handles customer requests on a surfing trip website.\nYou could know in advance that the requests will belong to either of 2 buckets (based on user choice), and you have a predefined workflow for each of these 2 cases.\n\nWant some knowledge on the trips? ‚áí give them access to a search bar to search your knowledge base\nWants to talk to sales? ‚áí let them type in a contact form.\n\nIf that deterministic workflow fits all queries, by all means just code everything! This will give you a 100% reliable system with no risk of error introduced by letting unpredictable LLMs meddle in your workflow. For the sake of simplicity and robustness, it‚Äôs advised to regularize towards not using any agentic behaviour.\nBut what if the workflow can‚Äôt be determined that well in advance?\nFor instance, a user wants to ask: ‚ÄúI can come on Monday, but I forgot my passport so risk being delayed to Wednesday, is it possible to take me and my stuff to surf on Tuesday morning, with a cancellation insurance?‚Äù This question hinges on many factors, and probably none of the predetermined criteria above will suffice for this request.\nIf the pre-determined workflow falls short too often, that means you need more flexibility.\nThat is where an agentic setup helps.\n\n\nPros and Cons\nThere are some obvious pros and cons with using agents\nPros\n\nYou don‚Äôt have to do it yourself.\n\nSometimes they perform better than humans.\nthey are easily scalable.\n\nCons\n\nsometimes they fail spectacularly at easy tasks. See again the vending machine example (Backlund & Petersson, 2025) from earlier for some hilarious anecdotes.\n\n\n\n\nReact agents\nThere are several types of agent, of which the ReAct agent is most often used. It is a type of agent that uses the ReAct framework to solve complex tasks by reasoning in multiple steps. It is based on the idea of ‚Äúthought-action-observation‚Äù loops. The LLM is given a task and it generates a thought, which is then used to decide on an action. The action is executed and the observation is fed back into the LLM. This process is repeated until the LLM decides that it has enough information to answer the question or if the maximum number of iterations is reached.\nLet us have a closer look at the inner workings of react agents.\n\n\n\n\n\n\nNoteüìù Task\n\n\n\nLet‚Äôs have a deeper look!\n\nReopen the notebook from earlier where you defined your agents\nHave a look at the prompt, the agent gives to the LLM ( in LLamaindex you can find it using agent.get_prompts()). Try to find it in smolagents.\nDiscuss the prompt with the group! What does it do? How does it do it?\nAsk the agent to tell you the weather in a given location.\nWatch in LM Studio how the LLM called by the agent creates the thought process, function calls and the final response.\nTry to break the agent by asking stuff it cannot answer. Be creative. (On one occasion I just said ‚ÄúHi‚Äù and it went into an infinite loop because it did not need a tool for that and there wasn‚Äôt a ‚Äúnone‚Äù tool üòú.)\nUpload your notebook to Moodle",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Agent basics</span>"
    ]
  },
  {
    "objectID": "content/agent_basics.html#references",
    "href": "content/agent_basics.html#references",
    "title": "Agent basics",
    "section": "References",
    "text": "References\n\n\n\n\nBacklund, A., & Petersson, L. (2025). Vending-Bench: A Benchmark for Long-Term Coherence of Autonomous Agents (arXiv:2502.15840). arXiv. https://doi.org/10.48550/arXiv.2502.15840\n\n\nLLM Agents ‚Äì Nextra. (2024). https://www.promptingguide.ai/research/llm-agents.\n\n\nWhat is an AI agent? (2024). In LangChain Blog. https://blog.langchain.dev/what-is-an-agent/.\n\n\nYao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., & Cao, Y. (2023). ReAct: Synergizing Reasoning and Acting in Language Models (arXiv:2210.03629). arXiv. https://doi.org/10.48550/arXiv.2210.03629",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Agent basics</span>"
    ]
  },
  {
    "objectID": "content/embeddings.html",
    "href": "content/embeddings.html",
    "title": "Embedding-based LLM-systems",
    "section": "",
    "text": "Semantic embeddings and vector stores\nAll agents (and all LLM-pipelines if sufficiently complex) we discussed until here are using tools that allow them to use their generated inputs in some way. In most of the task we want to utilize LLMs, we do not only want to generate text but to also inform the generation based on some kind of existing knowledge base. Examples for these kinds of usecases include:\nThough most modern LLMs are increasingly capable in answering basic knowledge-questions, the more complex a topic or the more relevant the factual basis of an answer is, the more it is important to base generated answers on actual data.\nTo empower a LLM too look up information during its thought-process, one has to build a tool that allows a LLM to use natural language to retrieve information necessary for a task. The fundamental principle to do this are so-called semantic embeddings. These are pretty close to the concept we introduced when talking about the foundations of LLMs (see here) and can be understood as a way to map textual data into a vector space. The main idea is that semantically similar texts should have similar embeddings, i.e., they are close in the vector space. Close in this context is meant as having a reasonably small distance between them. The go-to standard to measure this distance is the cosine similarity, which has proven useful enough to be the standard for a range of semantic retrieval implementations (i.e., they are used in OpenAI tutorials and in Azure embedding-applications). The cosine similarity is defined as:\n\\[\n\\text{cosine\\_similarity}(u, v) = \\frac{u \\cdot v}{\\|u\\| \\|v\\|} = \\frac{\\sum_{i=1}^{n} u_i v_i}{\\sqrt{\\sum_{i=1}^{n} u_i^2} \\sqrt{\\sum_{i=1}^{n} v_i^2}}\n\\] The rationale here is that sequences with semantically similar contents should point to similar directions in the high dimensional vector space. See Figure¬†7.1 for an illustration of this and other common similarity concepts seen in semantic retrieval.\n(a) Illustration of ‚Äúsemantic embeddings‚Äù of different word.\n\n\n\n\n\n\n\n\n\n\n\n(b) Illustration of 4 common similarity concepts seen in semantic retrieval: cosine, euclidean, dot product and manhattan. dot product and cosine are taking the direction of the vector into account, while the cosine ignores the length of the vectors and the dot product does not. Manhattan and euclidean are both measuring the distance between two points in a vector space, but they do it differently. Euclidean is the straight line between two points, while manhattan is the sum of the absolute differences between the coordinates of the two points.\n\n\n\n\n\n\nFig¬†7.1: Illustration of common similarity metrics in semantic search.\nAs always, there is not the one solution to all problems though and the applicability of cosine similarity might not be optimal for your usecase (Goyal & Sharma, 2022; Steck et al., 2024).\nThough one could use any kind of (L)LM to calculate embeddings for this case1, it is advisable to use models specifically trained for this purpose. Reimers & Gurevych (2019) proposed Sentence-BERT which is a simple but effective approach to calculate semantic embeddings. SBERT and similar approaches are based on a (L)LM that was trained to predict missing words as we discussed before, resulting in a general representation of natural language. In the case of the original paper, they used (among others) the BERT model Devlin et al. (2019) mentioned before.\nThe authors then use this to embed a pair of sentences into one embedding-vector each2, for which some measure of semantic similarity is known. An example for a dataset containing such sentences is the Stanford Natural Language Inferenc(SNLI) corpus Bowman et al. (2015) which labels 550k pairs of sentences as either entailment, contradiction or neutral. Reimers & Gurevych (2019) then concated the both senteces embeddings and their element-wise difference into a single vector which is fed to a multiclass classifier, indicating in which category the sentences relationship falls. At inference, this classification head was removed and replaced as the cosine similarity as discussed above. The resulting network is highly effective in calculating semantic similarities between sentences.\nA look at the sbert-website shows that the module has somewhat grown and now does supply a series of learning paradigms that can be used to efficiently tune a model for your specific usecase3. As the library has grown, so has the sheer amount of pretrained embedding-models in some way based on this architecture that are hosted on huggingface. The MTEB-Leaderboard is a good start to search for a model for your application. One utilization of this model-family, which has already been implicitly used in this script, is their very efficient ability to semantically search for documents. If a model is very good at finding similar sentences, it can also be very good to find documents that are very similar to a question.\nLook at the example illustrated in Figure¬†7.2. The question ‚Äúwhy is the sky blue‚Äù embedded with the same model as our 5 documents stating some facts.\nWe can then calculate the cosine-similarity between these embeddings and return the document, that has the highest similarity to our question.\nThis approach of using a model to embed documents and questions into a vector space is the basis for the so-called Retrieval augmented generation.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Embedding-based LLM-systems</span>"
    ]
  },
  {
    "objectID": "content/embeddings.html#semantic-embeddings-and-vector-stores",
    "href": "content/embeddings.html#semantic-embeddings-and-vector-stores",
    "title": "Embedding-based LLM-systems",
    "section": "",
    "text": "1¬†And there are approaches to use LLMs to solve this taks i.e., Jiang et al. (2023)2¬†The original BERT-paper did this by adding a pooling layer before the task-header that extracted and weighed the context-dependend embedding of the first token. The SBERT paper tried different pooling-strategies and used a mean over each embedding dimension of the sequence.\n\n3¬†And this does not have to be expensive. Tunstall et al. (2022) have shown a highly efficient contrastive learning paradigm that limits the amount of necessary labels for a ridiculously small amount of labels.\n\n\n\n\n\n\nFig¬†7.2: Illustration of the usage of embedding-based distances in retrieval.\n\n\n\n\n\n\n\n\n\n\nNoteüìù Task\n\n\n\nInstall the sentence-transformer package and download the climate_fever-dataset.\nChoose one model from the MTEB-Leaderboard that you deem adequately sized and appropriate for the task\nTest the different metrics shown in Figure¬†7.1 for the first twenty claims of the dataset and a question you formulate.\nUse the similarity-implementations from sklearn.metrics.pairwise.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Embedding-based LLM-systems</span>"
    ]
  },
  {
    "objectID": "content/embeddings.html#retrieval-augmented-generation",
    "href": "content/embeddings.html#retrieval-augmented-generation",
    "title": "Embedding-based LLM-systems",
    "section": "Retrieval augmented generation",
    "text": "Retrieval augmented generation\nRetrieval augmented generation (RAG) is a framework that does pretty much do what it says on the tin. You use a retrieval model to find documents that are similar to your question and then either return these documents or feed them into a generative model, which then generates an answer based on these documents. This process can additionally be wrapped as a tool to be used by an agent, so that your existing agent can now also use external knowledge sources to answer questions.\nRetrieval does not have to be semantics-based in this context - all kinds of data sources and databases can be made accessible for a LLM - we will focus on a purely embedding based approach here though.\nAlthough the small example in the last task was working, it is not really scalable. It was fine for a limited set of examples, if you want to realistically make a whole knowledge base searchable, you need to use an appropriate database system.\n\nVector databases\nA vector database is a database that stores vectors and allows for efficient similarity searches. As can be seen in the db-engines ranking there has been a surge of interest in this area recently, with many new players entering the market. From the plethora of vector databases, these three are examples that virtue a honorary mention:\n\nChroma - a in-memory database for small applications that is especially easy to get to run.\nElasticsearch - a well established database that is the go to system for open source search engines and has recently (and kind of naturally) also branched out into vector databases.\nQdrant - the product of a Berlin-based startup that focusses on stability and scalability. It can also run in memory, but does natively support hard drive storage.\n\nThe best way to use qdrant is to use docker to run it and the python sdk to interact with it. Since version 1.1.1, the sdk also allows to just run the client in memory.\n\n\n\n\n\n\nNoteüìù Task\n\n\n\nInstall the qdrant-client python-sdk.\nCreate a collection for the claims and one for the evidence in the climate_fever-dataset. Add the first 200 entries to each of these collections. Use qdrants sentencetransformers to do this, here is a medium tutorial that leads you through the process.\nTest the similarity search on a question you formulate.\n\n\n\n\nRAG\nThe last step to make this into a RAG pipeline is to use a generative model to answer the question based on the retrieved documents.\nThis means, that we do collect the relevant documents like we did before, still based on a natural language question, but instead of returning the hits we got from the index, we feed them into a LLM and ask it to generate an answer based on these documents. This is where the name retrieval augmented generation comes from - we use the retrieval step to augment the generative model with additional information. The diagram in Figure¬†7.3 illustrates this process.\n\n\n\n\n\n\nFig¬†7.3: Illustration of a RAG-system.\n\n\n\n\n\n\n\n\n\nNoteüìù Task\n\n\n\nImplement a RAG pipeline for the climate_fever dataset using qdrant as vector database and a LLM of your choice for the summarization.\nTry to find a prompt that results in the LLM\n\nusing the information given\nnot inventing new information\nreferencing the source of the information it uses\n\nUpload your results until here (embedder, database and summarization) to moodle.\n\n\nMost agent frameworks provide integrations for a variety of vector databases.\nIn terms of llamaindex, there are not just one but two tutorials on how to get qdrant to integrate into your agent, one from qdrant for general integration and one from llamaindex.\nThe pipeline is pretty close to what we discussed until here, it just uses the llamaindex-typical wrapper classes. See Tip¬†7.1 for an example RAG-system implemented in Llamaindex.\n\n\n\n\n\n\nTip¬†7.1: Llamaindex Rag\n\n\n\n\n\nThe first thing in both the Llamaindex and the manual way of creating a retrieval pipeline is the setup of a vector database:\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import Distance, VectorParams, Batch\nDIMENSIONS = 384\nclient = QdrantClient(location=\":memory:\")\nTo store data and query the database, we have to load a embedding-model. As in the manual way of creating a retrieval pipeline discussed before, we can use a huggingface-SentenceTranformer model. But instead of using the SentenceTransformer class from the sentence_transformers library, we have to use the HuggingFaceEmbedding class from Llamaindex. This model is entered into the Llamaindex-Settings.\nfrom llama_index.core import Settings\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\nembed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L12-v2\")\nSettings.embed_model = embed_model\nThe next step is to wrap the vector-store into a Llamaindex-VectorStoreIndex. This index can be used to add our documents to the database.\nfrom llama_index.vector_stores.qdrant import QdrantVectorStore\n\nvector_store = QdrantVectorStore(client=client, collection_name=\"paper\")\nAs an example, we will add the ‚ÄúAttention is all you need‚Äù paper. This is how the head of our txt-file looks like:\n         Attention Is All You Need\narXiv:1706.03762v7 [cs.CL] 2 Aug 2023\n\n\n\n\n                                             Ashish Vaswani‚àó                Noam Shazeer‚àó               Niki Parmar‚àó             Jakob Uszkoreit‚àó\n                                              Google Brain                   Google Brain             Google Research            Google Research\n                                          avaswani@google.com             noam@google.com            nikip@google.com            usz@google.com\nSince we can not just dump the document at once, we will chunk it in sentences (more about that later). This can be done like this (ignore the parameters by now, we will look at them later):\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core import Document\n\nnode_parser = SentenceSplitter(chunk_size=100, chunk_overlap=20)\n\nnodes = node_parser.get_nodes_from_documents(\n    [Document(text=text)], show_progress=False\n)\nThese documents are then added to our database and transformed in an index llamaindex can use:\nfrom llama_index.core import VectorStoreIndex\n\nindex = VectorStoreIndex(\n    nodes=nodes,\n    vector_store=vector_store,\n)\nThis index can already be used to retrieve documents from the database (by converting it to a retriever).\nretriever = index.as_retriever(similarity_top_k=10)\nretriever.retrieve('What do the terms Key, Value and Query stand for in self-attention?')\n[NodeWithScore(node=TextNode(id_='04c12537-5f33-4d41-a4d4-df30d2aed6e4', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={&lt;NodeRelationship.SOURCE: '1'&gt;: RelatedNodeInfo(node_id='847f2be4-3799-41b5-80c0-b390298eba24', node_type=&lt;ObjectType.DOCUMENT: '4'&gt;, metadata={}, hash='74e64008cffed21d58edef5058f6cf6b3bc853bf936b83eefb70563168b73c5a'), &lt;NodeRelationship.PREVIOUS: '2'&gt;: RelatedNodeInfo(node_id='22d5c0dc-d921-4790-ac6e-4f6a6d5f336f', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='772c092906000e119c69ad2e5cb90148a6c8b113d54a20fb9d5984d6a9695ee8'), &lt;NodeRelationship.NEXT: '3'&gt;: RelatedNodeInfo(node_id='893d077f-a8ab-4a3f-9765-69ef72d46ec4', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='df269253fe4504ec666a0a40380f9399466c5bd366c7ce6c853ee45b31d4bc84')}, text='of the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n\\n3.2.1   Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk , and\\n                                    ‚àö values of dimension dv .', mimetype='text/plain', start_char_idx=11715, end_char_idx=12088, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.588352239002419),\n NodeWithScore(node=TextNode(id_='c42d8e8c-24ac-447a-8058-d62d198ce9eb', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={&lt;NodeRelationship.SOURCE: '1'&gt;: RelatedNodeInfo(node_id='847f2be4-3799-41b5-80c0-b390298eba24', node_type=&lt;ObjectType.DOCUMENT: '4'&gt;, metadata={}, hash='74e64008cffed21d58edef5058f6cf6b3bc853bf936b83eefb70563168b73c5a'), &lt;NodeRelationship.PREVIOUS: '2'&gt;: RelatedNodeInfo(node_id='e961df5f-04be-4bf8-bba0-b30b346e6e3e', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='944203475caa494a68b2ca15140cea2278792db8546209bcc538388bf227b57d'), &lt;NodeRelationship.NEXT: '3'&gt;: RelatedNodeInfo(node_id='12962f1d-060f-49d3-9ff9-be2dceb23736', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='46773d9899458459b747af4980832a961621033663b11cb056304074633c0f14')}, text='Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].', mimetype='text/plain', start_char_idx=8003, end_char_idx=8396, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.5581949233902119),\n NodeWithScore(node=TextNode(id_='893d077f-a8ab-4a3f-9765-69ef72d46ec4', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={&lt;NodeRelationship.SOURCE: '1'&gt;: RelatedNodeInfo(node_id='847f2be4-3799-41b5-80c0-b390298eba24', node_type=&lt;ObjectType.DOCUMENT: '4'&gt;, metadata={}, hash='74e64008cffed21d58edef5058f6cf6b3bc853bf936b83eefb70563168b73c5a'), &lt;NodeRelationship.PREVIOUS: '2'&gt;: RelatedNodeInfo(node_id='04c12537-5f33-4d41-a4d4-df30d2aed6e4', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='4dc2893909c949675d444324e091b9dcae176eafe0faeb456e4f571f79863ac8'), &lt;NodeRelationship.NEXT: '3'&gt;: RelatedNodeInfo(node_id='e48f428a-1d0f-4830-8aca-82cbf4cd4b67', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='4a7481ff7440b3355d18a8f77fdbcf637903e138a37a44c74d4fd287baf610f2')}, text='We compute the dot products of the\\nquery with all keys, divide each by dk , and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V .', mimetype='text/plain', start_char_idx=12089, end_char_idx=12415, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.5557579023667499),\n NodeWithScore(node=TextNode(id_='0146f53a-f1b1-4d80-a333-26746920ab9d', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={&lt;NodeRelationship.SOURCE: '1'&gt;: RelatedNodeInfo(node_id='847f2be4-3799-41b5-80c0-b390298eba24', node_type=&lt;ObjectType.DOCUMENT: '4'&gt;, metadata={}, hash='74e64008cffed21d58edef5058f6cf6b3bc853bf936b83eefb70563168b73c5a'), &lt;NodeRelationship.PREVIOUS: '2'&gt;: RelatedNodeInfo(node_id='c0f333cd-8860-48e5-b177-649855617c5a', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='c5cea5e4a2c19b51c1912e3fbb06fd9f445f2ab46a888146c9540685c513a907'), &lt;NodeRelationship.NEXT: '3'&gt;: RelatedNodeInfo(node_id='5d433fd9-785b-4f25-b3b0-5cd206b0ca37', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='e52e557964f178c114303403bfab945ce6fc6bc18fbc723bc2c110071beaf965')}, text='‚Ä¢ The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\n           and queries come from the same place, in this case, the output of the previous layer in the\\n           encoder. Each position in the encoder can attend to all positions in the previous layer of the\\n           encoder.', mimetype='text/plain', start_char_idx=16021, end_char_idx=16345, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.5531707169222685),\n NodeWithScore(node=TextNode(id_='22d5c0dc-d921-4790-ac6e-4f6a6d5f336f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={&lt;NodeRelationship.SOURCE: '1'&gt;: RelatedNodeInfo(node_id='847f2be4-3799-41b5-80c0-b390298eba24', node_type=&lt;ObjectType.DOCUMENT: '4'&gt;, metadata={}, hash='74e64008cffed21d58edef5058f6cf6b3bc853bf936b83eefb70563168b73c5a'), &lt;NodeRelationship.PREVIOUS: '2'&gt;: RelatedNodeInfo(node_id='71788dae-10dc-4341-8ebd-250a8836bce5', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='f1c9e10879cdc5796376d70528c5ccd9d988818269ef633ea539e6d2df1922d1'), &lt;NodeRelationship.NEXT: '3'&gt;: RelatedNodeInfo(node_id='04c12537-5f33-4d41-a4d4-df30d2aed6e4', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='4dc2893909c949675d444324e091b9dcae176eafe0faeb456e4f571f79863ac8')}, text='3.2   Attention\\n\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n\\n\\n                                                  3\\n\\x0c           Scaled Dot-Product Attention                                  Multi-Head Attention\\n\\n\\n\\n\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.', mimetype='text/plain', start_char_idx=11208, end_char_idx=11712, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.5503383930857552),\n NodeWithScore(node=TextNode(id_='55481635-fcaa-4e90-9625-9b0c3bfa3109', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={&lt;NodeRelationship.SOURCE: '1'&gt;: RelatedNodeInfo(node_id='847f2be4-3799-41b5-80c0-b390298eba24', node_type=&lt;ObjectType.DOCUMENT: '4'&gt;, metadata={}, hash='74e64008cffed21d58edef5058f6cf6b3bc853bf936b83eefb70563168b73c5a'), &lt;NodeRelationship.PREVIOUS: '2'&gt;: RelatedNodeInfo(node_id='923d6eec-1ba9-4972-b457-47cc1cb5e5a7', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='534fa8133845bae34a1c58d14d5fe840710190a12c4951fa24b1acaaa4ed8e35'), &lt;NodeRelationship.NEXT: '3'&gt;: RelatedNodeInfo(node_id='ea0b511f-4179-4f64-8e5b-1cf5f6d76404', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='a958edeb1ca826ae9eb259fb9846f6fe7d822b9462583eea56914ae0383170e5')}, text='.                       .              .\\n                                                                                                                 &lt;EOS&gt;       &lt;EOS&gt;            &lt;EOS&gt;                 &lt;EOS&gt;\\n                                                                                                                  &lt;pad&gt;      &lt;pad&gt;             &lt;pad&gt;                &lt;pad&gt;\\n\\n\\n\\n\\n     Full attentions for head 5. Bottom: Isolated attentions from just the word ‚Äòits‚Äô for attention heads 5\\n     Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution.', mimetype='text/plain', start_char_idx=55980, end_char_idx=56574, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.46287885047540767),\n NodeWithScore(node=TextNode(id_='04b195bd-26e4-4d8c-afdc-780e96bdd345', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={&lt;NodeRelationship.SOURCE: '1'&gt;: RelatedNodeInfo(node_id='847f2be4-3799-41b5-80c0-b390298eba24', node_type=&lt;ObjectType.DOCUMENT: '4'&gt;, metadata={}, hash='74e64008cffed21d58edef5058f6cf6b3bc853bf936b83eefb70563168b73c5a'), &lt;NodeRelationship.PREVIOUS: '2'&gt;: RelatedNodeInfo(node_id='c28b6b26-7bbf-4682-9399-a7804be460ae', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='c3ad5697d4d156dd0b4c85c17741ee433c10899ddffbd3575904ce08cd6736de'), &lt;NodeRelationship.NEXT: '3'&gt;: RelatedNodeInfo(node_id='c0f333cd-8860-48e5-b177-649855617c5a', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='c5cea5e4a2c19b51c1912e3fbb06fd9f445f2ab46a888146c9540685c513a907')}, text='3.2.3    Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n\\n         ‚Ä¢ In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\n           and the memory keys and values come from the output of the encoder. This allows every\\n           position in the decoder to attend over all positions in the input sequence.', mimetype='text/plain', start_char_idx=15478, end_char_idx=15877, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.4550194901912972),\n NodeWithScore(node=TextNode(id_='d93b8e55-28cb-417e-838a-a22abf7cfbc9', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={&lt;NodeRelationship.SOURCE: '1'&gt;: RelatedNodeInfo(node_id='847f2be4-3799-41b5-80c0-b390298eba24', node_type=&lt;ObjectType.DOCUMENT: '4'&gt;, metadata={}, hash='74e64008cffed21d58edef5058f6cf6b3bc853bf936b83eefb70563168b73c5a'), &lt;NodeRelationship.PREVIOUS: '2'&gt;: RelatedNodeInfo(node_id='398e22c4-5cd8-42ed-ba1d-43f213413bc2', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='cd837bc3b60f4cff2ab7f296f85515886d65e8ca7c2a3fb9c7b10fb1c6904949'), &lt;NodeRelationship.NEXT: '3'&gt;: RelatedNodeInfo(node_id='e9ffed0b-00f1-4408-bd5d-512f5d05138d', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='d0057b6da67faef5766281c2cae5a165b6e5396059cd7c09222a6d9e77ca985c')}, text='On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv -dimensional\\n   4\\n     To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q ¬∑ k = di=1\\n                                                                        P k\\n                                                                              qi ki , has mean 0 and variance dk .', mimetype='text/plain', start_char_idx=14037, end_char_idx=14560, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.45141889186813816),\n NodeWithScore(node=TextNode(id_='158309a7-9a7a-47e6-ac58-1a4e98eee41b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={&lt;NodeRelationship.SOURCE: '1'&gt;: RelatedNodeInfo(node_id='847f2be4-3799-41b5-80c0-b390298eba24', node_type=&lt;ObjectType.DOCUMENT: '4'&gt;, metadata={}, hash='74e64008cffed21d58edef5058f6cf6b3bc853bf936b83eefb70563168b73c5a'), &lt;NodeRelationship.PREVIOUS: '2'&gt;: RelatedNodeInfo(node_id='e4e96748-8e42-4c45-a1b3-3e0b2a179475', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='162e546ee2aace8fdcf9330a044ed33bc46d32219ec57c876b93a1fad69425e7'), &lt;NodeRelationship.NEXT: '3'&gt;: RelatedNodeInfo(node_id='8229d93a-1fb8-492f-9227-2b13658180f7', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='e9431f13405886d724857fa8ba6e9d0bd84affbaf2d35beedeeda36e79d95de8')}, text='4     Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1 , ..., xn ) to another sequence of equal length (z1 , ..., zn ), with xi , zi ‚àà Rd , such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.', mimetype='text/plain', start_char_idx=20488, end_char_idx=20939, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.4348473100243987),\n NodeWithScore(node=TextNode(id_='721c5981-90a9-4046-a757-4593a362ddf7', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={&lt;NodeRelationship.SOURCE: '1'&gt;: RelatedNodeInfo(node_id='847f2be4-3799-41b5-80c0-b390298eba24', node_type=&lt;ObjectType.DOCUMENT: '4'&gt;, metadata={}, hash='74e64008cffed21d58edef5058f6cf6b3bc853bf936b83eefb70563168b73c5a'), &lt;NodeRelationship.PREVIOUS: '2'&gt;: RelatedNodeInfo(node_id='a07f95e3-64fb-4637-ac18-4a928541df80', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='b11620062050474b2e5a6317e981c8ad07b227f032ebe169b1cb4f87c8994aa6'), &lt;NodeRelationship.NEXT: '3'&gt;: RelatedNodeInfo(node_id='165241f9-efb1-433a-896b-b6ea61168d3f', node_type=&lt;ObjectType.TEXT: '1'&gt;, metadata={}, hash='559f529f69207d17371f20407b6f1b4691910f9c8a90c9cefbb741e95fbf5de9')}, text='Operations\\n      Self-Attention                      O(n2 ¬∑ d)             O(1)                O(1)\\n      Recurrent', mimetype='text/plain', start_char_idx=18618, end_char_idx=18733, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.4276254505797798)]\nThe retriever can then directly be use as a tool to answer questions about our documents:\nfrom llama_index.core.tools import BaseTool, FunctionTool\n\ndef find_references(question: str) -&gt; str:\n    \"\"\"Query a database containing the paper \"Attention is all you Need\" in parts.\n    This paper introduced the mechanism of self-attention to the NLP-literature.\n    Returns a collection of scored text-snippets that are relevant to your question.\"\"\"\n    return '\\n'.join([f'{round(n.score,2)} - {n.node.text}' for n in retriever.retrieve(question)])\n\n\nfind_references_tool = FunctionTool.from_defaults(fn=find_references)\nThis tool can then be added to an agent as we discussed before:\nfrom llama_index.core.agent import ReActAgent\n\nfrom llama_index.llms.lmstudio import LMStudio\n\n\nllm = LMStudio(model_name=\"llama-3.2-1b-instruct\",\n        base_url=\"http://localhost:1234/v1\",\n    temperature=0.5,\n    request_timeout=600)\n\n\nagent = ReActAgent.from_tools(tools=[find_references_tool],llm=llm, verbose=True)\n/home/brede/MEGA/Honorar/Generative AI/script/.venv/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in LMStudio has conflict with protected namespace \"model_\".\n\nYou may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n  warnings.warn(\nWhich can then be used to answer chat-requests:\nresponse = agent.chat(\"What is the meaning of Query, Key and Value in the context of self-attention?\")\nprint(str(response))\n&gt; Running step 062240ab-0d21-4fdb-a603-fb386970c32f. Step input: What is the meaning of Query, Key and Value in the context of self-attention?\nObservation: Error: Could not parse output. Please follow the thought-action-input format. Try again.\n&gt; Running step 2a291a80-5090-4373-945d-3a647ac2b758. Step input: None\nObservation: Error: Could not parse output. Please follow the thought-action-input format. Try again.\n&gt; Running step 908425d7-8f06-4830-8585-4ff312b43c45. Step input: None\nObservation: Error: Could not parse output. Please follow the thought-action-input format. Try again.\n&gt; Running step 543ab12f-e5e7-4a59-b103-b7fc7bd0a3fe. Step input: None\nObservation: Error: Could not parse output. Please follow the thought-action-input format. Try again.\n&gt; Running step 1b3cb4e3-e976-4420-a489-906b8f6c5776. Step input: None\nThought: Let's break down what Query, Key, and Value mean in the context of self-attention.\nAction: Use\nAction Input: {'input': \"What are the most relevant words for the sentence 'The quick brown fox jumps over the lazy dog'?\", 'num_beams': 5}\nObservation: Error: No such tool named `Use`.\n&gt; Running step 1101520e-54ff-42db-b327-d9d902acb957. Step input: None\nThought: I need to find a way to input the query and parameters into a tool.\nAction: Use\nAction Input: {'input': \"What are the most relevant words for the sentence 'The quick brown fox jumps over the lazy dog'?\", 'num_beams': 5}\nObservation: Error: No such tool named `Use`.\n&gt; Running step 47c5a9f6-5055-4f8c-9a3b-49f1db40abcb. Step input: None\nThought: I'm using a different tool to find references. Let me check if it supports finding relevant text snippets for the given query.\nAction: find_references\nAction Input: {'properties': AttributedDict([('question', \"What are the most relevant words for the sentence 'The quick brown fox jumps over the lazy dog'?\"), ('num_beams', 5)]), 'required': ['query', 'parameters']}\nObservation: Error: find_references() got an unexpected keyword argument 'properties'\n\nValueError: Reached max iterations.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)\nCell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----&gt; 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is the meaning of Query, Key and Value in the context of self-attention?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mstr\u001b[39m(response))\n\nFile \u001b[0;32m~/MEGA/Honorar/Generative AI/script/.venv/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:311\u001b[0m, in \u001b[0;36mDispatcher.span.&lt;locals&gt;.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m             _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--&gt; 311\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[1;32m    314\u001b[0m         new_future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(result)\n\nFile \u001b[0;32m~/MEGA/Honorar/Generative AI/script/.venv/lib/python3.10/site-packages/llama_index/core/callbacks/utils.py:41\u001b[0m, in \u001b[0;36mtrace_method.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m callback_manager \u001b[38;5;241m=\u001b[39m cast(CallbackManager, callback_manager)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m callback_manager\u001b[38;5;241m.\u001b[39mas_trace(trace_id):\n\u001b[0;32m---&gt; 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\nFile \u001b[0;32m~/MEGA/Honorar/Generative AI/script/.venv/lib/python3.10/site-packages/llama_index/core/agent/runner/base.py:647\u001b[0m, in \u001b[0;36mAgentRunner.chat\u001b[0;34m(self, message, chat_history, tool_choice)\u001b[0m\n\u001b[1;32m    642\u001b[0m     tool_choice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_tool_choice\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    644\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mAGENT_STEP,\n\u001b[1;32m    645\u001b[0m     payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mMESSAGES: [message]},\n\u001b[1;32m    646\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--&gt; 647\u001b[0m     chat_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchat_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchat_history\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatResponseMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWAIT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    653\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chat_response, AgentChatResponse)\n\u001b[1;32m    654\u001b[0m     e\u001b[38;5;241m.\u001b[39mon_end(payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mRESPONSE: chat_response})\n\nFile \u001b[0;32m~/MEGA/Honorar/Generative AI/script/.venv/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:311\u001b[0m, in \u001b[0;36mDispatcher.span.&lt;locals&gt;.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m             _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--&gt; 311\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[1;32m    314\u001b[0m         new_future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(result)\n\nFile \u001b[0;32m~/MEGA/Honorar/Generative AI/script/.venv/lib/python3.10/site-packages/llama_index/core/agent/runner/base.py:579\u001b[0m, in \u001b[0;36mAgentRunner._chat\u001b[0;34m(self, message, chat_history, tool_choice, mode)\u001b[0m\n\u001b[1;32m    576\u001b[0m dispatcher\u001b[38;5;241m.\u001b[39mevent(AgentChatWithStepStartEvent(user_msg\u001b[38;5;241m=\u001b[39mmessage))\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;66;03m# pass step queue in as argument, assume step executor is stateless\u001b[39;00m\n\u001b[0;32m--&gt; 579\u001b[0m     cur_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_choice\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cur_step_output\u001b[38;5;241m.\u001b[39mis_last:\n\u001b[1;32m    584\u001b[0m         result_output \u001b[38;5;241m=\u001b[39m cur_step_output\n\nFile \u001b[0;32m~/MEGA/Honorar/Generative AI/script/.venv/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:311\u001b[0m, in \u001b[0;36mDispatcher.span.&lt;locals&gt;.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m             _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--&gt; 311\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[1;32m    314\u001b[0m         new_future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(result)\n\nFile \u001b[0;32m~/MEGA/Honorar/Generative AI/script/.venv/lib/python3.10/site-packages/llama_index/core/agent/runner/base.py:412\u001b[0m, in \u001b[0;36mAgentRunner._run_step\u001b[0;34m(self, task_id, step, input, mode, **kwargs)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# TODO: figure out if you can dynamically swap in different step executors\u001b[39;00m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;66;03m# not clear when you would do that by theoretically possible\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m ChatResponseMode\u001b[38;5;241m.\u001b[39mWAIT:\n\u001b[0;32m--&gt; 412\u001b[0m     cur_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m ChatResponseMode\u001b[38;5;241m.\u001b[39mSTREAM:\n\u001b[1;32m    414\u001b[0m     cur_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_worker\u001b[38;5;241m.\u001b[39mstream_step(step, task, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\nFile \u001b[0;32m~/MEGA/Honorar/Generative AI/script/.venv/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:311\u001b[0m, in \u001b[0;36mDispatcher.span.&lt;locals&gt;.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m             _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--&gt; 311\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[1;32m    314\u001b[0m         new_future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(result)\n\nFile \u001b[0;32m~/MEGA/Honorar/Generative AI/script/.venv/lib/python3.10/site-packages/llama_index/core/callbacks/utils.py:41\u001b[0m, in \u001b[0;36mtrace_method.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m callback_manager \u001b[38;5;241m=\u001b[39m cast(CallbackManager, callback_manager)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m callback_manager\u001b[38;5;241m.\u001b[39mas_trace(trace_id):\n\u001b[0;32m---&gt; 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\nFile \u001b[0;32m~/MEGA/Honorar/Generative AI/script/.venv/lib/python3.10/site-packages/llama_index/core/agent/react/step.py:818\u001b[0m, in \u001b[0;36mReActAgentWorker.run_step\u001b[0;34m(self, step, task, **kwargs)\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;129m@trace_method\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_step\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, step: TaskStep, task: Task, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m&gt;\u001b[39m TaskStepOutput:\n\u001b[1;32m    817\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Run step.\"\"\"\u001b[39;00m\n\u001b[0;32m--&gt; 818\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\nFile \u001b[0;32m~/MEGA/Honorar/Generative AI/script/.venv/lib/python3.10/site-packages/llama_index/core/agent/react/step.py:576\u001b[0m, in \u001b[0;36mReActAgentWorker._run_step\u001b[0;34m(self, step, task)\u001b[0m\n\u001b[1;32m    572\u001b[0m reasoning_steps, is_done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_actions(\n\u001b[1;32m    573\u001b[0m     task, tools, output\u001b[38;5;241m=\u001b[39mchat_response\n\u001b[1;32m    574\u001b[0m )\n\u001b[1;32m    575\u001b[0m task\u001b[38;5;241m.\u001b[39mextra_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurrent_reasoning\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mextend(reasoning_steps)\n\u001b[0;32m--&gt; 576\u001b[0m agent_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextra_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcurrent_reasoning\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextra_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msources\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_done:\n\u001b[1;32m    580\u001b[0m     task\u001b[38;5;241m.\u001b[39mextra_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnew_memory\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mput(\n\u001b[1;32m    581\u001b[0m         ChatMessage(content\u001b[38;5;241m=\u001b[39magent_response\u001b[38;5;241m.\u001b[39mresponse, role\u001b[38;5;241m=\u001b[39mMessageRole\u001b[38;5;241m.\u001b[39mASSISTANT)\n\u001b[1;32m    582\u001b[0m     )\n\nFile \u001b[0;32m~/MEGA/Honorar/Generative AI/script/.venv/lib/python3.10/site-packages/llama_index/core/agent/react/step.py:437\u001b[0m, in \u001b[0;36mReActAgentWorker._get_response\u001b[0;34m(self, current_reasoning, sources)\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo reasoning steps were taken.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(current_reasoning) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_iterations:\n\u001b[0;32m--&gt; 437\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReached max iterations.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(current_reasoning[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], ResponseReasoningStep):\n\u001b[1;32m    440\u001b[0m     response_step \u001b[38;5;241m=\u001b[39m cast(ResponseReasoningStep, current_reasoning[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\n\u001b[0;31mValueError\u001b[0m: Reached max iterations.\nAs you can see, the model request ends up with errors. The model is not powerful enough to answer in the structured manner we need for the function-calling of the tool. To circumvent this, we can try a function-calling-finetuned model:\nWe can try to solve this issue by using a language model that is finetuned on function calling:\nfc_llm = LMStudio(model_name=\"phi-3-mini-4k-instruct-function-calling\",\n        base_url=\"http://localhost:1234/v1\",\n    temperature=0.2,\n    request_timeout=600)\n\nagent = ReActAgent.from_tools(tools=[find_references_tool],llm=fc_llm, verbose=True)\nresponse = agent.chat(\"What is the meaning of Query, Key and Value in the context of self-attention?\")\nprint(str(response))\n&gt; Running step 78c0a52b-55fa-4241-ade5-67c0b92b9bf3. Step input: What is the meaning of Query, Key and Value in the context of self-attention?\nThought: (Implicit) I can answer without any more tools!\nAnswer:  In the context of self-attention, \"Query\", \"Key\" and \"Value\" are terms used to describe different components of a neural network architecture. Here's what they mean:\n1. Query - The query component is used to retrieve information from memory banks during attention computation. It represents a set of learned parameters that enable the model to focus on specific parts of an input sequence when processing it. In other words, the query function defines how much importance we should give to each part of our input data while computing self-attention weights.\n2. Key - The key component is used to determine which parts of the input sequence are most relevant for a particular output location in the model's memory bank. It represents another set of learned parameters that help us identify important features in an input sequence during attention computation. In other words, the key function helps us decide what we should focus on when computing self-attention weights.\n3. Value - The value component is used to store the actual data corresponding to each memory bank location in a neural network architecture. It represents our stored knowledge or \"memory\" that can be retrieved later during attention computation. In other words, the value function holds all of the information we need to compute an output based on self-attention weights.\nIn summary, query, key and value are components of a neural network architecture used in self-attention that help us focus on specific parts of our input sequence, identify important features within it, and retrieve relevant stored knowledge/memory to compute outputs.\n In the context of self-attention, \"Query\", \"Key\" and \"Value\" are terms used to describe different components of a neural network architecture. Here's what they mean:\n1. Query - The query component is used to retrieve information from memory banks during attention computation. It represents a set of learned parameters that enable the model to focus on specific parts of an input sequence when processing it. In other words, the query function defines how much importance we should give to each part of our input data while computing self-attention weights.\n2. Key - The key component is used to determine which parts of the input sequence are most relevant for a particular output location in the model's memory bank. It represents another set of learned parameters that help us identify important features in an input sequence during attention computation. In other words, the key function helps us decide what we should focus on when computing self-attention weights.\n3. Value - The value component is used to store the actual data corresponding to each memory bank location in a neural network architecture. It represents our stored knowledge or \"memory\" that can be retrieved later during attention computation. In other words, the value function holds all of the information we need to compute an output based on self-attention weights.\nIn summary, query, key and value are components of a neural network architecture used in self-attention that help us focus on specific parts of our input sequence, identify important features within it, and retrieve relevant stored knowledge/memory to compute outputs.\nThis model does not run into an issue with the structured output, it does not try to use the tool anymore though.\nOne way to try to solve this issue is to adapt the agent-prompt:\nprint(agent.get_prompts()['agent_worker:system_prompt'].template)\nYou are designed to help with a variety of tasks, from answering questions to providing summaries to other types of analyses.\n\n## Tools\n\nYou have access to a wide variety of tools. You are responsible for using the tools in any sequence you deem appropriate to complete the task at hand.\nThis may require breaking the task into subtasks and using different tools to complete each subtask.\n\nYou have access to the following tools:\n{tool_desc}\n\n\n## Output Format\n\nPlease answer in the same language as the question and use the following format:\n\n```\nThought: The current language of the user is: (user's language). I need to use a tool to help me answer the question.\nAction: tool name (one of {tool_names}) if using a tool.\nAction Input: the input to the tool, in a JSON format representing the kwargs (e.g. {{\"input\": \"hello world\", \"num_beams\": 5}})\n```\n\nPlease ALWAYS start with a Thought.\n\nNEVER surround your response with markdown code markers. You may use code markers within your response if you need to.\n\nPlease use a valid JSON format for the Action Input. Do NOT do this {{'input': 'hello world', 'num_beams': 5}}.\n\nIf this format is used, the user will respond in the following format:\n\n```\nObservation: tool response\n```\n\nYou should keep repeating the above format till you have enough information to answer the question without using any more tools. At that point, you MUST respond in one of the following two formats:\n\n```\nThought: I can answer without using any more tools. I'll use the user's language to answer\nAnswer: [your answer here (In the same language as the user's question)]\n```\n\n```\nThought: I cannot answer the question with the provided tools.\nAnswer: [your answer here (In the same language as the user's question)]\n```\n\n## Current Conversation\n\nBelow is the current conversation consisting of interleaving human and assistant messages.\nThis we can adapt in the following way:\nfrom llama_index.core import PromptTemplate\nnew_agent_template_str = \"\"\"\nYou are designed to help answer questions based on a collection of paper-excerpts.\n\n## Tools\n\nYou have access to tools that allow you to query paper-content. You are responsible for using the tools in any sequence you deem appropriate to complete the task at hand.\nThis may require breaking the task into subtasks and using different tools to complete each subtask. Do not answer without tool-usage if a tool can be used to answer a question. Do try to find a text passage to back up your claims whenever possible. Do not answer without reference if the appropriate text is available in the tools you have access to.\n\nYou have access to the following tools:\n{tool_desc}\n\n\n## Output Format\n\nPlease answer in the same language as the question and use the following format:\n\n\\`\\`\\`\nThought: The current language of the user is: (user's language). I need to use a tool to help me answer the question.\nAction: tool name (one of {tool_names}) if using a tool.\nAction Input: the input to the tool, in a JSON format representing the kwargs (e.g. {{\"input\": \"hello world\", \"num_beams\": 5}})\n\\`\\`\\`\n\n\nPlease ALWAYS start with a Thought.\n\nNEVER surround your response with markdown code markers. You may use code markers within your response if you need to.\n...\n## Current Conversation\n\nBelow is the current conversation consisting of interleaving human and assistant messages.\n\"\"\"\nnew_agent_template = PromptTemplate(new_agent_template_str)\nagent.update_prompts(\n    {\"agent_worker:system_prompt\": new_agent_template}\n)\nWe can test this new prompt with the same question:\nresponse = agent.chat(\"What is the meaning of Query, Key and Value in the context of self-attention?\")\nprint(str(response))\n&gt; Running step d5fb46ea-de7a-4e8b-ace7-7ed3ae6a9706. Step input: What is the meaning of Query, Key and Value in the context of self-attention?\nThought: (Implicit) I can answer without any more tools!\nAnswer:  In the context of natural language processing (NLP), \"Query\", \"Key\" and \"Value\" are used as components for a type of neural network architecture called Transformer model. The Transformer model employs self-attention mechanism to improve its ability to process sequential data such as text or audio. \nHere's how these terms relate to the model:\n1. Query - A query is an input vector that represents the current state of a sequence being processed by the transformer network. It contains information about which words or tokens are currently being attended to, and helps guide the attention mechanism towards relevant parts of the input sequence.\n2. Key - The key component in a transformer model refers to a set of learned weights that help determine how much importance should be given to each word or token during self-attention computation. These keys are computed for all words or tokens in an input sequence and they form part of the attention mechanism used by the Transformer network.\n3. Value - The value component is responsible for storing information from a specific memory slot corresponding to a particular input token in the transformer model. It represents the output produced when we apply a transformation function on the query vector (which contains contextual information about the current word or token being processed) using learned weights, and then weighted-summed with the key vectors.\nIn summary, Query, Key and Value are components of a neural network architecture used in Transformer models for NLP that help us process sequential data such as text by guiding attention towards relevant parts of an input sequence, identifying important features within it, and computing outputs based on self-attention weights.\n In the context of natural language processing (NLP), \"Query\", \"Key\" and \"Value\" are used as components for a type of neural network architecture called Transformer model. The Transformer model employs self-attention mechanism to improve its ability to process sequential data such as text or audio. \nHere's how these terms relate to the model:\n1. Query - A query is an input vector that represents the current state of a sequence being processed by the transformer network. It contains information about which words or tokens are currently being attended to, and helps guide the attention mechanism towards relevant parts of the input sequence.\n2. Key - The key component in a transformer model refers to a set of learned weights that help determine how much importance should be given to each word or token during self-attention computation. These keys are computed for all words or tokens in an input sequence and they form part of the attention mechanism used by the Transformer network.\n3. Value - The value component is responsible for storing information from a specific memory slot corresponding to a particular input token in the transformer model. It represents the output produced when we apply a transformation function on the query vector (which contains contextual information about the current word or token being processed) using learned weights, and then weighted-summed with the key vectors.\nIn summary, Query, Key and Value are components of a neural network architecture used in Transformer models for NLP that help us process sequential data such as text by guiding attention towards relevant parts of an input sequence, identifying important features within it, and computing outputs based on self-attention weights.\nThe model still tries to answer without the tool.\nLet‚Äôs try to ask a more specific question:\nresponse = agent.chat(\"How does the paper 'Attention is all you need' define the term self attention?\")\nprint(str(response))\n&gt; Running step 3c1b3050-f4a0-4b46-9006-366161df0219. Step input: How does the paper 'Attention is all you need' define the term self attention?\nThought: (Implicit) I can answer without any more tools!\nAnswer:  In the paper \"Attention Is All You Need\", the authors present a novel Transformer model that relies heavily on an attention mechanism to improve its ability to process sequential data such as text or audio. The paper introduces several key concepts related to this mechanism, including the notion of \"self-attention\". \nSelf-attention is defined in the paper as follows: given a sequence of input tokens (or words), self-attention enables us to compute contextualized representations for each token by computing attention weights over all other tokens in the sequence. These attention weights reflect how much importance we should give to each token when computing our output representation. In particular, during training, these weights are learned based on the input data itself and can be adjusted dynamically as new inputs come in. The resulting contextualized representations produced by self-attention provide a rich source of information for downstream tasks like language modeling or machine translation.\n In the paper \"Attention Is All You Need\", the authors present a novel Transformer model that relies heavily on an attention mechanism to improve its ability to process sequential data such as text or audio. The paper introduces several key concepts related to this mechanism, including the notion of \"self-attention\". \nSelf-attention is defined in the paper as follows: given a sequence of input tokens (or words), self-attention enables us to compute contextualized representations for each token by computing attention weights over all other tokens in the sequence. These attention weights reflect how much importance we should give to each token when computing our output representation. In particular, during training, these weights are learned based on the input data itself and can be adjusted dynamically as new inputs come in. The resulting contextualized representations produced by self-attention provide a rich source of information for downstream tasks like language modeling or machine translation.\nStill no dice.\nOne solution to this problem is to just use a bigger model:\nllm = LMStudio(model_name=\"llama-3.2-3b-instruct\", #3 Billion instead of 1\n        base_url=\"http://localhost:1234/v1\",\n    temperature=0.2,\n    request_timeout=600)\n\n\nagent = ReActAgent.from_tools(tools=[find_references_tool],llm=llm, verbose=True)\n\nresponse = agent.chat(\"How does the paper 'Attention is all you need' define the term self attention?\")\nprint(str(response))\n&gt; Running step 9326aba5-48cf-40dd-8b85-b5da82554e5c. Step input: How does the paper 'Attention is all you need' define the term self attention?\nThought: The current language of the user is English. I need to use a tool to help me answer the question.\nAction: find_references\nAction Input: {'properties': AttributedDict([('question', AttributedDict([('title', 'self-attention definition'), ('type', 'string')]))]), 'required': ['question'], 'type': 'object'}\nObservation: Error: find_references() got an unexpected keyword argument 'properties'\n&gt; Running step b9bd6255-c348-473e-a031-2fd1e4e74cdf. Step input: None\nThought: The current language of the user is English. I need to use a tool to help me answer the question, but it seems like find_references doesn't support the properties argument.\nAction: find_references\nAction Input: {'question': \"How does the paper 'Attention is all you Need' define the term self attention?\"}\nObservation: 0.69 - Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n0.52 - .                       .              .\n                                                                                                                 &lt;EOS&gt;       &lt;EOS&gt;            &lt;EOS&gt;                 &lt;EOS&gt;\n                                                                                                                  &lt;pad&gt;      &lt;pad&gt;             &lt;pad&gt;                &lt;pad&gt;\n\n\n\n\n     Full attentions for head 5. Bottom: Isolated attentions from just the word ‚Äòits‚Äô for attention heads 5\n     Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution.\n0.5 - &lt;EOS&gt;\n                                                                                                                                           &lt;pad&gt;\n                                                                                                                                                   &lt;pad&gt;\n                                                                                                                                                   &lt;pad&gt;\n                                                                                                                                                           &lt;pad&gt;\n                                                                                                                                                                   &lt;pad&gt;\n                                                                                                                                                                           &lt;pad&gt;\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\nthe verb ‚Äòmaking‚Äô, completing the phrase ‚Äòmaking...more difficult‚Äô.\n0.5 - Each layer has two\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\nthe two sub-layers, followed by layer normalization [1].\n0.49 - 4     Why Self-Attention\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\ntional layers commonly used for mapping one variable-length sequence of symbol representations\n(x1 , ..., xn ) to another sequence of equal length (z1 , ..., zn ), with xi , zi ‚àà Rd , such as a hidden\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\nconsider three desiderata.\n0.47 - In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [17, 18] and [9].\n\n\n3   Model Architecture\n\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\nHere, the encoder maps an input sequence of symbol representations (x1 , ..., xn ) to a sequence\nof continuous representations z = (z1 , ..., zn ).\n0.45 - .                       .             .\n                                                                                                              &lt;EOS&gt;       &lt;EOS&gt;            &lt;EOS&gt;                &lt;EOS&gt;\n                                                                                                               &lt;pad&gt;      &lt;pad&gt;             &lt;pad&gt;               &lt;pad&gt;\n\n\n\n\n     sentence. We give two such examples above, from two different heads from the encoder self-attention\n     Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\n0.44 - Operations\n      Self-Attention                      O(n2 ¬∑ d)             O(1)                O(1)\n      Recurrent\n0.43 - ‚Ä¢ The encoder contains self-attention layers. In a self-attention layer all of the keys, values\n           and queries come from the same place, in this case, the output of the previous layer in the\n           encoder. Each position in the encoder can attend to all positions in the previous layer of the\n           encoder.\n0.42 - As side benefit, self-attention could yield more interpretable models. We inspect attention distributions\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\nand semantic structure of the sentences.\n\n\n5     Training\n\nThis section describes the training regime for our models.\n&gt; Running step ac9c9225-596e-4c84-8e86-1518a4fd7d55. Step input: None\nThought: The current language of the user is English. I was able to retrieve relevant information about self-attention from the paper \"Attention is all you Need\". It seems that the authors define self-attention as an attention mechanism that relates different positions of a single sequence in order to compute a representation of the sequence.\nAnswer: Self-attention, also known as intra-attention, is an attention mechanism that computes a representation of a sequence by attending to different positions within the same sequence. It has been used successfully in various tasks such as reading comprehension, abstractive summarization, textual entailment, and learning task-independent sentence representations.\nSelf-attention, also known as intra-attention, is an attention mechanism that computes a representation of a sequence by attending to different positions within the same sequence. It has been used successfully in various tasks such as reading comprehension, abstractive summarization, textual entailment, and learning task-independent sentence representations.\nThis is not always feasible though.\nAnother way to use the retrieval-pipeline is to not give a weak model the opportunity to mess up the tool calling. This can be implemented by using a query-engine instead of the retriever. This directly wraps the retrieval in a LLM-Summarization-Module that only returns summaries.\nDoing this, we can use two separate models for each part of the task - one for the planning and answering and one for the structured summarization:\nquery_engine = index.as_query_engine(use_async=False, llm=fc_llm, verbose=True)\nresponse = query_engine.query(\"What is the meaning of Query, Key and Value in the context of self-attention?\")\nprint(str(response))\n In the context of self-attention, \"Query\" refers to the keys that are used to retrieve relevant information from a sequence. \"Key\" represents the values associated with each element in the sequence, which determine their importance or relevance. \"Value\" corresponds to the actual data being processed by the attention mechanism.\nFinally an answer we can work with!\n\n\n\n\n\n\nNoteüìù Task\n\n\n\nBuild a llamaindex-application that allows you to chat with the climate_fever evidence.\n\n\n\n\n\n\n\nDocument chunking\nThe examples we looked at until now were all working with short text-snippets that comforably fit into the context window of a LLM. If you think about usual usecases for RAG-systems, this is not the most common case though. Usually, you will have a base of documents that can span multiple 1000‚Äôs of tokens and you want to be able to answer questions about these documents. Furthermore, you do not only want to know which document might be relevant, but ideally also which part of the document matches your question best.\nThis is where the process of doctument chunking or document splitting comes into play. There is a series of possible approaches to split a document, the most common, so called naive chunking method, is to use a structural element of the document though. This means that you parse the documents into sentences, paragraphs or pages and then use these as chunks that you individually embed and store in your vector database. To prevent loss of relevant context when splitting a document into chunks, it is additionally common to add some overlap between the chunks. This tries to solve the lost context problem, does however create reduncencies in the data.\nAn alternative approach is to use semantic chunking. This means that you split a document into chunks based on their meaning. Jina.ai explained in a blogpost (Late Chunking in Long-Context Embedding Models, 2024) their so called ‚Äúlate chunking‚Äù method. which iteratively runs the whole document through the attention head of the transformer to gain embeddings per token, and then averages these embeddings per naive chunk. This way, the chunks are still structure based but contain semantic information about the whole context. \nAnother approach to semantic chunking is described on the doc-pages of LlamaIndex. In their approach to semantic chunking, an adaptive splitting-rule is used, that splits the documents based on semantic similarity of sentences. This means that sentences that are semantically similar are grouped together into chunks.\n\n\n\n\n\n\nNoteüìù Task\n\n\n\nImplement a document chunking strategy for a book of your choice from the project_gutenberg dataset.\nYou can use any approach you like, but you should explain your choice and why it is appropriate for this dataset.\n\n\n\n\nQuery Expansion/Transformation\nUntil now, we have based our retrieval on the assumption, that the question the user formulates is a good representation of their information need. This is not always the case though. Often, users do not know what they are looking for or they use synonyms or paraphrases that are not present in the documents. If the question is not formulated well, or if it is too specific, the system might not be able to find relevant documents. To improve the quality of the questions, we can use query expansion. This means that we take the original question and expand it with additional information to make it more specific and to increase the chances of finding relevant documents. This can be done in multiple ways, one common approach is to use a generative model to generate multiple queries based on the original question. Another approach is to use a keyword extraction algorithm to extract keywords from the question and then use these keywords to expand the query.\nThe most basic way to implement a query-expansion is to build a tool that instructs a LLM to give multiple alternate formulations of the original query. Though this will probably work, there are more refined methods.\nLlamaindex implements two more sophisticated approaches to transform queries:\n\nHypothetical Document Embeddings (HyDe): A LLM is instructed to generate a hypothetical document that answers the query. This document is then used to query the index\nMulti-Step Query Transformations: After a first execution of a (complex) query against an index, the answer is used to iteratively formulate follow-up questions that are then executed against the index.\n\n\n\n\n\n\n\nNoteüìù Task\n\n\n\nImplement query expansion for the climate_fever dataset using llamaindex. This might be helpful.\nExperiment with different prompts and temperatures.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Embedding-based LLM-systems</span>"
    ]
  },
  {
    "objectID": "content/embeddings.html#further-readings",
    "href": "content/embeddings.html#further-readings",
    "title": "Embedding-based LLM-systems",
    "section": "Further Readings",
    "text": "Further Readings\n\nThis blogpost by DeepSet gives a good overview of the concept of RAG\nThis blogpost by qdrant about (their) vector store and its inner workings",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Embedding-based LLM-systems</span>"
    ]
  },
  {
    "objectID": "content/embeddings.html#references",
    "href": "content/embeddings.html#references",
    "title": "Embedding-based LLM-systems",
    "section": "References",
    "text": "References\n\n\n\n\nBowman, S. R., Angeli, G., Potts, C., & Manning, C. D. (2015). A large annotated corpus for learning natural language inference. In L. M√†rquez, C. Callison-Burch, & J. Su (Eds.), Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (pp. 632‚Äì642). Association for Computational Linguistics. https://doi.org/10.18653/v1/D15-1075\n\n\nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (arXiv:1810.04805). arXiv. https://doi.org/10.48550/arXiv.1810.04805\n\n\nGoyal, K., & Sharma, M. (2022). Comparative Analysis of Different Vectorizing Techniques for Document Similarity using Cosine Similarity. 2022 Second International Conference on Advanced Technologies in Intelligent Control, Environment, Computing & Communication Engineering (ICATIECE), 1‚Äì5. https://doi.org/10.1109/ICATIECE56365.2022.10046766\n\n\nJiang, T., Huang, S., Luan, Z., Wang, D., & Zhuang, F. (2023). Scaling Sentence Embeddings with Large Language Models (arXiv:2307.16645). arXiv. https://doi.org/10.48550/arXiv.2307.16645\n\n\nLate Chunking in Long-Context Embedding Models. (2024). https://jina.ai/news/late-chunking-in-long-context-embedding-models.\n\n\nReimers, N., & Gurevych, I. (2019). Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks (arXiv:1908.10084). arXiv. https://doi.org/10.48550/arXiv.1908.10084\n\n\nSteck, H., Ekanadham, C., & Kallus, N. (2024). Is Cosine-Similarity of Embeddings Really About Similarity? Companion Proceedings of the ACM Web Conference 2024, 887‚Äì890. https://doi.org/10.1145/3589335.3651526\n\n\nTunstall, L., Reimers, N., Jo, U. E. S., Bates, L., Korat, D., Wasserblat, M., & Pereg, O. (2022). Efficient Few-Shot Learning Without Prompts (arXiv:2209.11055). arXiv. https://arxiv.org/abs/2209.11055",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Embedding-based LLM-systems</span>"
    ]
  },
  {
    "objectID": "content/agent_interaction.html",
    "href": "content/agent_interaction.html",
    "title": "LLM pipelines",
    "section": "",
    "text": "Pipelines vs.¬†agents\nAs already mentioned, an agent is defined by its ability to make decisions. This means the agent decides if and what tool to use or not to use. One potential problem with this is that, a lot of the time, the LLM may decide not to use any tool. This is especially true for retrieval augmented generation (RAG) tasks, where looking up the answer in a document store is not really optional, but the whole reason to have the system in the first place.\nAt other times, the workflow may just not be complex enough to warrant the additional complexity of using agents in your application. When you find yourself having a workflow where you basically iterate through a number of steps in always the same order, you may skip the agent altogether and just hardcode a series of LLM calls instead. This is called an LLM pipeline.\nWe will discuss an example pipeline and the difference to agent systems in detail below. But before we get to this, we want to introduce a concept, which is widely used in a lot of LLM-based systems.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>LLM pipelines</span>"
    ]
  },
  {
    "objectID": "content/agent_interaction.html#llm-as-a-judge",
    "href": "content/agent_interaction.html#llm-as-a-judge",
    "title": "LLM pipelines",
    "section": "LLM as a judge",
    "text": "LLM as a judge\nThe basic idea is to use a an LLM to criticize (to judge) a text snippet. To understand why this would be useful, please do the following task:\n\n\n\n\n\n\nNoteüìù Task\n\n\n\n\nOpen a notebook and connect it with a local LLM using LM Studio (or other).\nAsk it to generate a story containing fried eggs on a sunrise.\nHow good is the story? Do you like it? What is good and what is not so good? How did you come to this conclusion?\nNow generate 100 such stories. Dial up the temperature so you do not get the same story every time.\nRepeat step 3 for all of the 100 stories‚Ä¶\n\n\n\n‚Ä¶ or don‚Äôt.\nBased on our experience, we can formulate a problem description.\n\nWe generate text (be it natural language or structured output) using LLMs.\nThe generated text is not always correct or appropriate for our use case.\nWe need a way to evaluate the quality of the generated text.\nTo do this, we have to read it.\nWe don‚Äôt have time for this.\n\nThe solution to this problem is, of course, to use an LLM to read and evaluate the text. This is only fair and proper, since it was an LLM that generated the text in the first place. The generated evaluation can then be used\n\nto decide whether to accept or reject the generated text. This is the most basic form of judgement.\nto improve the model itself e.g., for fine-tuning it on the generated text and its evaluation. This was used in fine tuning the ‚ÄúClaude‚Äù model family by anthropic (Constitutional AI, n.d.)\nto get an LLM to improve the text based on the evaluation. This is used in a variety of frameworks, e.g.¬†textgrad (Yuksekgonul et al., 2024).\n\nThis approach is called LLM as a judge. It is an example of a system that uses several calls to one or several LLMs to solve a problem.\nThis approach has a number of benefits as well as drawbacks.\n\nBenefits:\n\nThe evaluation can be very accurate and fast.\nIt is easy to implement.\nIt is easy to scale up the number of LLMs used for evaluation.\nIt is easy to use different LLMs for generation and evaluation.\nIt is easy to use different prompts for generation and evaluation.\n\nDrawbacks:\n\nThe evaluation can be very expensive, since it requires several calls to the LLM.\nThe evaluation can be biased, especially when it is evaluating its own text. Indeed many LLMs tend to like their own creations.\nThe evaluation can be subjective, since it is based on the LLMs‚Äô interpretation of the prompt.\nThe evaluation can be misleading, since it is based on the LLMs‚Äô interpretation of the generated text, which may not be the same as the human interpretation. For example, many LLMs seem to prefer long answers over shorter ones.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>LLM pipelines</span>"
    ]
  },
  {
    "objectID": "content/agent_interaction.html#a-basic-pipeline",
    "href": "content/agent_interaction.html#a-basic-pipeline",
    "title": "LLM pipelines",
    "section": "A basic pipeline",
    "text": "A basic pipeline\nLet us now look at a simple example of such a system. We will use the following scenario: We want to generate Anki flashcards from text.1 To do this, we will build a system that consists of a series of specialized LLM calls (we will call them agents for now, even though this may not be completely correct). Later, we will expand it to a full agentic system to illustrate the differences between the two concepts. The Agents we will use here are:\n1¬†The following is loosely based on ‚ÄúBuilding a Multi-Agent Framework from Scratch with LlamaIndex‚Äù (2024), though I took the liberty to streamline and simplify it a bit. \nAn Anki card generator that generates Anki flashcards from a text.\nA Reviewer, that reviews the generated Anki flashcards and gives tips on how to improve them.\nAn Editor, that generates a new set of Anki flashcards based on the reviewer‚Äôs feedback.\n\nOptionally, when building an agent system, not a pipeline, there is also\n\nAn Orchestrator, that serves as the decision maker, managing the other agents and deciding when to stop.\n\nWe could also add more specialized agents, like a fact checker agent, that checks the generated cards for factual correctness, a translator that translates either the input text or the generated cards, or a topic analyzer that breaks down down complex topics into manageable parts before card generation.\n\n\nGenerator\n\nLet us first implement the Anki card generator. It will take text as input and return a card. A system prompt for the generator could look like this:\nYou are an educational content creator specializing in Anki flashcard generation.\nYour task is to create one clear, concise flashcards following these guidelines:\n\n1. The card should focus on ONE specific concept\n2. The question should be clear and unambiguous\n3. The answer should be concise but complete\n4. Include relevant extra information in the extra field\n5. Follow the minimum information principle\n\nFormat the card as:\n&lt;card&gt;\n    &lt;question&gt;Your question here&lt;/question&gt;\n    &lt;answer&gt;Your answer here&lt;/answer&gt;\n    &lt;extra&gt;Additional context, examples, or explanations&lt;/extra&gt;\n&lt;/card&gt;\nInstead of the complicated formatting above, you can simply use a pydantic class, of course.\n\n\n\n\n\n\nNoteüìù Task\n\n\n\nYour turn!\n\nIn your notebook, implement a card generator. You can use a simple wrapper around an LLM call or use one of the agent frameworks to implement it as an agent with no tools.\nDiscuss: is it still an agent, if it does not have tools? Ask an LLM about its opinion on that üòâ.\nLet it generate cards from the text below (or any other text of your choice).\nLLM-as-a-Judge is an evaluation method to assess the quality of text outputs from any LLM-powered product, including chatbots, Q&A systems, or agents. It uses a large language model (LLM) with an evaluation prompt to rate generated text based on criteria you define.\nEvaluate the results.\n\n\n\n\n\nReviewer\n\nLet us now implement the reviewer. It will take a card as input and return feedback on how to improve it. A system prompt for the reviewer could look like this:\nYou are an expert in educational content creation, specializing in Anki flashcard generation.\nYou are the Reviewer agent. Your task is to review an Anki flashcard based on the following rules:\n\n1. The card should test ONE piece of information\n2. The question must be:\n    - Simple and direct\n    - Testing a single fact\n    - Using cloze format (cloze deletion or occlusion) when appropriate\n3. The answers must be:\n    - Brief and precise\n    - Limited to essential information\n4. The extra field must include:\n    - Detailed explanations\n    - Examples\n    - Context\n5. Information should not be repeated, i.e. the extra information should not repeat the answer. \n\nPlease give brief and concise feedback to the card you received in natural language. \n\n\n\n\n\n\nNoteüìù Task\n\n\n\nLet‚Äôs build us a very judgemental robot!\n\nIn the same notebook, initialize a reviewer as well.\nLet the reviewer review the cards generated by the generator. You may find that the reviewer always thinks the cards are great. This happens a lot. So:\nGet the reviewer to actually find stuff to improve.\n\n\n\n\n\nEditor\n\nLet us now implement the Editor. It will take a card and feedback as input and return a new card based on the feedback. A system prompt for the editor could look like this:\nYou are an expert in educational content creation, specializing in Anki flashcard generation.\nYou are the Editor agent. Your task is to generate a new Anki flashcard based on the original card and the feedback you received from the Reviewer.\nFollow these guidelines:\n\n1. Incorporate the feedback into your new card\n2. The new card should still focus on ONE specific concept\n3. The question should be clear and unambiguous \n4. The answer should be concise but complete \n5. Include relevant extra information in the extra field \n6. Follow the minimum information principle\n7. If no feedback is provided, return the original card\n8. Format the card as:\n\n&lt;card&gt;\n    &lt;question&gt;Your question here&lt;/question&gt;\n    &lt;answer&gt;Your answer here&lt;/answer&gt;\n    &lt;extra&gt;Additional context, examples, or explanations&lt;/extra&gt;\n&lt;/card&gt;\n\n\n\n\n\n\nNoteüìù Task\n\n\n\nTime to improve!\n\nIn the same notebook, initialize the editor as well.\nLet the editor generate new cards based on the feedback from the reviewer.\nGet the editor to actually generate something that is different from the generators version! (Play around with models, prompts and/or input text. In this example, this only worked for me when using a weaker model as a generator and a larger one as reviewer and editor.)\n\n\n\n\n\nFull pipeline\nWe now basically have a working LLM pipeline.\n\nA good next step would be to add a loop around the review process, so that it only stops if the reviewer is happy. In pseudocode, this could look like this:\n# Pseudocode\ninitialize:\n    Generator\n    Reviewer\n    Editor\n\ncard = Generator.generate(input text)\n\n# now the loop\nwhile n &lt; max_iter:  # define maximum number of review steps\n    n += 1\n    review = Reviewer.review(card)\n    if review is positive:  # this has to be defined\n        stop\n    else:\n        card = Editor.edit(card, review)\nreturn card\nFor this to work, you need to find a way to determine if the reviewer feedback is positive enough to stop and return the card. You could, for instance, prompt the reviewer to return a certain phrase when satisfied and than parse the response for that. Or you could make a sentiment analysis, or you could make an LLM call!\n\n\n\n\n\n\nNoteüìù Task\n\n\n\nBuild it!\n\nThink of a way to stop the review process once the card is good enough.\nImplement the loop.\n\nThis is the minimum goal for today, so when you are done, you can upload your notebook to moodle. Alternatively, go on and implement the agent (see below) as well!\n\n\n\n\nOrchestrator agent\nWhile we‚Äôre at it, we can implement the orchestrator as well. Let us think for a moment what the orchestrators job should be. Its task should be decision making. That is, it‚Äôs the orchestrators job to decide which of the other agents to call next. It is also responsible for deciding whether the job is finished or not, i.e.¬†whether to call any more agents. In terms of input and output, the orchestrator should get a current state of affairs along with the current chat history and output a decision. So the output can be one of the other agents or a stop signal.\nAn example prompt for our case would be:\nYou are the Orchestrator agent. Your task is to coordinate the interaction between all agents to create high-quality flashcards.\n\nAvailable agents:\n* Generator - Creates flashcards\n* Reviewer - Improves card quality\n* Editor\n\nDecision Guidelines:\n- Use Generator to create cards\n- Use Reviewer to generate feedback\n- Use Editor to improve cards based on feedback.\n- Choose END when the cards are ready\n\nOutput only the next agent to run (\"Generator\", \"Reviewer\", \"Editor\", or \"END\")\n\n\nAgent workflow\nNow, all we have to do is integrate our agents into a pipeline. The basic idea is to call the orchestrator at each step and let it decide which agent to call next or wether to stop. For this, the agent will need an understanding of the basic state of affairs and the past interaction. This is easily implemented like this:\nstate = {\n    \"input_text\": text,\n    \"qa_card\": \"\",\n    \"review_status\": \"pending\",\n    \"edit_status\": \"pending\"\n    }\n\nmemory = ChatMemoryBuffer.from_defaults(token_limit=8000) # using LLamaindex here\nThe memory can be read using the memory.get() method.\nThen we define our workflow as an iterative process. Below is a piece of pseudocode illustrating the basic idea:\n# pseudocode\ninitialize:\n    generator\n    reviewer\n    editor\n    orchestrator\n    state\n    memory\n\nwhile true\n    send state and memory to orchestrator -&gt; response\n    if response == \"end\" \n        stop\n    if response == \"generator\" \n        send input text to generator -&gt; card, change state and memory\n    (same for the other agents)\nreturn state\n\n\n\n\n\n\nNoteüìù Task\n\n\n\nTime to play!\n\nIn the same notebook, initialize the orchestrator as well.\nImplement the workflow shown above in real code.\nWatch happily as it all works without any issues whatsoever.\nUpload to Moodle.\n\n\n\nWhat we did not cover but what would be a great idea:\n\nRight now, we just assume that generator and editor return valid output. It would be better to build an automated check using a pydantic class for that.\nWe let the orchestrator agent decide for how long this process lasts. &lt;sarcasm&gt;I cannot imagine that leading to problems under any circumstances.&lt;/sarcasm&gt; It would be better to give it a timeout or maximal number of iterations.",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>LLM pipelines</span>"
    ]
  },
  {
    "objectID": "content/agent_interaction.html#project-work",
    "href": "content/agent_interaction.html#project-work",
    "title": "LLM pipelines",
    "section": "Project work",
    "text": "Project work\nTime to work on your projects!\n\n\n\n\n\n\nNoteüìù Task\n\n\n\n\nDiscuss a topic for your project.\nSet up the repository\nPlan the project\nStart collecting data or finding a data set\nStart implementing\n\nHappy coding!",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>LLM pipelines</span>"
    ]
  },
  {
    "objectID": "content/agent_interaction.html#further-readings",
    "href": "content/agent_interaction.html#further-readings",
    "title": "LLM pipelines",
    "section": "Further Readings",
    "text": "Further Readings\n\nHere is a video describing other multi-agent systems, including an agent hospital and a multi-agent translator",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>LLM pipelines</span>"
    ]
  },
  {
    "objectID": "content/agent_interaction.html#references",
    "href": "content/agent_interaction.html#references",
    "title": "LLM pipelines",
    "section": "References",
    "text": "References\n\n\n\n\nBuilding a Multi-Agent Framework from Scratch with LlamaIndex. (2024). In DEV Community. https://dev.to/yukooshima/building-a-multi-agent-framework-from-scratch-with-llamaindex-5ecn.\n\n\nConstitutional AI: Harmlessness from AI Feedback. (n.d.). https://www.anthropic.com/research/constitutional-ai-harmlessness-from-ai-feedback.\n\n\nYuksekgonul, M., Bianchi, F., Boen, J., Liu, S., Huang, Z., Guestrin, C., & Zou, J. (2024). TextGrad: Automatic \"Differentiation\" via Text (arXiv:2406.07496). arXiv. https://doi.org/10.48550/arXiv.2406.07496",
    "crumbs": [
      "Language Models",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>LLM pipelines</span>"
    ]
  },
  {
    "objectID": "content/diff_models.html",
    "href": "content/diff_models.html",
    "title": "AI image generation",
    "section": "",
    "text": "AI image generator basics\nThis and the following chapters will focus on the topic of AI image generation. This is a very broad field, so we will start with some basics and then move on to more specific topics. We will also try to give an overview of the current state of the art in this field.\nYou can not talk about the history of AI image generation without talking about GANs (Goodfellow et al., 2014). To have a nicer chunking of the courses contents though, we will talk about them in the chapter ?sec-GANS and focus on more recent approaches here. GANs are the architecture behind the page thispersondoesnotexist.com and its clones.",
    "crumbs": [
      "Image Generation",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>AI image generation</span>"
    ]
  },
  {
    "objectID": "content/diff_models.html#ai-image-generator-basics",
    "href": "content/diff_models.html#ai-image-generator-basics",
    "title": "AI image generation",
    "section": "",
    "text": "DALL-E\nThe reigning position of GANs as the de-facto standard for AI image generation was challenged by the release of DALL-E by OpenAI in January 2021. DALL-E is a text-to-image model, which means that it can generate images based on a text description.\nThis model was trained on a dataset containing image-caption pairs in two parts:\n\nA Variational Autoencoder (VAE)1 to compress the image data into a latent space. This means, that each image was compressed into a 32x32 grid, for which each grid cell was encoded as a discrete probability distribution with 8192 dimensions. This latent ‚Äútoken‚Äù-space is, although the architecture is pretty different, quite close to what our text-transformers outputted in the MLM-task.\nA Transformer to learn the relationship between text-captions and the latent space. This was done by encoding images using the pretrained VAE und argmax choosing the 32x32-token-representation of the image. The text-captions were limited to 256 tokens and concatenated with the 1024-dimensional image-tokens. The model is then trained to predict the next token in the sequence, which is either a text or an image token, similarly to the learning-paradigm we discussed when talking about the transformer-training.\n\n1¬†Since the latent space these images are compressed to is of a defined set of classes, the authors call the model a discrete VAE which makes a lot of sense.The resulting 1024 image-tokens can then be fed into the decoder-Block of the VAE to generate an image. An illustration of the training-process can be seen in Figure¬†9.1.\n\n\n\n\n\n\n\nFig¬†9.1: Illustration of the DALL-E-VAE (A) and Illustration of the whole DALL-E-Stack (B). Both images are taken from Abideen (2023).\n\n\n\n\n\nCLIP\nClose to the release of DALL-E, the team at OpenAI did also publish CLIP (Radford et al., 2021). The paper, which introduced a contrastive2 method to learn visual representations from images and text descriptions, bridged the gap between image and text embeddings. This contrastive principle is illustrated in Figure¬†9.2.\n2¬†Contrastive also being the namesake of the method (Contrastive Language-Image Pre-training)\n\n\n\n\n\nFig¬†9.2: Illustration of the contrastive learning paradigm used in CLIP, taken from Radford et al. (2021)\n\n\n\nA matrix of all combinations of images and text descriptions is created. The model then learns to predict the correct image for a given text description and vice versa. This is done by encoding both the image and the text into a vector space, which is then used to calculate the similarity between the two vectors. to do this, both a vision- and a text-transformer are trained as encoders to maximize the cosine similarity between the encoded image and text for each pair in the matrix and minimizing it for all other pairs. The authors also show that this method can be used to transfer the learned representations to other tasks, such as zero-shot classification.",
    "crumbs": [
      "Image Generation",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>AI image generation</span>"
    ]
  },
  {
    "objectID": "content/diff_models.html#diffusion-models",
    "href": "content/diff_models.html#diffusion-models",
    "title": "AI image generation",
    "section": "Diffusion Models",
    "text": "Diffusion Models\nThough models like DALL-E and CLIP represented significant milestones in the journey of text-to-image generation, the field continued to evolve rapidly, leading to the advent of Stable Diffusion. This evolution was partly inspired by the need for more control over the generation process and a desire for higher-quality outputs at lower computational costs.\nThe GAN-architecture (first published in 2014) was the de-facto standard for quite some time and though the central principle of their successors diffusion models was published in 2015 (Sohl-Dickstein et al., 2015), it took until 2020 for them to beat GANs on most benchmarks (Dhariwal & Nichol, 2021).\nThe diffusion model‚Äôs central principle is training on a sequence of gradually noised images. This process involves systematically adding noise to an image over a series of steps, progressively transforming the original image into pure noise. The model is trained to reverse this process by predicting the noise added to each image, based on the current step in the noising sequence and the noisy image itself.\nThis step-by-step noise addition serves two main purposes:\n\nGradual Complexity: By progressively corrupting the image, the model can learn to reverse the process in manageable steps, leading to a better understanding of how to reconstruct data at each stage.\nMathematical Framework: This approach aligns with the stochastic differential equation (SDE) framework, enabling the model to map the noise distribution back to the original data distribution iteratively.\n\nThis approach, rather than predicting the denoised image directly, also offers practical advantages: it allows for efficient parallelization during training since the noise is parameterized by a scheduler and can be applied dynamically. This stepwise noise-addition is visually represented in Figure¬†9.3.\n\n\n\n\n\n\nFig¬†9.3: Illustration of the diffusion process. The first row shows a 2-d swiss roll gradually getting more noisy, the second row shows the corresponding outputs of the diffusion model. Image taken from Sohl-Dickstein et al. (2015).\n\n\n\nRombach et al. (2022) build upon this principle when suggesting their Latent Diffusion Model architecture and introduced a few key innovations to achieve their state-of-the-art results:\n\nThey introduce a method called latent diffusion, which allows them to generate high-resolution images more efficiently by operating on a lower-dimensional representation of the image data. This is achieved by using an autoencoder (VAE) to compress the original image into a smaller latent space and then applying the diffusion process to this compressed representation. This process is built on work by Esser et al. (2021) and is conceptually similar to the dVAE-approach utilized by DALL-E.\nThey use a denoising diffusion probabilistic model (DDPM) as the fundamental generation process for their architecture, which allows them to generate high-quality images with fewer steps compared to previous methods. This DDPM model is implemented as a time-conditional UNet.\nTo improve the quality of generated images and reduce artifacts, they integrate a cross-attention mechanism into the UNet architecture. This mechanism conditions the denoising process directly on the input text embeddings, allowing the diffusion process to generate images that align better with the given text prompt.\n\nTo improve the results on inference, they additionally utilize classifier-free guidance (Ho & Salimans, 2022), a technique where the model is run once with the prompt (‚Äúconditional on the prompt‚Äù) and once with an empty pseudo-prompt (‚Äúunconditional‚Äù). A weighted combination of the conditioned and unconditioned predictions is used to enhance the alignment with the text prompt while preserving image quality. This is done using the following formula:\n\\[\n\\text{Guided Prediction} = \\text{Unconditioned Prediction} + w \\cdot (\\text{Conditioned Prediction} - \\text{Unconditioned Prediction})\n\\]\nWhere \\(w\\) is the weight with which the conditioned prediction is preferred over the unconditioned one.\n\n\n\n\n\n\nFig¬†9.4: Illustration of the Latent Diffusion Model architecture. Image taken from Rombach et al. (2022)\n\n\n\nThis architecture has been widely adopted and is used as a foundation3 for many state-of-the-art text-to-image models, including Stable Diffusion, as well as DALL-E 2.\n3¬†Or at least as an orientation.\n\n\n\n\n\nNoteüìù Task\n\n\n\nTest out a SD-model!\nUse the colab-Notebook you can find here to test out Stable Diffusion using the huggingface diffusers-module and generate some images.\n\nPrint out the model architecture and try to map the components of the model to the description above.\nGenerate some images using different prompts, guidance_scales and seeds. What do you observe?\nThere are many pages with tips on how to ‚Äúcorrectly‚Äù prompt SD-models to improve their performance. Find one and test the described tips out. What do you find?\nTest the num_inference_steps-parameter. How does it affect the quality of the generated image?",
    "crumbs": [
      "Image Generation",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>AI image generation</span>"
    ]
  },
  {
    "objectID": "content/diff_models.html#multimodal-models",
    "href": "content/diff_models.html#multimodal-models",
    "title": "AI image generation",
    "section": "Multimodal Models",
    "text": "Multimodal Models\nSo called multimodal models are models that are trained to fit one latent distribution for multiple modalities. This means that instead of only using the image encoder and decoder with some kind of informed diffusion model to generate images in between, encoders and decoders for multiple modalities are trained to map onto the same latent space. This results in a family of models that can take inputs in multiple modalities and create outputs in a similar fashion. There are different approaches to solve this task, of which two will be discussed in the following section\n\nUnidiffuser\nOne of the first multimodal models is Unidiffuser, an architecture described in Bao et al. (2023). The architecture is illustrated in Figure¬†9.5.\n\n\n\n\n\n\nFig¬†9.5: Illustration of the Unidiffuser architecture. Image taken from Bao et al. (2023)\n\n\n\nThe model is based on a transformer-encoder and decoder that are trained to map inputs of multiple modalities onto the same latent space. In the text-image implementation, there are two encoders and two decoders. The image encoder consists of two parts. One is the VAE-encoder from Stable Diffusion, which maps the input image into a lower dimensional representation. This is appended by the CLIP-image-embedder described in Radford et al. (2021). The text gets also encoded by the CLIP-trained model used in Stable Diffusion.\nFor image-decoding, the Stable Diffusion VAE-decoder is used to map the latent space back into an image. For text-decoding, a GPT-2-based (Radford et al., 2019)model is finetuned to take the latent space embeddings as a prefix-embedding and to autoregressively generate text. During finetuning, the CLIP-embeddings were held constant and only the GPT-2-parameters were updated. This means that the already defined latent space learned by the CLIP-model is used to map the GPT-2 decoder onto it.\nThese embeddings are then used to train a U-ViT (Bao et al., 2022) model, which takes the concated time-step-tokens, noised text- and image-embeddings as input-tokens and outputs the estimated noise-vector for the denoising process.\n\n\n\n\n\n\nNoteüìù Task\n\n\n\nUse the same colab-notebook as before to test out Unidiffuser using the huggingface diffusers-module and generate some images and text.\nTry the tips you tested on the basic SD-model and test whether the model accurately generates descriptions for your generated images.\nPresent your results of both tasks to the course and upload your adapted notebook to moodle.\n\n\n\n\nLlama 3.2\nLlama 3.2 introduced image-understanding to the Llama-model family. Similarly to the decoder-training in the unidiffuser case, this was done by mapping existing embeddings onto a new latent space. Instead of finetuning a part of the model on a constant other embedding though, Meta describes a slightly different approach in their launch-blogpost (‚ÄúLlama 3.2,‚Äù n.d.).\nThey describe a procedure in which they use both a pretrained image encoder as well as a fixed pretrained language model. The embeddings of both models are aligned using a special third adapter model, that builds on multiple cross-attention layers to map the encoded image onto the language models text-embedding space. The encoder and adapter were then trained using image-text pairs to correctly generate the text-labels for the images.",
    "crumbs": [
      "Image Generation",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>AI image generation</span>"
    ]
  },
  {
    "objectID": "content/diff_models.html#further-reading",
    "href": "content/diff_models.html#further-reading",
    "title": "AI image generation",
    "section": "Further Reading",
    "text": "Further Reading\n\nThis blogpost about the reparametrization trick\nThis Medium-article about how the first DALL-E worked\nThe tutorial-paper by Doersch (2021) about the intuition and mathematics of VAEs\nComputerphile did some very nice videos about SD and CLIP",
    "crumbs": [
      "Image Generation",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>AI image generation</span>"
    ]
  },
  {
    "objectID": "content/diff_models.html#references",
    "href": "content/diff_models.html#references",
    "title": "AI image generation",
    "section": "References",
    "text": "References\n\n\n\n\nAbideen, Z. ul. (2023). How OpenAI‚Äôs DALL-E works? In Medium.\n\n\nBao, F., Li, C., Cao, Y., & Zhu, J. (2022). All are worth words: A vit backbone for score-based diffusion models. NeurIPS 2022 Workshop on Score-Based Methods.\n\n\nBao, F., Nie, S., Xue, K., Li, C., Pu, S., Wang, Y., Yue, G., Cao, Y., Su, H., & Zhu, J. (2023). One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale (arXiv:2303.06555). arXiv. https://doi.org/10.48550/arXiv.2303.06555\n\n\nDhariwal, P., & Nichol, A. (2021). Diffusion Models Beat GANs on Image Synthesis. Advances in Neural Information Processing Systems, 34, 8780‚Äì8794.\n\n\nDoersch, C. (2021). Tutorial on Variational Autoencoders (arXiv:1606.05908). arXiv. https://doi.org/10.48550/arXiv.1606.05908\n\n\nEsser, P., Rombach, R., & Ommer, B. (2021). Taming Transformers for High-Resolution Image Synthesis (arXiv:2012.09841). arXiv. https://doi.org/10.48550/arXiv.2012.09841\n\n\nGoodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., & Bengio, Y. (2014). Generative Adversarial Networks (arXiv:1406.2661). arXiv. https://doi.org/10.48550/arXiv.1406.2661\n\n\nHo, J., & Salimans, T. (2022). Classifier-Free Diffusion Guidance (arXiv:2207.12598). arXiv. https://doi.org/10.48550/arXiv.2207.12598\n\n\nLlama 3.2: Revolutionizing edge AI and vision with open, customizable models. (n.d.). In Meta AI. https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/.\n\n\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., & Sutskever, I. (2021). Learning Transferable Visual Models From Natural Language Supervision. Proceedings of the 38th International Conference on Machine Learning, 8748‚Äì8763.\n\n\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language models are unsupervised multitask learners. OpenAI Blog, 1(8), 9.\n\n\nRombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2022). High-Resolution Image Synthesis with Latent Diffusion Models (arXiv:2112.10752). arXiv. https://doi.org/10.48550/arXiv.2112.10752\n\n\nSohl-Dickstein, J., Weiss, E., Maheswaranathan, N., & Ganguli, S. (2015). Deep unsupervised learning using nonequilibrium thermodynamics. International Conference on Machine Learning, 2256‚Äì2265.",
    "crumbs": [
      "Image Generation",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>AI image generation</span>"
    ]
  },
  {
    "objectID": "content/generation_in_agent_pipelines.html",
    "href": "content/generation_in_agent_pipelines.html",
    "title": "AI image generation pipelines",
    "section": "",
    "text": "Basics of using Open Source AI image generation models\nOne of the challenges of using image generation models is the required computational power and the fine-tuning effort needed to obtain high quality images. This can be a significant barrier for individuals or smaller organizations that may not have access to large computing resources. It is, therefore, good to have access to pre-trained image generation models. Today, we want to focus on using image generation models locally and how to integrate them into pipelines.\nFor large language models, we used mainly LM Studio to run the models on our laptops. Image generation models, however, do not run in LM Studio as of 2025. Additionally, there is no real equivalent for image generation models. There is, however, a tool that makes running image generation models locally more convenient: AUTOMATIC1111‚Äôs Stable Diffusion web UI.\nThis tool surely does make image generation more convenient. Most of the time, however, we do not want to deal with a web UI, but with an API endpoint. Fortunately, A1111‚Äôs webUI also has an API mode, which is quite easy to use and supports all features of the web UI (and some more). We are mostly interested in the txt2img API endpoint, which allows us to generate images from a text prompt. Let‚Äôs have a look at how this works:\nWe now know how to easily generate images using a local model. The next steps would be to try different models, and to add Lora (or other) adapters to them.",
    "crumbs": [
      "Image Generation",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>AI image generation pipelines</span>"
    ]
  },
  {
    "objectID": "content/generation_in_agent_pipelines.html#basics-of-using-open-source-ai-image-generation-models",
    "href": "content/generation_in_agent_pipelines.html#basics-of-using-open-source-ai-image-generation-models",
    "title": "AI image generation pipelines",
    "section": "",
    "text": "Noteüìù Task\n\n\n\nLet‚Äôs have a look!\n\nInstall AUTOMATIC1111‚Äôs Stable Diffusion web UI on your laptop using these instructions.\nStart the server, open the webUI.\nStart generating images.üéâ\nChange some of the settings and see what happens.\nWhat does the Sampling steps parameter do?\nAlso check out the img2img function.\n\n\n\n\n\n\n\n\n\n\nNoteüìù Task\n\n\n\n\nOpen the documentation of the API.\nRun the web UI in API mode.\nin a notebook, run an example call to the txt2img endpoint.",
    "crumbs": [
      "Image Generation",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>AI image generation pipelines</span>"
    ]
  },
  {
    "objectID": "content/generation_in_agent_pipelines.html#ai-image-generators-in-agent-systems-or-pipelines",
    "href": "content/generation_in_agent_pipelines.html#ai-image-generators-in-agent-systems-or-pipelines",
    "title": "AI image generation pipelines",
    "section": "AI image generators in agent systems or pipelines",
    "text": "AI image generators in agent systems or pipelines\nIn this section we want to explore the use of AI image generators as components in agent systems or LLM pipelines. An example for this might be a system that takes a few keywords, generates a text from it and then uses a language model to generate an image generation prompt based on this text. This prompt is used to generate an image. The final image is then sent to some quality assurance system to check if the output matches the input (or at least makes sense).\n\nSo, we want to build a pipeline that\n\ngenerates or retrieves some text based on some input keywords.\nuses this text as context for generating an image generation prompt.\ngenerates an image from the prompt.\ndoes some kind of quality assurance by comparing the original text embedding with the generated image embedding.\n\nNote, that comparing the embeddings of the generated image with the input text may not be the best way of judging image quality.\nMost agent frameworks we already introduced support building pipelines in addition to agents. See for example this tutorial on how to implement query pipelines in llamaindex or this documentation for pipelines in haystack. To get a full understanding of the basic principles, it is most educational to implement a pipeline from scratch.\n\nText generation or retrieval\nThe pipeline we are about to build starts with some input given by the user. In previous chapters we covered several ways of doing this. You could:\n\nuse a local LLM to generate the text for you.\nuse a retrieval function from a vector store or other text database (e.g.¬†wikipedia API).\ncombine both approaches in a RAG system.\n\n\n\n\n\n\n\nNoteüìù Task\n\n\n\nLet‚Äôs get started!\n\nOpen a notebook and implement a simple text generation or retrieval function.\nGet some text from an input.\n\n\n\n\n\nImage generation\nThe next step is to to generate an image that fits the text. While we could just send the full text to the image generator and let it do its thing, a better approach is to generate a special prompt for the image generator. This prompt is then used to generate the image.\n\n\n\n\n\n\nNoteüìù Task\n\n\n\n\nIn your notebook, implement a call to an LLM that generates an image generation prompt from your text.\nAlso implement a call to an image generator.\nConnect to an LLM (if not already done so) and to an image generation model.\nGenerate an image for your text.\n\n\n\n\n\nQuality assurance\nNow that we have the image, we want to assure that it fits the text. There are several ways of doing this. We could, for instance, evaluate text and images manually (or, rather, by eyeballing it). This works well for small amounts of images. However, it is not scalable for larger amounts.\nOne way of automating the examination is to check if the image matches the text semantically, i.e.¬†in meaning. One could translate the image back to text, using an image-to-text model. This description of the image can then be compared to the original text using embeddings and a suitable distance metric, e.g.¬†cosine. Or we could embed both image and text using a multi-modal model and calculate the distance directly. In both cases, we need a predefined criterion, i.e.¬†a fixed distance, that has to be reached to accept the image as good enough. Alternatively, we could generate several images and just chose the best matching one.\n\n\n\n\n\n\nNoteüìù Task\n\n\n\nLet‚Äôs have a look!\n\nIn your notebook, implement a function that displays text and image for manual inspection.\nImplement an automated similarity rater for text and images. You can use CLIP for that task.\nThink of another way to automate quality insurance on the generated images.\n\n\n\n\n\nPipeline\nFinally, we can wrap everything in a pipeline. The pseudocode below shows the general principle. This is shown here for generating a number of images and picking the best matching one, but it can easily be converted to generate images until a predefined criterion is matched.\n## pseudocode\ndef image_gen_pipeline(user_input):\n    get_text(user_input) -&gt; text\n    generate_image_prompt(text) -&gt; image_prompt\n    for i in range 5:\n        generate_image(image_prompt) -&gt; image\n        rate_image(image) -&gt; rate_value\n    find_best_rated_image(images, rate_values) -&gt; best_image\n    return best_image\n\n\n\n\n\n\nNoteüìù Task\n\n\n\nLet‚Äôs finalize\n\nIn your notebook, implement the pipeline outlined above.\nMake a few test runs.\nUpload your notebook to Moodle.",
    "crumbs": [
      "Image Generation",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>AI image generation pipelines</span>"
    ]
  },
  {
    "objectID": "content/model_context_protocol.html",
    "href": "content/model_context_protocol.html",
    "title": "Model Context Protocol",
    "section": "",
    "text": "Function Calling\nModel Context Protocol (MCP) is a function calling standard and framework developed by anthropic and published in November 2024.\nLet‚Äôs reiterate what we know about function calling.\nFunction calling allows the LLM to do tasks it could not normally achieve. This is done by giving the model some tools that allow it to interact with the outside world to, for instance, gather information or control other applications. To use function calling, we first have to\nWe already did these things in the course. The basic function calling workflow then looks something like this:",
    "crumbs": [
      "Other",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Model Context Protocol</span>"
    ]
  },
  {
    "objectID": "content/model_context_protocol.html#function-calling",
    "href": "content/model_context_protocol.html#function-calling",
    "title": "Model Context Protocol",
    "section": "",
    "text": "write a system prompt that\n\ndescribes the tools the LLM has access to.\nprompts the LLM to structure the output in a certain way (usually JSON format) if it wants to use a tool.\n\n\n\n\nThe LLM is presented with a user query.\nIt starts reasoning internally about wether to use a tool for the task. If so, it\ngenerates a function call. As mentioned, this takes the form of generating structured output.\nThe LLM output is then parsed for tool calls. If tool calls are present, and syntactically correct, then\nThe tool calls are executed.\nThe result of the tool call is returned to the LLM, usually indicated by a key word (e.g.¬†Observation:).\nWhen the LLM has gathered enough information or thinks it does not need any more tool calls, a final answer is generated, if applicable.",
    "crumbs": [
      "Other",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Model Context Protocol</span>"
    ]
  },
  {
    "objectID": "content/model_context_protocol.html#a-unified-standard",
    "href": "content/model_context_protocol.html#a-unified-standard",
    "title": "Model Context Protocol",
    "section": "A unified standard",
    "text": "A unified standard\nThis is all well, as long as the tools are relatively easy to implement, like executing python functions. Additionally, you may want to give the LLM the option to:\n\nRead your files\nCheck your calendar\nSearch the web\nAccess your databases\nUse your company‚Äôs internal tools\n\nEvery time developers want to give an AI access to external data or tools, they have to build custom, one-off integrations. It‚Äôs like building a different key for every single door.\nMCP creates a standardized way for AI models to connect to external resources. Think of it as:\n\nUniversal translator: One standard ‚Äúlanguage‚Äù for AI-to-tool communication\nPlugin system: Like browser extensions, but for AI assistants\nModular approach: Write a tool once, use it with any MCP-compatible AI\n\nSo, first and foremost, MCP is an interface standard that defines how external tools can be integrated into large language models (LLMs). It provides a set of protocols and APIs that allow LLMs to interact with these tools seamlessly. This includes defining the inputs and outputs for each interaction, as well as the mechanisms for error handling and feedback loops.\nIn addition to a standard definition, anthropic also released an implementation of MCP in a number of languages, including Python. It also hosts a repository on GitHub with examples and documentation that you can use as a starting point for building your own integrations. There is also a large and growing collection of pre-built connectors to third-party services like databases, APIs, and other tools. That means developers don‚Äôt have to start from scratch every time they want to add new functionality to their LLMs.\n\n\n\n\n\n\nNoteüìù Task\n\n\n\nHave a look!\n\nFind the MCP and MCP server repository on GitHub\nBrowse through the list of server implementations, find one (or many) that you find interesting.\n\n\n\nBut what is an MCP server anyway? And how does MCP work exactly?",
    "crumbs": [
      "Other",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Model Context Protocol</span>"
    ]
  },
  {
    "objectID": "content/model_context_protocol.html#core-concepts",
    "href": "content/model_context_protocol.html#core-concepts",
    "title": "Model Context Protocol",
    "section": "Core concepts",
    "text": "Core concepts\nThe following is more or less taken from the official website.\nAt its core, MCP follows a client-server architecture where a host application can connect to multiple servers:\n\n\n\nMCP architecture (‚ÄúModel Context Protocol,‚Äù n.d.)\n\n\n\nMCP Hosts: Programs like Claude Desktop, IDEs, or AI tools that want to access data through MCP\nMCP Clients: Protocol clients that maintain 1:1 connections with servers\nMCP Servers: Lightweight programs that each expose specific capabilities through the standardized Model Context Protocol\nLocal Data Sources: Your computer‚Äôs files, databases, and services that MCP servers can securely access\nRemote Services: External systems available over the internet (e.g., through APIs) that MCP servers can connect to\n\nThe servers come in three main flavours:\n\nResources: File-like data that can be read by clients (like API responses or file contents)\nTools: Functions that can be called by the LLM (with user approval)\nPrompts: Pre-written templates that help users accomplish specific tasks\n\nAs you have seen in the repository, there are lots of pre-built servers available.",
    "crumbs": [
      "Other",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Model Context Protocol</span>"
    ]
  },
  {
    "objectID": "content/model_context_protocol.html#mcp-in-action",
    "href": "content/model_context_protocol.html#mcp-in-action",
    "title": "Model Context Protocol",
    "section": "MCP in action",
    "text": "MCP in action\nNow it‚Äôs time to implement a simple example using MCP! There are several ways to get started:\n\nFollow the official quickstart tutorial in the python-sdk repo\nCombine the quick-start guides for client and server developers.\nFind some other example implementation on stackoverflow, reddit, personal blogs, medium ect.\nAsk an LLM to generate a simple script or notebook to set up MCP for you!\n\nA comment on option 4 seems to be in order (this is a course on using generative AI, after all): It‚Äôs ok to use an LLM to create code, especially if it‚Äôs not your area of expertise. However, always double-check the generated code and make sure that you understand what each line does before running it! This has not only security implications but also practical ones. For example, you may have to explain your code to others. In this course, for instance, you will have to explain the code in your final project presentation to pass. Another word of warning against using LLM-generated code: It often works out of the box, but sometimes it does not. In this case, debugging LLM generated code often takes longer than writing it yourself from scratch!\n\n\n\n\n\n\nNoteüìù Task\n\n\n\nNow it‚Äôs your turn!\n\nSet up a simple MCP system in a notebook using a local LLM.\nIf you use LLM-generated code as a starting point, find code snippets the LLM generates, but does not use or is superfluous in some other manner.\nHave fun using one of the fancy servers from the repo\nUpload your notebook to Moodle as usual!",
    "crumbs": [
      "Other",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Model Context Protocol</span>"
    ]
  },
  {
    "objectID": "content/model_context_protocol.html#discussion",
    "href": "content/model_context_protocol.html#discussion",
    "title": "Model Context Protocol",
    "section": "Discussion",
    "text": "Discussion\nThe final question to discuss is the following:\nIs MCP worth spending my (valuable!) time on, or should I rather focus on other things?\nMCP is intended to be a standard for function calling in LLMs. It‚Äôs not clear yet if others will adopt this. And what worth is a standard if it doesn‚Äôt get adopted by the big players? Likewise, it is a framework for function calling, but htere are other frameworks out there that also facilitate that, e.g.¬†LangChain, Llamaindex, Haystack, Smolagents etc. Is MCP the next big thing or just another framework?\nNobody knows what the future holds. A good way of making an educated guess is to ask these three questions:\n\nWhat do you think. Is it a good idea?\nWhat do others think? Especially the big players are of interest here.\nWhere does the money come from? Who pays for this stuff? How does the company make money?\n\n\n\n\n\n\n\nNoteüìù Task\n\n\n\nTry to find out.\n\nTake some time googling around on the internet and see what you can find about MCP, its adoption by big players, who is behind it etc.\nDiscuss in the group! What do you think? Is this a good idea or not? Why?",
    "crumbs": [
      "Other",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Model Context Protocol</span>"
    ]
  },
  {
    "objectID": "content/model_context_protocol.html#references",
    "href": "content/model_context_protocol.html#references",
    "title": "Model Context Protocol",
    "section": "References",
    "text": "References\n\n\n\n\nModel Context Protocol. (n.d.). In Model Context Protocol. https://modelcontextprotocol.io/introduction.",
    "crumbs": [
      "Other",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Model Context Protocol</span>"
    ]
  },
  {
    "objectID": "content/finetuning_approaches.html",
    "href": "content/finetuning_approaches.html",
    "title": "Finetuning Approaches",
    "section": "",
    "text": "Full Finetuning\nFinetuning in terms of generative models means the general concept taking a pre-trained, ‚Äúfoundational‚Äù model and updating its parameters using new data. This data is usually much smaller than the data used to train the original model. The goal is to adapt the model to the new data while preserving as much of the knowledge it has already learned from the original training data. We have already seen an example of a finetuning approach when we were talking about instruct-tuned models Instruct-tuned models. These models are based on plain MLM-trained language models, that are then trained on new data that is presented in a Instruct - Response format. The result of this specific example of finetuning was a model that, instead of just completing a text, answered in the format present in the finetuning data.\nThough the central concept of finetuning is always the same, i.e., updating the parameters of a pre-trained model using new data, there are many different ways to do this. The following sections will give an overview of some of the most common approaches.\nFull finetuning is the simplest approach to finetuning. As the name says, it is based on completely updating the parameters of the pre-trained model using new data. This means that all weights of the model are updated during training using regular gradient descent or a variant thereof. The main advantage of this approach is that it is very simple and easy to implement. Complete (few-shot) fine-tuning has also shown to perform better in the domain of finetuning and in Out-of-domain tasks when compared to Few-Shot-Prompt-approaches (Mosbach et al., 2023). However, it also has some disadvantages.\nFirstly, it can be computationally expensive as it requires training all parameters of the model.\nSecondly, it can lead to catastrophic forgetting, i.e., the model forgets what it has learned during pre-training when adapting to new data (Luo et al., 2024).",
    "crumbs": [
      "Finetuning",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Finetuning Approaches</span>"
    ]
  },
  {
    "objectID": "content/finetuning_approaches.html#parameter-efficient-finetuning-peft",
    "href": "content/finetuning_approaches.html#parameter-efficient-finetuning-peft",
    "title": "Finetuning Approaches",
    "section": "Parameter-Efficient Finetuning (PEFT)",
    "text": "Parameter-Efficient Finetuning (PEFT)\nAnother approach to finetuning is to not update all a models parameters but to (partially) freeze them and only update a small subset of the parameters or to train an adaptor module that can be added to the model. This approach is called parameter-efficient fine-tuning (PEFT). The main advantage of PEFT is that it is much more computationally efficient than full finetuning as it only requires updating a small subset of the parameters. We will look at three different approaches to PEFT:\n\n\nPrompt-based Finetuning (Prefix-tuning and Prompt tuning)\n\nBonus: TextGrad\n\nAdapter-based finetuning (Low-Rank Adaptation and its relatives)\n(IA)¬≥ (Infused Adapter by Inhibiting and Amplifying Inner Activations)\n\n\nPrompt-based Finetuning\nPrompt-based finetuning is a family of methods that use so called ‚Äúsoft-prompts‚Äù to guide a models generation. The general concept is pretty close to prompting as we discussed it in Prompting. The main difference is that instead of engineering a prompt constructed from discrete tokens that results in opportune results, we let standard optimization procedures find a continuos embedding-vector in a pre-trained LMs embedding-space. Prefix-Tuning, Prompt Tuning and P-tuning are three different approaches to prompt-based finetuning - all utilizing some implementation of this soft-prompt concept.\n\nPrefix tuning\nPrefix-Tuning (Li & Liang, 2021) is a method of adapting a language model to a specific down-stream task by adding a continuous prefix vector to the input embeddings. This is done by learning a continuos matrix with a set amount of columns (i.e., tokens) and the frozen models embeddings-dimensionality1 that is prepended to the input of each transformer layer (i.e., the encoder and the decoder-stack). The principle is illustrated in Figure¬†12.1.\n1¬†Since directly learning the prefix-weights proved to result in unstable performance, the authors did not directly train prefix-vectors but a MLP scaling up from a smaller dimensionality to the embedding size. Since the rest of the proxy-model is discarded after training though, the method can be treated as the same principle.\n\n\n\n\n\nFig¬†12.1: Illustration of Prefix-tuning. A continuous prefix vector is learned and concatenated to the input embeddings before they are fed into the transformer layers. From Li & Liang (2021)\n\n\n\nThis vector can then be used to guide the model during inference. The main advantages of this method are\n\na small number of parameters that need to be learned and\nthe ability to quickly adapt to different tasks by simply switching out the prefix vector.\n\nSince the learned prefix-weights have to be prepended to each input though, one has to have access to the models internal representation during inference (at least for encoder-decoder-stacks). This is not always possible, especially when using black-box models like LLMs that are hosted on a remote server.\n\n\nPrompt-Tuning\nPrompt-tuning (Lester et al., 2021) is a method that is conceptually very similar to prefix-tuning, but avoids the need for accessing the internal representation of the model during inference by using what the authors call ‚Äúsoft prompts‚Äù. Again, instead of prompting using discrete tokens, continuous ‚Äúspecial tokens‚Äù are learned that are concatenated to the input embeddings. The main contribution of Prompt-Tuning over Prefix-Tuning is a) that they showed that inputting the soft-prompts to the encoder alone suffices and more importantly b) that the performance of models fine-tuned in this manner is comparable to full finetuning, at least for larger LLMs (Figure¬†12.2).\n\n\n\n\n\n\nFig¬†12.2: Results of Prompt-tuning compared to prompt-engineering and complete finetuning, taken from Lester et al. (2021)\n\n\n\n\n\n\n\n\n\nNoteüìù Task\n\n\n\nYour turn!\nThe huggingface-page on prompt-based finetuning describes three more variants of soft-prompt finetuning:\n\nP-Tuning\nMultitask prompt tuning\nContext-Aware prompt tuning\n\nSelect one of the three and try to answer the following questions in a markdown-file:\n\nWhat is the core principle?\nWhat is the context in which this tuning method is most efficient?\nHow much memory can be saved by leveraging this technique (if you can find this indication)\n\nPresent your results to the group. Upload your results to moodle.\n\n\n\n\nTextGrad\nYuksekgonul et al. (2025) described a method for text-based auto differentiation. The authors claim that, given a loss-target, their approach ‚ÄúTextGrad‚Äù allows to improve a model‚Äôs performance by directly tuning the discrete textual prompt used for generating the model‚Äôs answer across various tasks. This is done by implementing a system analogous to the autograd implementation in PyTorch (see Figure¬†12.3 for an illustration).\n\n\n\n\n\n\nFig¬†12.3: The TextGrad optimization schema, taken from Yuksekgonul et al. (2025)\n\n\n\nA ‚Äúloss‚Äù-function is defined, i.e., any function that takes the textual output and evaluates its quality. This can be a classic loss function like binary accuracy or a set of rules for evaluating text quality that are given to a language model to compare the result with the ground truth. This ‚Äúloss‚Äù is then taken into account by the ‚Äúoptimizer‚Äù, which is another LLM-Call that takes the output and loss and critiques the appropriate Variables (i.e., the initial prompt). This critique or ‚Äúgradient‚Äù as it is called in the Autograd analogy is then taken to update the initial prompt using another LLM-call (the step in Autograd). This process continues iteratively until the results are satisfactory or a predefined number of iterations is reached.\n\n\n\n\n\n\nNoteüìù Task\n\n\n\nYour turn!\nInstall TextGrad and get the tutorial on optimizing a solution to run using the following snippet to initialize your LMStudio/Ollama server:\nimport textgrad as tg\nfrom openai import OpenAI\nfrom textgrad.engine.local_model_openai_api import ChatExternalClient\n\nclient = OpenAI(base_url=\"&lt;your endpoint&gt;\", api_key=\"&lt;some key&gt;\") \nengine = ChatExternalClient(client=client, model_string=\"&lt;your model of choice&gt;\") \n\ntg.set_backward_engine(engine, override=True)\nIf this tutorial runs for you, adapt the code so that it generates the reasoning to the following riddle:\n\nA farmer with a wolf, a goat, and a cabbage must cross a river by boat. The boat can carry only the farmer and a single item. If left unattended together, the wolf would eat the goat, or the goat would eat the cabbage. How can they cross the river without anything being eaten? - Taken from wikipedia\n\nSolution: goat -&gt; empty -&gt; wolf -&gt; goat -&gt; cabbage -&gt; empty -&gt; goat\n\n\n\n\n\nAdapter-based finetuning\nInstead of focusing on the embeddings and thus the input of the language models, LoRA and its relatives focus on adapting the output of the attention and feed-forward layers of a transformer. The family of Low-Rank Adaptation (LoRA) methods (Hu et al., 2021) we will discuss here is a group of parameter-efficient fine-tuning techniques that adapt the models output by injecting trainable rank decomposition matrices into a transformers layer, greatly reducing the amount of parameters that need to be learned.\n\nLoRA (Low-Rank Adaptation)\nThe first and most common candidate of the group of LoRA-finetuning techniques is the name giver itself: Low-Rank Adaptation (LoRA). Hu et al. (2021) criticized soft-prompting methods as being hard to optimize2 and being dependent on reserving part of the input space for the prompt, effectively reducing the context window.\n2¬†As was also reported in Li & Liang (2021) in the context of their reported unstable learning.LoRA builds on the findings by Aghajanyan et al. (2020) that the intrinsic dimensionality of transformer layers is low, i.e., that there exists a lower dimensionality representation of the models parameters that suffices for an effective finetuning and thus only a few parameters are needed to adapt them. They show this by successfully finetuning a model on a random projection to a far smaller subspace without losing too much performance.\nThe central idea behind LoRA is that finetuning can be represented as learning the updates to the models parameter matrix \\(\\Delta W\\) so that the results of a fine-tuned generation \\(h\\) is based on the initial weights \\(W_0\\) and the update \\(\\Delta W\\):\n\\[\nh = W_0x + \\Delta Wx\n\\]\nBased on the idea of Aghajanyan et al. (2020), LoRA approximates this update matrix as the product of the lower-rank matrices \\(A\\) and \\(B\\), where \\(B \\in \\mathbb{R}^{d_{in} \\times r}\\), \\(A  \\in \\mathbb{R}^{r \\times d_{out}}\\) and \\(r &lt;&lt; d_{in}, d_{out}\\):\n\\[\nh = W_0x + \\Delta Wx = W_0x + BAx\n\\]\nA is initialized with random values sampled from a normal distribution and B is initialized as a zero matrix so that \\(\\Delta W\\) is zero at the start of the training.\nThis results in a reduction of the number of parameters to be trained from \\(d_{in} \\cdot d_{out}\\) to \\(d_{in} \\cdot r  + d_{out} \\cdot r\\) as is illustrated in Figure¬†12.4.\n\n\n\n\n\n\n\n\nFig¬†12.4: Illustration of the LoRA approximation of a weight matrix \\(\\Delta W\\) as the product of two lower-rank matrices \\(A\\) and \\(B\\). The rank of the approximation is \\(r &lt;&lt; d_{in}, d_{out}\\).\n\n\n\n\n\n\n\nQLoRA (Quantized Low-Rank Adaptation)\nQLoRA (Dettmers et al., 2023) builds on the concept of LoRA by further reducing the memory footprint and computational requirements. It does this, next do some other optimizations, by quantizing, i.e.¬†reducing the precision of, the frozen pretrained LLM. The process of quantization is illustrated in Figure¬†12.5.\n\n\n\n\n\n\nFig¬†12.5: Illustration of the result of quantization to 32, 16, 8 and 4 bits. The top of the image shows the same color-gradient under all quantizations, the bottom image is the quantized chapter-illustration.\n\n\n\nThey report a reduction of GPU-requirements for finetuning a 65B parameter model from more than 780GB VRAM to a measly number under 48 GB, allowing it to be finetuned in a single GPU. They also report performance values of up to 99.3% of the performance of ChatGPT on the vicuna benchmark3.\n3¬†which is now defunct and replaced by the MT-Bench score Chatbot Arena Leaderboard Week 8 (n.d.)\n\nX-LoRA (Mixture of Experts with LoRA)\nMixture of experts is a pretty old idea generally (Jacobs et al., 1991) and has been used in the context of Deep Learning and more specifically NLP for quite some time now (Shazeer et al., 2017). There are also some examples for recent LLMs that are utilizing the concept to achieve better performance, e.g. Jiang et al. (2024) The basic idea is to split a model into multiple smaller models, each of which is an expert on a specific topic. During inference, the input is routed to the expert that is most likely to be able to answer the question. This can be done by having a router-model that predicts the topic of the input and then routes it to the corresponding expert. This approach was applied to LoRA-based finetuning by (buehlerXLoRAMixtureLowRank2024a?) who propose X-LoRA, which is a mixture of experts that uses LoRA-finetuned models as experts. This is done by training a set of low rank adaptation matrices and using a router-model that predicts a scaling factor for each expert based on the input. The output of the model is then the weighted sum of the outputs of all experts. This scaling is done on a token-by-token basis, which allows a highly granular control over the output of the model.\n\n\nUnsloth\nUnsloth (Daniel Han & team, 2023) is a python-module that implements LoRA-finetuning in a very efficient way that further reduces raining resource requirements. This is mostly done by a far more efficient Gradient Descent algorithm that is specifically optimized for LoRA finetuning (‚ÄúIntroducing Unsloth,‚Äù n.d.).\nThey additionally introduced dynamic quantization to their models, which allows them to further reduce the memory footprint without losing too much performance. \n\n\n\n(IA)¬≥\nLiu et al. (2022) propose (IA)¬≥ (Infused Adapter by Inhibiting and Amplifying Inner Activations) which additionally builds on the central concepts of Soft Prompting and LoRA. Instead of learning additional tokens to prepend to the input or adaptation matrices for each layer, they propose the training of a small set of additional vectors that are used to item-wise rescale select hidden states of the model. A schematic illustration can be seen in Figure¬†12.6.\n\n\n\n\n\n\nFig¬†12.6: Illustration of the adaptation principle of (IA)¬≥. The input is passed through the model and then the selected hidden states are rescaled by the learned vectors. Q, K and V are the learned hidden weights for the queries, keys and values of a self-attention mechanism. The depiction on the right illustrates the adaptation of the weights of the feed-forward-part of a transformer. Image taken from Liu et al. (2022)\n\n\n\nThey also report their adaptation-strategy to work better and in a less resource-intensive way than LoRA and the other methods we have discussed so far, achieving higher accuracy with fewer parameters on their benchmark (Figure¬†12.7).\n\n\n\n\n\n\nFig¬†12.7: Performance of (IA)¬≥ compared to other parameter-efficient finetuning approaches. Image taken from Liu et al. (2022)\n\n\n\nAdditionally, they report a super-human performance of 75.8% on the RAFT, which provides only 50 training examples per task.",
    "crumbs": [
      "Finetuning",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Finetuning Approaches</span>"
    ]
  },
  {
    "objectID": "content/finetuning_approaches.html#further-readings",
    "href": "content/finetuning_approaches.html#further-readings",
    "title": "Finetuning Approaches",
    "section": "Further Readings",
    "text": "Further Readings\n\nThe huggingface-hub for PEFT-Methods is a great source to get an overview and a better hub to get to the original papers proposing the presented methods.\nThey also have a nice blogpost about MoE-models.\n\n\n\n\n\nAghajanyan, A., Zettlemoyer, L., & Gupta, S. (2020). Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning (arXiv:2012.13255). arXiv. https://doi.org/10.48550/arXiv.2012.13255\n\n\nChatbot Arena Leaderboard Week 8: Introducing MT-Bench and Vicuna-33B LMSYS Org. (n.d.). https://lmsys.org/blog/2023-06-22-leaderboard.\n\n\nDaniel Han, M. H., & team, U. (2023). Unsloth.\n\n\nDettmers, T., Pagnoni, A., Holtzman, A., & Zettlemoyer, L. (2023). QLoRA: Efficient Finetuning of Quantized LLMs (arXiv:2305.14314). arXiv. https://doi.org/10.48550/arXiv.2305.14314\n\n\nHu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., & Chen, W. (2021). LoRA: Low-Rank Adaptation of Large Language Models (arXiv:2106.09685). arXiv. https://doi.org/10.48550/arXiv.2106.09685\n\n\nIntroducing Unsloth. (n.d.). In Unsloth - Open source Fine-tuning for LLMs. https://unsloth.ai/introducing.\n\n\nJacobs, R. A., Jordan, M. I., Nowlan, S. J., & Hinton, G. E. (1991). Adaptive mixtures of local experts. Neural Computation, 3(1), 79‚Äì87.\n\n\nJiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D. S., Casas, D. de las, Hanna, E. B., Bressand, F., Lengyel, G., Bour, G., Lample, G., Lavaud, L. R., Saulnier, L., Lachaux, M.-A., Stock, P., Subramanian, S., Yang, S., ‚Ä¶ Sayed, W. E. (2024). Mixtral of Experts (arXiv:2401.04088). arXiv. https://doi.org/10.48550/arXiv.2401.04088\n\n\nLester, B., Al-Rfou, R., & Constant, N. (2021). The Power of Scale for Parameter-Efficient Prompt Tuning (arXiv:2104.08691). arXiv. https://doi.org/10.48550/arXiv.2104.08691\n\n\nLi, X. L., & Liang, P. (2021). Prefix-Tuning: Optimizing Continuous Prompts for Generation (arXiv:2101.00190). arXiv. https://doi.org/10.48550/arXiv.2101.00190\n\n\nLiu, H., Tam, D., Muqeeth, M., Mohta, J., Huang, T., Bansal, M., & Raffel, C. (2022). Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning (arXiv:2205.05638). arXiv. https://doi.org/10.48550/arXiv.2205.05638\n\n\nLuo, Y., Yang, Z., Meng, F., Li, Y., Zhou, J., & Zhang, Y. (2024). An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning (arXiv:2308.08747). arXiv. https://doi.org/10.48550/arXiv.2308.08747\n\n\nMosbach, M., Pimentel, T., Ravfogel, S., Klakow, D., & Elazar, Y. (2023). Few-shot Fine-tuning vs. In-context Learning: A Fair Comparison and Evaluation (arXiv:2305.16938). arXiv. https://doi.org/10.48550/arXiv.2305.16938\n\n\nShazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., & Dean, J. (2017). Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer (arXiv:1701.06538). arXiv. https://doi.org/10.48550/arXiv.1701.06538\n\n\nYuksekgonul, M., Bianchi, F., Boen, J., Liu, S., Lu, P., Huang, Z., Guestrin, C., & Zou, J. (2025). Optimizing generative AI by backpropagating language model feedback. Nature, 639(8055), 609‚Äì616. https://doi.org/10.1038/s41586-025-08661-4",
    "crumbs": [
      "Finetuning",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Finetuning Approaches</span>"
    ]
  },
  {
    "objectID": "content/alignment.html",
    "href": "content/alignment.html",
    "title": "Alignment",
    "section": "",
    "text": "Outer alignment\nWhen we were talking about finetuning, we were always looking at the following principle: We take a foundational model trained on a masked learning task1 that we want to adapt based on its general representation on language‚Äôs conditional probability distribution. As we discussed, this is based on new, specific datasets, that depict behavior we want a model to show. This can be for example the task we saw in Prefix tuning, where a model was finetuned on the parsing of tabular data. All finetuning approaches we have seen so far were based on some standard loss-function (i.e.¬†cross entropy) and optimized the model‚Äôs parameters to minimize this loss.\nAlignment is a specific approach to finetuning that aims to align the foundational models representation with human values and preferences. So we are still looking at adapting a pretrained model, but instead of using a standard loss-function, we use a reward function that measures how well the model‚Äôs output aligns with human values and preferences.\nThe general idea of aligning Artificial Intelligence with human goals and values is not new to LLMs but has long been the topic of research. Norbert Wiener, the originator of cybernetics, formulated the following observation in his paper reflecting the moral implications of automated systems with agency (Wiener, 1960)2:\nHe continues to usher the following warning about the alignment of a machine actors goals with human values:\nThese concerns laid the groundwork for modern discussions around the ethical challenges of AI alignment, particularly in systems with high autonomy and complexity. Due to the rapid pace at which modern generative models improve while being more and more complex - and thus harder to understand and control - these concerns are becoming increasingly relevant. (kirchnerResearchingAlignmentResearch2022a?) show a stark increase in research on alignment over the last years, as shown in Figure¬†13.1, with more specific sub-domains emerging as the field develops. The sharp increase in publications indicates a growing recognition of alignment as a critical area of research, with emerging sub-domains reflecting diverse approaches to addressing this challenge.\nIn the context of language or generative models, these values might include avoiding harmful outputs, the generation of helpful and harmless content, the adherence to a set of rules or the alignment with human preferences. For instance: A model should not generate instructions on how to build bombs or deepfakes of public figures, even if it would technically be able to do so.\nShen et al. (2023) define AI alignment itself as follows:\nWe will look at those two aspects into more detail in the following sections.\nBut first, we will try to get a feeling of the results of alignment:\nThe definition of a learning objective suitable for training or finetuning a model to act in accordance with human values is not trivial. In fact, it is an open research question. Instead of just using, as an example, cross-entropy loss to signify whether the predicted missing word is correct, evaluating a model‚Äôs output based on a set of human values is a good bit more complex.\nThis starts by the definition of these values, continues in the measurement of these values and does not end with the quantization of these measurements into a set of metrics that can be used to optimize a model. Additionally, there is the issue of target values becoming the victim of Goodhart‚Äôs Law which pretty much states:\nIn practice, a measurable proxy for safety, such as minimizing the frequency of certain harmful phrases, might lead the model to adopt undesirable shortcuts, such as refusing to answer questions entirely. The issue becomes even more evident when we consider alignment processes that involve human evaluations. Hicks et al. (2024)3 are arguing (very convincingly) that ChatGPT and other LLMs illustrate this challenge by generating texts that are optimized to sound convincing, regardless of their factual accuracy - making them outright bullshit machines. They base this argument on the following reference to the term of bullshit coined by Harry Frankfurt:\nThey go on to argue:\nOne could go further and argue that LLMs are unintentionally specifically trained and aligned to be bullshit generators. By using human feedback in the alignment process, specifically to tune a language model to get higher scores assigned by humans based on the factual accuracy of its output, we can find ourselves in a situation where a model is optimized to generate text that is more likely to be perceived as true by humans, regardless of whether it is actually true or if it actually means to deceit the rater into thinking that it sounds correct, just resulting in a higher grade of bullshit (Labs, 2023). This is especially the case where raters, that naturally can‚Äôt be experts in all fields, are asked to evaluate the factual accuracy of generated texts. They will increasingly need to rely on heuristics for rating the quality of texts, the higher the specificity of its topic.\nThis example highlights the importance of clearly defining alignment values ‚Äî such as honesty ‚Äî and developing robust ways to measure them. Without reliable metrics, optimization processes risk reinforcing outputs that meet surface-level heuristics while failing to align with deeper human values. The described behavior is an example of a model gaming the goal specification (Robert Miles AI Safety, 2020) and illustrates the crucial role of defining and measuring values in alignment research.\nSo, a first step towards aligning a model with human values is to define these values. Askell et al. (2021) propose the following targets for a LLM-assistant‚Äôs alignment:\nSuch a model should be\n(Askell et al., 2021, p. 44)\nThese optimization goals need to be then implemented in a fashion that make them traceable and measurable. There is a variety of approaches to do this, which get grouped by Shen et al. (2023) into the following categories:\nAn overview of these categories and methods that can be grouped thereunder is depicted in Figure¬†13.2. As with nearly all taxonomies, this one is not exhaustive and the boundaries between the categories are not always clear. Methods in the Non-recursive Oversight category are often used as a component of methods in the Scalable Oversight category.\nOuterAlignment\n\n\n\nOuterAlignment\n\nOuter Alignment\n\n\n\nNonRecursiveOversight\n\nNon-recursive Oversight\n\n\n\nOuterAlignment-&gt;NonRecursiveOversight\n\n\n\n\n\nScalableOversight\n\nScalable Oversight\n\n\n\nOuterAlignment-&gt;ScalableOversight\n\n\n\n\n\nRLBasedMethods\n\nRL-based Methods\n\n\n\nNonRecursiveOversight-&gt;RLBasedMethods\n\n\n\n\n\nSLBasedMethods\n\nSL-based Methods\n\n\n\nNonRecursiveOversight-&gt;SLBasedMethods\n\n\n\n\n\nTaskDecomposition\n\nTask Decomposition\n\n\n\nScalableOversight-&gt;TaskDecomposition\n\n\n\n\n\nConstitutionalAI\n\nConstitutional AI\n\n\n\nScalableOversight-&gt;ConstitutionalAI\n\n\n\n\n\nDebate\n\nDebate\n\n\n\nScalableOversight-&gt;Debate\n\n\n\n\n\nMarketMaking\n\nMarket Making\n\n\n\nScalableOversight-&gt;MarketMaking\n\n\n\n\n\nProxyTasks\n\nProxy Tasks\n\n\n\nScalableOversight-&gt;ProxyTasks\n\n\n\n\n\nRLHFVariants\n\nRLHF and Its Variants\n\n\n\nRLBasedMethods-&gt;RLHFVariants\n\n\n\n\n\nOtherRLMethods\n\nOther RL-based Methods\n\n\n\nRLBasedMethods-&gt;OtherRLMethods\n\n\n\n\n\nTextFeedback\n\nText-based Feedback Signals\n\n\n\nSLBasedMethods-&gt;TextFeedback\n\n\n\n\n\nRankingFeedback\n\nRanking-based Feedback Signals\n\n\n\nSLBasedMethods-&gt;RankingFeedback\n\n\n\n\n\n\n\n\nFig¬†13.2: An overview of outer alignment methods, based on Shen et al. (2023). Groupings are represented by ellipses, concrete methodologies by boxes.\nWe will first look at RLHF as one of if not the most common methods for outer alignment.",
    "crumbs": [
      "Finetuning",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Alignment</span>"
    ]
  },
  {
    "objectID": "content/alignment.html#references",
    "href": "content/alignment.html#references",
    "title": "Alignment",
    "section": "References",
    "text": "References\n\n\n\n\nAligning language models to follow instructions. (n.d.). https://openai.com/index/instruction-following/.\n\n\nAskell, A., Bai, Y., Chen, A., Drain, D., Ganguli, D., Henighan, T., Jones, A., Joseph, N., Mann, B., DasSarma, N., Elhage, N., Hatfield-Dodds, Z., Hernandez, D., Kernion, J., Ndousse, K., Olsson, C., Amodei, D., Brown, T., Clark, J., ‚Ä¶ Kaplan, J. (2021). A General Language Assistant as a Laboratory for Alignment (arXiv:2112.00861). arXiv. https://doi.org/10.48550/arXiv.2112.00861\n\n\nCasper, S., Davies, X., Shi, C., Gilbert, T. K., Scheurer, J., Rando, J., Freedman, R., Korbak, T., Lindner, D., Freire, P., Wang, T., Marks, S., Segerie, C.-R., Carroll, M., Peng, A., Christoffersen, P., Damani, M., Slocum, S., Anwar, U., ‚Ä¶ Hadfield-Menell, D. (2023). Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback (arXiv:2307.15217). arXiv. https://doi.org/10.48550/arXiv.2307.15217\n\n\nDu, Y., Li, S., Torralba, A., Tenenbaum, J. B., & Mordatch, I. (2023). Improving Factuality and Reasoning in Language Models through Multiagent Debate (arXiv:2305.14325). arXiv. https://doi.org/10.48550/arXiv.2305.14325\n\n\nHicks, M. T., Humphries, J., & Slater, J. (2024). ChatGPT is bullshit. Ethics and Information Technology, 26(2), 38. https://doi.org/10.1007/s10676-024-09775-5\n\n\nHubinger, E., Merwijk, C. van, Mikulik, V., Skalse, J., & Garrabrant, S. (2021). Risks from Learned Optimization in Advanced Machine Learning Systems (arXiv:1906.01820). arXiv. https://doi.org/10.48550/arXiv.1906.01820\n\n\nLabs, R. (2023). Can AI Alignment and Reinforcement Learning with Human Feedback (RLHF) Solve Web3 Issues? [Substack Newsletter]. In Ryze Labs.\n\n\nNg, A. Y., & Russell, S. (2000). Algorithms for inverse reinforcement learning. Icml, 1, 2.\n\n\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P. F., Leike, J., & Lowe, R. (2022). Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35, 27730‚Äì27744.\n\n\nRobert Miles AI Safety. (2020). 9 Examples of Specification Gaming.\n\n\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017). Proximal Policy Optimization Algorithms (arXiv:1707.06347). arXiv. https://doi.org/10.48550/arXiv.1707.06347\n\n\nShen, T., Jin, R., Huang, Y., Liu, C., Dong, W., Guo, Z., Wu, X., Liu, Y., & Xiong, D. (2023). Large Language Model Alignment: A Survey (arXiv:2309.15025). arXiv. https://doi.org/10.48550/arXiv.2309.15025\n\n\nWiener, N. (1960). Some Moral and Technical Consequences of Automation. Science, 131(3410), 1355‚Äì1358. https://doi.org/10.1126/science.131.3410.1355",
    "crumbs": [
      "Finetuning",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Alignment</span>"
    ]
  }
]