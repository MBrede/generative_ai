@misc{424127,
  title = {What Exactly Are Keys, Queries, and Values in Attention Mechanisms?},
  author = {(https://stats.stackexchange.com/users/95569/dontloo), dontloo},
  eprint = {https://stats.stackexchange.com/q/424127},
  howpublished = {Cross Validated}
}

@misc{AIAAICChatGPTTraining,
  title = {{AIAAIC - ChatGPT training emits 502 metric tons of carbon}},
  urldate = {2024-11-09},
  abstract = {ChatGPT training estimated to emit 502 metric tonnes of carbon},
  howpublished = {https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chatgpt-training-emits-502-metric-tons-of-carbon},
  langid = {ngerman},
  file = {/home/brede/Zotero/storage/9Z5XXA65/chatgpt-training-emits-502-metric-tons-of-carbon.html}
}

@misc{angelovTop2VecDistributedRepresentations2020a,
  title = {{{Top2Vec}}: {{Distributed Representations}} of {{Topics}}},
  shorttitle = {{{Top2Vec}}},
  author = {Angelov, Dimo},
  year = {2020},
  month = aug,
  number = {arXiv:2008.09470},
  eprint = {2008.09470},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2008.09470},
  urldate = {2023-02-08},
  abstract = {Topic modeling is used for discovering latent semantic structure, usually referred to as topics, in a large collection of documents. The most widely used methods are Latent Dirichlet Allocation and Probabilistic Latent Semantic Analysis. Despite their popularity they have several weaknesses. In order to achieve optimal results they often require the number of topics to be known, custom stop-word lists, stemming, and lemmatization. Additionally these methods rely on bag-of-words representation of documents which ignore the ordering and semantics of words. Distributed representations of documents and words have gained popularity due to their ability to capture semantics of words and documents. We present \${\textbackslash}texttt\{top2vec\}\$, which leverages joint document and word semantic embedding to find \${\textbackslash}textit\{topic vectors\}\$. This model does not require stop-word lists, stemming or lemmatization, and it automatically finds the number of topics. The resulting topic vectors are jointly embedded with the document and word vectors with distance between them representing semantic similarity. Our experiments demonstrate that \${\textbackslash}texttt\{top2vec\}\$ finds topics which are significantly more informative and representative of the corpus trained on than probabilistic generative models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/brede/Zotero/storage/G6ILNZ7B/Angelov - 2020 - Top2Vec Distributed Representations of Topics.pdf;/home/brede/Zotero/storage/7Z69E8EM/2008.html}
}

@article{bahdanauNeuralMachineTranslation2014,
  title = {Neural Machine Translation by Jointly Learning to Align and Translate},
  author = {Bahdanau, Dzmitry},
  year = {2014},
  journal = {arXiv preprint arXiv:1409.0473},
  eprint = {1409.0473},
  urldate = {2024-11-09},
  archiveprefix = {arXiv},
  file = {/home/brede/Zotero/storage/QAP6H8A8/Bahdanau - 2014 - Neural machine translation by jointly learning to align and translate.pdf}
}

@misc{bojanowskiEnrichingWordVectors2017,
  title = {Enriching {{Word Vectors}} with {{Subword Information}}},
  author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  year = {2017},
  month = jun,
  number = {arXiv:1607.04606},
  eprint = {1607.04606},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1607.04606},
  urldate = {2024-11-08},
  abstract = {Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character \$n\$-grams. A vector representation is associated to each character \$n\$-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/brede/Zotero/storage/9ZTXW5NC/Bojanowski et al. - 2017 - Enriching Word Vectors with Subword Information.pdf;/home/brede/Zotero/storage/8FS9UJYI/1607.html}
}

@inproceedings{bowmanLargeAnnotatedCorpus2015,
  title = {A Large Annotated Corpus for Learning Natural Language Inference},
  booktitle = {Proceedings of the 2015 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Bowman, Samuel R. and Angeli, Gabor and Potts, Christopher and Manning, Christopher D.},
  editor = {M{\`a}rquez, Llu{\'i}s and {Callison-Burch}, Chris and Su, Jian},
  year = {2015},
  month = sep,
  pages = {632--642},
  publisher = {Association for Computational Linguistics},
  address = {Lisbon, Portugal},
  doi = {10.18653/v1/D15-1075},
  urldate = {2024-11-16},
  file = {/home/brede/Zotero/storage/WBPFQ96X/Bowman et al. - 2015 - A large annotated corpus for learning natural language inference.pdf}
}

@misc{brownLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = {2020},
  month = jul,
  number = {arXiv:2005.14165},
  eprint = {2005.14165},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2005.14165},
  urldate = {2024-06-13},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/brede/Zotero/storage/CW2DPRGQ/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf;/home/brede/Zotero/storage/I2MGQ8UE/2005.html}
}

@misc{cerUniversalSentenceEncoder2018a,
  title = {Universal {{Sentence Encoder}}},
  author = {Cer, Daniel and Yang, Yinfei and Kong, Sheng-yi and Hua, Nan and Limtiaco, Nicole and John, Rhomni St and Constant, Noah and {Guajardo-Cespedes}, Mario and Yuan, Steve and Tar, Chris and Sung, Yun-Hsuan and Strope, Brian and Kurzweil, Ray},
  year = {2018},
  month = apr,
  number = {arXiv:1803.11175},
  eprint = {1803.11175},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1803.11175},
  urldate = {2023-08-31},
  abstract = {We present models for encoding sentences into embedding vectors that specifically target transfer learning to other NLP tasks. The models are efficient and result in accurate performance on diverse transfer tasks. Two variants of the encoding models allow for trade-offs between accuracy and compute resources. For both variants, we investigate and report the relationship between model complexity, resource consumption, the availability of transfer task training data, and task performance. Comparisons are made with baselines that use word level transfer learning via pretrained word embeddings as well as baselines do not use any transfer learning. We find that transfer learning using sentence embeddings tends to outperform word level transfer. With transfer learning via sentence embeddings, we observe surprisingly good performance with minimal amounts of supervised training data for a transfer task. We obtain encouraging results on Word Embedding Association Tests (WEAT) targeted at detecting model bias. Our pre-trained sentence encoding models are made freely available for download and on TF Hub.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/brede/Zotero/storage/4Q6S5D9M/Cer et al. - 2018 - Universal Sentence Encoder.pdf;/home/brede/Zotero/storage/GXMM6FIH/1803.html}
}

@misc{dettmersQLoRAEfficientFinetuning2023,
  title = {{{QLoRA}}: {{Efficient Finetuning}} of {{Quantized LLMs}}},
  shorttitle = {{{QLoRA}}},
  author = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  year = {2023},
  month = may,
  number = {arXiv:2305.14314},
  eprint = {2305.14314},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.14314},
  urldate = {2024-06-13},
  abstract = {We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters{\textasciitilde}(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3\% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/brede/Zotero/storage/QH8FN2H6/Dettmers et al. - 2023 - QLoRA Efficient Finetuning of Quantized LLMs.pdf;/home/brede/Zotero/storage/QZ3F8SMT/2305.html}
}

@misc{devlinBERTPretrainingDeep2019a,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  number = {arXiv:1810.04805},
  eprint = {1810.04805},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1810.04805},
  urldate = {2023-08-31},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/brede/Zotero/storage/BT3MA522/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf;/home/brede/Zotero/storage/HGPE94DD/1810.html}
}

@article{ganesanEmpiricalEvaluationPretrained2021,
  title = {Empirical {{Evaluation}} of {{Pre-trained Transformers}} for {{Human-Level NLP}}: {{The Role}} of {{Sample Size}} and {{Dimensionality}}},
  shorttitle = {Empirical {{Evaluation}} of {{Pre-trained Transformers}} for {{Human-Level NLP}}},
  author = {Ganesan, Adithya V and Matero, Matthew and Ravula, Aravind Reddy and Vu, Huy and Schwartz, H. Andrew},
  year = {2021},
  month = jun,
  journal = {Proceedings of the conference. Association for Computational Linguistics. North American Chapter. Meeting},
  volume = {2021},
  pages = {4515--4532},
  doi = {10.18653/v1/2021.naacl-main.357},
  urldate = {2024-05-23},
  abstract = {In human-level NLP tasks, such as predicting mental health, personality, or demographics, the number of observations is often smaller than the standard 768+ hidden state sizes of each layer within modern transformer-based language models, limiting the ability to effectively leverage transformers. Here, we provide a systematic study on the role of dimension reduction methods (principal components analysis, factorization techniques, or multi-layer auto-encoders) as well as the dimensionality of embedding vectors and sample sizes as a function of predictive performance. We first find that fine-tuning large models with a limited amount of data pose a significant difficulty which can be overcome with a pre-trained dimension reduction regime. RoBERTa consistently achieves top performance in human-level tasks, with PCA giving benefit over other reduction methods in better handling users that write longer texts. Finally, we observe that a majority of the tasks achieve results comparable to the best performance with just 112 of the embedding dimensions.},
  pmcid = {PMC8294338},
  pmid = {34296226},
  file = {/home/brede/Zotero/storage/ZCR443YX/Ganesan et al. - 2021 - Empirical Evaluation of Pre-trained Transformers f.pdf}
}

@misc{GoogleGemma2bHugging2024,
  title = {Google/Gemma-2b {$\cdot$} {{Hugging Face}}},
  year = {2024},
  month = may,
  urldate = {2024-06-04},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  howpublished = {https://huggingface.co/google/gemma-2b},
  file = {/home/brede/Zotero/storage/JYDJSZCE/gemma-2b.html}
}

@inproceedings{goyalComparativeAnalysisDifferent2022,
  title = {Comparative {{Analysis}} of {{Different Vectorizing Techniques}} for {{Document Similarity}} Using {{Cosine Similarity}}},
  booktitle = {2022 {{Second International Conference}} on {{Advanced Technologies}} in {{Intelligent Control}}, {{Environment}}, {{Computing}} \& {{Communication Engineering}} ({{ICATIECE}})},
  author = {Goyal, Kanav and Sharma, Megha},
  year = {2022},
  month = dec,
  pages = {1--5},
  doi = {10.1109/ICATIECE56365.2022.10046766},
  urldate = {2024-11-17},
  abstract = {In this paper, multiple methods to vectorize documents were compared, and cosine similarities were calculated for the corresponding documents. Some of the vectorizing methods also consider the text's semantic meaning. The methods involve cosine similarity with algorithms like Bag of Words, Binary Bag of Words, Tf-Idf, Bidirectional Encoder Representations from Transformers, and Universal Sentence Encoder. Two important libraries to preprocess the text were used; these are NLTK and Genism. The Binary bag of words with Genism gave the best results of all the methods used. The dataset used involved around 2000 short news articles; these belonged to 5 categories.},
  keywords = {Bit error rate,Cosine Similarity,F1 Score,Libraries,Precision,Recall,Semantics,Transformers,Vectorizing Methods},
  file = {/home/brede/Zotero/storage/JSMECNVT/Goyal and Sharma - 2022 - Comparative Analysis of Different Vectorizing Techniques for Document Similarity using Cosine Simila.pdf;/home/brede/Zotero/storage/CYADIHWM/10046766.html}
}

@inproceedings{guhrTrainingBroadCoverageGerman2020a,
  title = {Training a {{Broad-Coverage German Sentiment Classification Model}} for {{Dialog Systems}}},
  booktitle = {Proceedings of the {{Twelfth Language Resources}} and {{Evaluation Conference}}},
  author = {Guhr, Oliver and Schumann, Anne-Kathrin and Bahrmann, Frank and B{\"o}hme, Hans Joachim},
  year = {2020},
  month = may,
  pages = {1627--1632},
  publisher = {European Language Resources Association},
  address = {Marseille, France},
  urldate = {2023-02-08},
  abstract = {This paper describes the training of a general-purpose German sentiment classification model. Sentiment classification is an important aspect of general text analytics. Furthermore, it plays a vital role in dialogue systems and voice interfaces that depend on the ability of the system to pick up and understand emotional signals from user utterances. The presented study outlines how we have collected a new German sentiment corpus and then combined this corpus with existing resources to train a broad-coverage German sentiment model. The resulting data set contains 5.4 million labelled samples. We have used the data to train both, a simple convolutional and a transformer-based classification model and compared the results achieved on various training configurations. The model and the data set will be published along with this paper.},
  isbn = {979-10-95546-34-4},
  langid = {english},
  file = {/home/brede/Zotero/storage/DDEMGL9V/Guhr et al. - 2020 - Training a Broad-Coverage German Sentiment Classif.pdf}
}

@misc{heidloffFinetuningSmallLLMs2023,
  title = {Fine-Tuning Small {{LLMs}} with {{Output}} from Large {{LLMs}}},
  author = {Heidloff, Niklas},
  year = {2023},
  month = aug,
  journal = {Niklas Heidloff},
  urldate = {2024-11-10},
  abstract = {The innovation in the AI space is continuing in an incredible speed. This post describes a new technique that has evolved recently which allows smaller fine-tuned models to almost reach the performance of much larger models.},
  howpublished = {https://heidloff.net/article/fine-tune-small-llm-with-big-llm/},
  langid = {english},
  file = {/home/brede/Zotero/storage/E6E4RWCA/fine-tune-small-llm-with-big-llm.html}
}

@misc{heidloffFoundationModelsTransformers2023,
  title = {Foundation {{Models}}, {{Transformers}}, {{BERT}} and {{GPT}}},
  author = {Heidloff, Niklas},
  year = {2023},
  month = feb,
  journal = {Niklas Heidloff},
  urldate = {2024-11-09},
  abstract = {Since I'm excited by the incredible capabilities which technologies like ChatGPT and Bard provide, I'm trying to understand better how they work. This post summarizes my current understanding about foundation models, transformers, BERT and GPT.},
  howpublished = {https://heidloff.net/article/foundation-models-transformers-bert-and-gpt/},
  langid = {english},
  file = {/home/brede/Zotero/storage/DT2LB8RE/foundation-models-transformers-bert-and-gpt.html}
}

@misc{hoffmannTrainingComputeOptimalLarge2022a,
  title = {Training {{Compute-Optimal Large Language Models}}},
  author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and van den Driessche, George and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Rae, Jack W. and Vinyals, Oriol and Sifre, Laurent},
  year = {2022},
  month = mar,
  number = {arXiv:2203.15556},
  eprint = {2203.15556},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.15556},
  urldate = {2023-08-31},
  abstract = {We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4\${\textbackslash}times\$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5\% on the MMLU benchmark, greater than a 7\% improvement over Gopher.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/brede/Zotero/storage/NR86D9MN/Hoffmann et al. - 2022 - Training Compute-Optimal Large Language Models.pdf;/home/brede/Zotero/storage/NP6PY7HV/2203.html}
}

@misc{hsiehDistillingStepStepOutperforming2023,
  title = {Distilling {{Step-by-Step}}! {{Outperforming Larger Language Models}} with {{Less Training Data}} and {{Smaller Model Sizes}}},
  author = {Hsieh, Cheng-Yu and Li, Chun-Liang and Yeh, Chih-Kuan and Nakhost, Hootan and Fujii, Yasuhisa and Ratner, Alexander and Krishna, Ranjay and Lee, Chen-Yu and Pfister, Tomas},
  year = {2023},
  month = jul,
  number = {arXiv:2305.02301},
  eprint = {2305.02301},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.02301},
  urldate = {2024-11-09},
  abstract = {Deploying large language models (LLMs) is challenging because they are memory inefficient and compute-intensive for practical applications. In reaction, researchers train smaller task-specific models by either finetuning with human labels or distilling using LLM-generated labels. However, finetuning and distillation require large amounts of training data to achieve comparable performance to LLMs. We introduce Distilling step-by-step, a new mechanism that (a) trains smaller models that outperform LLMs, and (b) achieves so by leveraging less training data needed by finetuning or distillation. Our method extracts LLM rationales as additional supervision for training small models within a multi-task framework. We present three findings across 4 NLP benchmarks: First, compared to both finetuning and distillation, our mechanism achieves better performance with much fewer labeled/unlabeled training examples. Second, compared to few-shot prompted LLMs, we achieve better performance using substantially smaller model sizes. Third, we reduce both the model size and the amount of data required to outperform LLMs; our finetuned 770M T5 model outperforms the few-shot prompted 540B PaLM model using only 80\% of available data on a benchmark, whereas standard finetuning the same T5 model struggles to match even by using 100\% of the dataset. We release the code at: https://github.com/google-research/distilling-step-by-step .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/brede/Zotero/storage/PLQRFHEI/Hsieh et al. - 2023 - Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Mo.pdf;/home/brede/Zotero/storage/HYUSD5UD/2305.html}
}

@misc{huLoRALowRankAdaptation2021,
  title = {{{LoRA}}: {{Low-Rank Adaptation}} of {{Large Language Models}}},
  shorttitle = {{{LoRA}}},
  author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and {Allen-Zhu}, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  year = {2021},
  month = oct,
  number = {arXiv:2106.09685},
  eprint = {2106.09685},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2106.09685},
  urldate = {2024-06-13},
  abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/brede/Zotero/storage/WVIRCL8W/Hu et al. - 2021 - LoRA Low-Rank Adaptation of Large Language Models.pdf;/home/brede/Zotero/storage/MZ92BU6Z/2106.html}
}

@article{hussainTutorialOpensourceLarge2024,
  title = {A Tutorial on Open-Source Large Language Models for Behavioral Science},
  author = {Hussain, Zak and Binz, Marcel and Mata, Rui and Wulff, Dirk U.},
  year = {2024},
  month = aug,
  journal = {Behavior Research Methods},
  volume = {56},
  number = {8},
  pages = {8214--8237},
  issn = {1554-3528},
  doi = {10.3758/s13428-024-02455-8},
  urldate = {2024-11-09},
  abstract = {Abstract                            Large language models (LLMs) have the potential to revolutionize behavioral science by accelerating and improving the research cycle, from conceptualization to data analysis. Unlike closed-source solutions, open-source frameworks for LLMs can enable transparency, reproducibility, and adherence to data protection standards, which gives them a crucial advantage for use in behavioral science. To help researchers harness the promise of LLMs, this tutorial offers a primer on the open-source Hugging Face ecosystem and demonstrates several applications that advance conceptual and empirical work in behavioral science, including feature extraction, fine-tuning of models for prediction, and generation of behavioral responses. Executable code is made available at               github.com/Zak-Hussain/LLM4BeSci.git               . Finally, the tutorial discusses challenges faced by research with (open-source) LLMs related to interpretability and safety and offers a perspective on future research at the intersection of language modeling and behavioral science.},
  langid = {english},
  file = {/home/brede/Zotero/storage/JTIT3SZS/Hussain et al. - 2024 - A tutorial on open-source large language models for behavioral science.pdf}
}

@misc{IntroductionLLMAgents2023,
  title = {Introduction to {{LLM Agents}}},
  year = {2023},
  month = nov,
  journal = {NVIDIA Technical Blog},
  urldate = {2024-11-17},
  abstract = {Consider a large language model (LLM) application that is designed to help financial analysts answer questions about the performance of a company. With a well-designed retrieval augmented generation{\dots}},
  howpublished = {https://developer.nvidia.com/blog/introduction-to-llm-agents/},
  langid = {american},
  file = {/home/brede/Zotero/storage/PZDM8SZ7/introduction-to-llm-agents.html}
}

@misc{jiangMoRAHighRankUpdating2024,
  title = {{{MoRA}}: {{High-Rank Updating}} for {{Parameter-Efficient Fine-Tuning}}},
  shorttitle = {{{MoRA}}},
  author = {Jiang, Ting and Huang, Shaohan and Luo, Shengyue and Zhang, Zihan and Huang, Haizhen and Wei, Furu and Deng, Weiwei and Sun, Feng and Zhang, Qi and Wang, Deqing and Zhuang, Fuzhen},
  year = {2024},
  month = may,
  number = {arXiv:2405.12130},
  eprint = {2405.12130},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.12130},
  urldate = {2024-06-13},
  abstract = {Low-rank adaptation is a popular parameter-efficient fine-tuning method for large language models. In this paper, we analyze the impact of low-rank updating, as implemented in LoRA. Our findings suggest that the low-rank updating mechanism may limit the ability of LLMs to effectively learn and memorize new knowledge. Inspired by this observation, we propose a new method called MoRA, which employs a square matrix to achieve high-rank updating while maintaining the same number of trainable parameters. To achieve it, we introduce the corresponding non-parameter operators to reduce the input dimension and increase the output dimension for the square matrix. Furthermore, these operators ensure that the weight can be merged back into LLMs, which makes our method can be deployed like LoRA. We perform a comprehensive evaluation of our method across five tasks: instruction tuning, mathematical reasoning, continual pretraining, memory and pretraining. Our method outperforms LoRA on memory-intensive tasks and achieves comparable performance on other tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/brede/Zotero/storage/LRHDEERF/Jiang et al. - 2024 - MoRA High-Rank Updating for Parameter-Efficient F.pdf;/home/brede/Zotero/storage/IRGF8SXT/2405.html}
}

@misc{jiangScalingSentenceEmbeddings2023,
  title = {Scaling {{Sentence Embeddings}} with {{Large Language Models}}},
  author = {Jiang, Ting and Huang, Shaohan and Luan, Zhongzhi and Wang, Deqing and Zhuang, Fuzhen},
  year = {2023},
  month = jul,
  number = {arXiv:2307.16645},
  eprint = {2307.16645},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.16645},
  urldate = {2024-02-12},
  abstract = {Large language models (LLMs) have recently garnered significant interest. With in-context learning, LLMs achieve impressive results in various natural language tasks. However, the application of LLMs to sentence embeddings remains an area of ongoing research. In this work, we propose an in-context learning-based method aimed at improving sentence embeddings performance. Our approach involves adapting the previous prompt-based representation method for autoregressive models, constructing a demonstration set that enables LLMs to perform in-context learning, and scaling up the LLMs to different model sizes. Through extensive experiments, in-context learning enables LLMs to generate high-quality sentence embeddings without any fine-tuning. It helps LLMs achieve performance comparable to current contrastive learning methods. By scaling model size, we find scaling to more than tens of billion parameters harms the performance on semantic textual similarity (STS) tasks. However, the largest model outperforms other counterparts and achieves the new state-of-the-art result on transfer tasks. We also fine-tune LLMs with current contrastive learning approach, and the 2.7B OPT model, incorporating our prompt-based method, surpasses the performance of 4.8B ST5, achieving the new state-of-the-art results on STS tasks. Our code is available at https://github.com/kongds/scaling\_sentemb.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{jiangScalingSentenceEmbeddings2023a,
  title = {Scaling {{Sentence Embeddings}} with {{Large Language Models}}},
  author = {Jiang, Ting and Huang, Shaohan and Luan, Zhongzhi and Wang, Deqing and Zhuang, Fuzhen},
  year = {2023},
  month = jul,
  number = {arXiv:2307.16645},
  eprint = {2307.16645},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.16645},
  urldate = {2024-02-12},
  abstract = {Large language models (LLMs) have recently garnered significant interest. With in-context learning, LLMs achieve impressive results in various natural language tasks. However, the application of LLMs to sentence embeddings remains an area of ongoing research. In this work, we propose an in-context learning-based method aimed at improving sentence embeddings performance. Our approach involves adapting the previous prompt-based representation method for autoregressive models, constructing a demonstration set that enables LLMs to perform in-context learning, and scaling up the LLMs to different model sizes. Through extensive experiments, in-context learning enables LLMs to generate high-quality sentence embeddings without any fine-tuning. It helps LLMs achieve performance comparable to current contrastive learning methods. By scaling model size, we find scaling to more than tens of billion parameters harms the performance on semantic textual similarity (STS) tasks. However, the largest model outperforms other counterparts and achieves the new state-of-the-art result on transfer tasks. We also fine-tune LLMs with current contrastive learning approach, and the 2.7B OPT model, incorporating our prompt-based method, surpasses the performance of 4.8B ST5, achieving the new state-of-the-art results on STS tasks. Our code is available at https://github.com/kongds/scaling\_sentemb.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/brede/Zotero/storage/C5KPRQVM/Jiang et al. - 2023 - Scaling Sentence Embeddings with Large Language Mo.pdf;/home/brede/Zotero/storage/HIEBLRMP/2307.html}
}

@misc{JphmeEm_german_13b_v01Hugging2023a,
  title = {Jphme/Em\_german\_13b\_v01 {$\cdot$} {{Hugging Face}}},
  year = {2023},
  month = oct,
  urldate = {2023-12-12},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  howpublished = {https://huggingface.co/jphme/em\_german\_13b\_v01},
  file = {/home/brede/Zotero/storage/TBF3ZQRN/em_german_13b_v01.html}
}

@misc{kaplanScalingLawsNeural2020a,
  title = {Scaling {{Laws}} for {{Neural Language Models}}},
  author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  year = {2020},
  month = jan,
  number = {arXiv:2001.08361},
  eprint = {2001.08361},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2001.08361},
  urldate = {2023-08-31},
  abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/brede/Zotero/storage/S5ZRP7TH/Kaplan et al. - 2020 - Scaling Laws for Neural Language Models.pdf;/home/brede/Zotero/storage/NH6YTF7K/2001.html}
}

@inproceedings{kentonBertPretrainingDeep2019,
  title = {Bert: {{Pre-training}} of Deep Bidirectional Transformers for Language Understanding},
  shorttitle = {Bert},
  booktitle = {Proceedings of {{naacL-HLT}}},
  author = {Kenton, Jacob Devlin Ming-Wei Chang and Toutanova, Lee Kristina},
  year = {2019},
  volume = {1},
  pages = {2},
  publisher = {Minneapolis, Minnesota},
  urldate = {2024-11-02},
  file = {/home/brede/Zotero/storage/W9QRM6I7/Kenton and Toutanova - 2019 - Bert Pre-training of deep bidirectional transformers for language understanding.pdf}
}

@article{klingerAutomaticEmotionDetection2016a,
  title = {Automatic {{Emotion Detection}} for {{Quantitative Literary Studies}}},
  author = {Klinger, Roman and Suliya, Surayya Samat and Reiter, Nils},
  year = {2016},
  file = {/home/brede/Zotero/storage/SZNRK78C/Klinger et al. - 2016 - Automatic Emotion Detection for antitative Literar.pdf}
}

@misc{kumarSelectiveFinetuningLLMlabeled2024,
  title = {Selective {{Fine-tuning}} on {{LLM-labeled Data May Reduce Reliance}} on {{Human Annotation}}: {{A Case Study Using Schedule-of-Event Table Detection}}},
  shorttitle = {Selective {{Fine-tuning}} on {{LLM-labeled Data May Reduce Reliance}} on {{Human Annotation}}},
  author = {Kumar, Bhawesh and Amar, Jonathan and Yang, Eric and Li, Nan and Jia, Yugang},
  year = {2024},
  month = aug,
  number = {arXiv:2405.06093},
  eprint = {2405.06093},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.06093},
  urldate = {2024-11-10},
  abstract = {Large Language Models (LLMs) have demonstrated their efficacy across a broad spectrum of tasks in healthcare applications. However, often LLMs need to be fine-tuned on task-specific expert annotated data to achieve optimal performance, which can be expensive and time consuming. In this study, we fine-tune PaLM-2 with parameter efficient fine-tuning (PEFT) using noisy labels obtained from gemini-pro 1.0 for the detection of Schedule-of-Event (SoE) tables, which specify care plan in clinical trial protocols. We introduce a filtering mechanism to select high-confidence labels for this table classification task, thereby reducing the noise in the auto-generated labels. We show that fine-tuned PaLM-2 with those labels achieves performance that exceeds the gemini-pro 1.0 and other LLMs. Furthermore, its performance is close to a PaLM-2 fine-tuned on labels obtained from non-expert annotators. Our results show that leveraging LLM-generated labels through powerful models like gemini-pro can potentially serve as a viable strategy for improving LLM performance through fine-tuning in specialized tasks, particularly in domains where expert annotations are scarce, expensive, or time-consuming to obtain.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/brede/Zotero/storage/IPVYRZBU/Kumar et al. - 2024 - Selective Fine-tuning on LLM-labeled Data May Reduce Reliance on Human Annotation A Case Study Usin.pdf;/home/brede/Zotero/storage/M3VJ9XRL/2405.html}
}

@misc{kuratovBABILongTestingLimits2024,
  title = {{{BABILong}}: {{Testing}} the {{Limits}} of {{LLMs}} with {{Long Context Reasoning-in-a-Haystack}}},
  shorttitle = {{{BABILong}}},
  author = {Kuratov, Yuri and Bulatov, Aydar and Anokhin, Petr and Rodkin, Ivan and Sorokin, Dmitry and Sorokin, Artyom and Burtsev, Mikhail},
  year = {2024},
  month = nov,
  number = {arXiv:2406.10149},
  eprint = {2406.10149},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.10149},
  urldate = {2024-11-12},
  abstract = {In recent years, the input context sizes of large language models (LLMs) have increased dramatically. However, existing evaluation methods have not kept pace, failing to comprehensively assess the efficiency of models in handling long contexts. To bridge this gap, we introduce the BABILong benchmark, designed to test language models' ability to reason across facts distributed in extremely long documents. BABILong includes a diverse set of 20 reasoning tasks, including fact chaining, simple induction, deduction, counting, and handling lists/sets. These tasks are challenging on their own, and even more demanding when the required facts are scattered across long natural text. Our evaluations show that popular LLMs effectively utilize only 10-20{\textbackslash}\% of the context and their performance declines sharply with increased reasoning complexity. Among alternatives to in-context reasoning, Retrieval-Augmented Generation methods achieve a modest 60{\textbackslash}\% accuracy on single-fact question answering, independent of context length. Among context extension methods, the highest performance is demonstrated by recurrent memory transformers after fine-tuning, enabling the processing of lengths up to 50 million tokens. The BABILong benchmark is extendable to any length to support the evaluation of new upcoming models with increased capabilities, and we provide splits up to 10 million token lengths.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/brede/Zotero/storage/2G3IDYWX/Kuratov et al. - 2024 - BABILong Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack.pdf;/home/brede/Zotero/storage/JZLP98WJ/2406.html}
}

@misc{LateChunkingLongContext2024,
  title = {Late {{Chunking}} in {{Long-Context Embedding Models}}},
  year = {2024},
  month = aug,
  urldate = {2024-11-17},
  abstract = {Chunking long documents while preserving contextual information is challenging. We introduce the "Late Chunking" that leverages long-context embedding models to generate contextual chunk embeddings for better retrieval applications.},
  howpublished = {https://jina.ai/news/late-chunking-in-long-context-embedding-models},
  langid = {american},
  file = {/home/brede/Zotero/storage/2WVMQYN8/late-chunking-in-long-context-embedding-models.html}
}

@inproceedings{leDistributedRepresentationsSentences2014a,
  title = {Distributed {{Representations}} of {{Sentences}} and {{Documents}}},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Machine Learning}}},
  author = {Le, Quoc and Mikolov, Tomas},
  year = {2014},
  month = jun,
  pages = {1188--1196},
  publisher = {PMLR},
  issn = {1938-7228},
  urldate = {2023-02-18},
  abstract = {Many machine learning algorithms require the  input to be represented as a fixed length feature  vector. When it comes to texts, one of the most  common representations is bag-of-words. Despite their popularity, bag-of-words models have  two major weaknesses: they lose the ordering  of the words and they also ignore semantics of  the words. For example, "powerful," "strong"  and "Paris" are equally distant. In this paper,  we propose an unsupervised algorithm that learns  vector representations of sentences and text documents. This algorithm represents each document by a dense vector which is trained to predict  words in the document. Its construction gives our  algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that our technique outperforms bag-of-words models as well as other techniques for  text representations. Finally, we achieve new  state-of-the-art results on several text classification and sentiment analysis tasks.},
  langid = {english},
  file = {/home/brede/Zotero/storage/NNBN9D86/Le and Mikolov - 2014 - Distributed Representations of Sentences and Docum.pdf}
}

@misc{liuRoBERTaRobustlyOptimized2019a,
  title = {{{RoBERTa}}: {{A Robustly Optimized BERT Pretraining Approach}}},
  shorttitle = {{{RoBERTa}}},
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  year = {2019},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1907.11692},
  urldate = {2024-06-04},
  abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences}
}

@misc{LLMAgentsNextra2024,
  title = {{{LLM Agents}} -- {{Nextra}}},
  year = {2024},
  month = oct,
  urldate = {2024-11-17},
  abstract = {A Comprehensive Overview of Prompt Engineering},
  howpublished = {https://www.promptingguide.ai/research/llm-agents},
  langid = {english},
  file = {/home/brede/Zotero/storage/ZBACM4LD/llm-agents.html}
}

@misc{longLargeLanguageModel2023,
  title = {Large {{Language Model Guided Tree-of-Thought}}},
  author = {Long, Jieyi},
  year = {2023},
  month = may,
  number = {arXiv:2305.08291},
  eprint = {2305.08291},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-17},
  abstract = {In this paper, we introduce the Tree-of-Thought (ToT) framework, a novel approach aimed at improving the problem-solving capabilities of auto-regressive large language models (LLMs). The ToT technique is inspired by the human mind's approach for solving complex reasoning tasks through trial and error. In this process, the human mind explores the solution space through a tree-like thought process, allowing for backtracking when necessary. To implement ToT as a software system, we augment an LLM with additional modules including a prompter agent, a checker module, a memory module, and a ToT controller. In order to solve a given problem, these modules engage in a multi-round conversation with the LLM. The memory module records the conversation and state history of the problem solving process, which allows the system to backtrack to the previous steps of the thought-process and explore other directions from there. To verify the effectiveness of the proposed technique, we implemented a ToT-based solver for the Sudoku Puzzle. Experimental results show that the ToT framework can significantly increase the success rate of Sudoku puzzle solving. Our implementation of the ToT-based Sudoku solver is available on GitHub: https://github.com/jieyilong/tree-of-thought-puzzle-solver.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/brede/Zotero/storage/KETV5VD5/Long - 2023 - Large Language Model Guided Tree-of-Thought.pdf}
}

@misc{mikolovEfficientEstimationWord2013,
  title = {Efficient {{Estimation}} of {{Word Representations}} in {{Vector Space}}},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  year = {2013},
  month = sep,
  number = {arXiv:1301.3781},
  eprint = {1301.3781},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1301.3781},
  urldate = {2024-11-08},
  abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/brede/Zotero/storage/IMPU3CG6/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Vector Space.pdf;/home/brede/Zotero/storage/R7HKK4MJ/1301.html}
}

@misc{MistralaiMixtral8x7BInstructv01Hugginga,
  title = {Mistralai/{{Mixtral-8x7B-Instruct-v0}}.1 {$\cdot$} {{Hugging Face}}},
  urldate = {2024-06-04},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  howpublished = {https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1}
}

@misc{petersDeepContextualizedWord2018,
  title = {Deep Contextualized Word Representations},
  author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  year = {2018},
  month = mar,
  number = {arXiv:1802.05365},
  eprint = {1802.05365},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1802.05365},
  urldate = {2024-11-08},
  abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/brede/Zotero/storage/6ZR8KTCS/Peters et al. - 2018 - Deep contextualized word representations.pdf;/home/brede/Zotero/storage/CIK2YYEM/1802.html}
}

@article{radfordImprovingLanguageUnderstanding2018,
  title = {Improving Language Understanding with Unsupervised Learning},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year = {2018},
  publisher = {Technical report, OpenAI}
}

@article{radfordLanguageModelsAre2019,
  title = {Language Models Are Unsupervised Multitask Learners},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year = {2019},
  journal = {OpenAI blog},
  volume = {1},
  number = {8},
  pages = {9},
  urldate = {2024-06-13}
}

@misc{reimersMakingMonolingualSentence2020a,
  title = {Making {{Monolingual Sentence Embeddings Multilingual}} Using {{Knowledge Distillation}}},
  author = {Reimers, Nils and Gurevych, Iryna},
  year = {2020},
  month = oct,
  number = {arXiv:2004.09813},
  eprint = {2004.09813},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2004.09813},
  urldate = {2023-02-08},
  abstract = {We present an easy and efficient method to extend existing sentence embedding models to new languages. This allows to create multilingual versions from previously monolingual models. The training is based on the idea that a translated sentence should be mapped to the same location in the vector space as the original sentence. We use the original (monolingual) model to generate sentence embeddings for the source language and then train a new system on translated sentences to mimic the original model. Compared to other methods for training multilingual sentence embeddings, this approach has several advantages: It is easy to extend existing models with relatively few samples to new languages, it is easier to ensure desired properties for the vector space, and the hardware requirements for training is lower. We demonstrate the effectiveness of our approach for 50+ languages from various language families. Code to extend sentence embeddings models to more than 400 languages is publicly available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/brede/Zotero/storage/LGJQP756/Reimers and Gurevych - 2020 - Making Monolingual Sentence Embeddings Multilingua.pdf;/home/brede/Zotero/storage/EZWPSH48/2004.html}
}

@misc{reimersSentenceBERTSentenceEmbeddings2019a,
  title = {Sentence-{{BERT}}: {{Sentence Embeddings}} Using {{Siamese BERT-Networks}}},
  shorttitle = {Sentence-{{BERT}}},
  author = {Reimers, Nils and Gurevych, Iryna},
  year = {2019},
  month = aug,
  number = {arXiv:1908.10084},
  eprint = {1908.10084},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1908.10084},
  urldate = {2023-08-31},
  abstract = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textasciitilde}65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/brede/Zotero/storage/S6BJQH72/Reimers and Gurevych - 2019 - Sentence-BERT Sentence Embeddings using Siamese B.pdf;/home/brede/Zotero/storage/HFIUQF7D/1908.html}
}

@inproceedings{remusSentiWSPubliclyAvailableGermanlanguage2010,
  title = {{{SentiWS-A Publicly Available German-language Resource}} for {{Sentiment Analysis}}},
  booktitle = {Proceedings of the {{Seventh International Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}}'10)},
  author = {Remus, Robert and Quasthoff, Uwe and Heyer, Gerhard},
  year = {2010},
  file = {/home/brede/Zotero/storage/WRVPFV3C/L10-1339.html}
}

@misc{sardanaChinchillaOptimalAccountingInference2024,
  title = {Beyond {{Chinchilla-Optimal}}: {{Accounting}} for {{Inference}} in {{Language Model Scaling Laws}}},
  shorttitle = {Beyond {{Chinchilla-Optimal}}},
  author = {Sardana, Nikhil and Portes, Jacob and Doubov, Sasha and Frankle, Jonathan},
  year = {2024},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2401.00448},
  urldate = {2024-11-08},
  abstract = {Large language model (LLM) scaling laws are empirical formulas that estimate changes in model quality as a result of increasing parameter count and training data. However, these formulas, including the popular Deepmind Chinchilla scaling laws, neglect to include the cost of inference. We modify the Chinchilla scaling laws to calculate the optimal LLM parameter count and pre-training data size to train and deploy a model of a given quality and inference demand. We conduct our analysis both in terms of a compute budget and real-world costs and find that LLM researchers expecting reasonably large inference demand ({\textasciitilde}1B requests) should train models smaller and longer than Chinchilla-optimal. Furthermore, we train 47 models of varying sizes and parameter counts to validate our formula and find that model quality continues to improve as we scale tokens per parameter to extreme ranges (up to 10,000). Finally, we ablate the procedure used to fit the Chinchilla scaling law coefficients and find that developing scaling laws only from data collected at typical token/parameter ratios overestimates the impact of additional tokens at these extreme ranges.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences,Machine Learning (cs.LG)}
}

@misc{SemanticSimilarityText,
  title = {Semantic {{Similarity}} for {{Text Comparison}} between {{Textual Documents}} or {{Sentences}} {\textbar} {{IEEE Conference Publication}} {\textbar} {{IEEE Xplore}}},
  urldate = {2024-11-17},
  howpublished = {https://ieeexplore.ieee.org/document/10465440},
  file = {/home/brede/Zotero/storage/HXVWBY9N/10465440.html}
}

@misc{shinnReflexionLanguageAgents2023,
  title = {Reflexion: {{Language Agents}} with {{Verbal Reinforcement Learning}}},
  shorttitle = {Reflexion},
  author = {Shinn, Noah and Cassano, Federico and Berman, Edward and Gopinath, Ashwin and Narasimhan, Karthik and Yao, Shunyu},
  year = {2023},
  month = oct,
  number = {arXiv:2303.11366},
  eprint = {2303.11366},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.11366},
  urldate = {2024-11-17},
  abstract = {Large language models (LLMs) have been increasingly used to interact with external environments (e.g., games, compilers, APIs) as goal-driven agents. However, it remains challenging for these language agents to quickly and efficiently learn from trial-and-error as traditional reinforcement learning methods require extensive training samples and expensive model fine-tuning. We propose Reflexion, a novel framework to reinforce language agents not by updating weights, but instead through linguistic feedback. Concretely, Reflexion agents verbally reflect on task feedback signals, then maintain their own reflective text in an episodic memory buffer to induce better decision-making in subsequent trials. Reflexion is flexible enough to incorporate various types (scalar values or free-form language) and sources (external or internally simulated) of feedback signals, and obtains significant improvements over a baseline agent across diverse tasks (sequential decision-making, coding, language reasoning). For example, Reflexion achieves a 91\% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous state-of-the-art GPT-4 that achieves 80\%. We also conduct ablation and analysis studies using different feedback signals, feedback incorporation methods, and agent types, and provide insights into how they affect performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/brede/Zotero/storage/75WT2DH9/Shinn et al. - 2023 - Reflexion Language Agents with Verbal Reinforcement Learning.pdf;/home/brede/Zotero/storage/5ME2IZBF/2303.html}
}

@article{silvaImprovingDenseRetrieval2024,
  title = {Improving Dense Retrieval Models with {{LLM}} Augmented Data for Dataset Search},
  author = {Silva, Levy and Barbosa, Luciano},
  year = {2024},
  month = jun,
  journal = {Knowledge-Based Systems},
  volume = {294},
  pages = {111740},
  issn = {0950-7051},
  doi = {10.1016/j.knosys.2024.111740},
  urldate = {2024-11-10},
  abstract = {Data augmentation for training supervised models has achieved great results in different areas. With the popularity of Large Language Models (LLMs), a research area has emerged focused on applying LLMs for text data augmentation. This approach is particularly beneficial for low-resource tasks, whereby the availability of labeled data is very scarce. Dataset search is an information retrieval task that aims to retrieve relevant datasets based on user queries. However, due to the lack of labeled data tailored explicitly for this task, developing accurate retrieval models becomes challenging. In this paper, we target LLMs to create training examples for retrieval models in the dataset search task. Specifically, we propose a new pipeline that generates synthetic queries from dataset descriptions using LLMs. The query-description pairs are utilized to fine-tune dense retrieval approaches for re-ranking, which we assume as soft matches to our task. We evaluated our pipeline using fine-tuned embedding models for semantic search over dataset search benchmarks (NTCIR and ACORDAR). We tuned these models in the dataset search task using the synthetic data generated by our solution and compared their performance with the original models. The results show the models tuned on the synthetic data statistically outperform the baselines at different normalized discounted cumulative gain levels.},
  keywords = {Data augmentation,Dataset retrieval,Dataset search,Large language models,Query generation},
  file = {/home/brede/Zotero/storage/U8R2XZ58/S0950705124003757.html}
}

@misc{singhEmpiricalStudyValidating2024,
  title = {An {{Empirical Study}} of {{Validating Synthetic Data}} for {{Formula Generation}}},
  author = {Singh, Usneek and Cambronero, Jos{\'e} and Gulwani, Sumit and Kanade, Aditya and Khatry, Anirudh and Le, Vu and Singh, Mukul and Verbruggen, Gust},
  year = {2024},
  month = nov,
  number = {arXiv:2407.10657},
  eprint = {2407.10657},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.10657},
  urldate = {2024-11-10},
  abstract = {Large language models (LLMs) can be leveraged to help with writing formulas in spreadsheets, but resources on these formulas are scarce, impacting both the base performance of pre-trained models and limiting the ability to fine-tune them. Given a corpus of formulas, we can use a(nother) model to generate synthetic natural language utterances for fine-tuning. However, it is important to validate whether the NL generated by the LLM is indeed accurate to be beneficial for fine-tuning. In this paper, we provide empirical results on the impact of validating these synthetic training examples with surrogate objectives that evaluate the accuracy of the synthetic annotations. We demonstrate that validation improves performance over raw data across four models (2 open and 2 closed weight). Interestingly, we show that although validation tends to prune more challenging examples, it increases the complexity of problems that models can solve after being fine-tuned on validated data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@inproceedings{steckCosineSimilarityEmbeddingsReally2024,
  title = {Is {{Cosine-Similarity}} of {{Embeddings Really About Similarity}}?},
  booktitle = {Companion {{Proceedings}} of the {{ACM Web Conference}} 2024},
  author = {Steck, Harald and Ekanadham, Chaitanya and Kallus, Nathan},
  year = {2024},
  month = may,
  series = {{{WWW}} '24},
  pages = {887--890},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3589335.3651526},
  urldate = {2024-11-17},
  abstract = {Cosine-similarity is the cosine of the angle between two vectors, or equivalently the dot product between their normalizations. A popular application is to quantify semantic similarity between high-dimensional objects by applying cosine-similarity to a learned low-dimensional feature embedding. This can work better but sometimes also worse than the unnormalized dot-product between embedded vectors in practice. To gain insight into this empirical observation, we study embeddings derived from regularized linear models, where closed-form solutions facilitate analytical insights. We derive analytically how cosine-similarity can yield arbitrary and therefore meaningless 'similarities.' For some linear models the similarities are not even unique, while for others they are implicitly controlled by the regularization. We discuss implications beyond linear models: a combination of different regularizations are employed when learning deep models; these have implicit and unintended effects when taking cosine-similarities of the resulting embeddings, rendering results opaque and possibly arbitrary. Based on these insights, we caution against blindly using cosine-similarity and outline alternatives.},
  isbn = {9798400701726},
  file = {/home/brede/Zotero/storage/MKJKUPKY/Steck et al. - 2024 - Is Cosine-Similarity of Embeddings Really About Similarity.pdf}
}

@misc{touvronLLaMAOpenEfficient2023a,
  title = {{{LLaMA}}: {{Open}} and {{Efficient Foundation Language Models}}},
  shorttitle = {{{LLaMA}}},
  author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
  year = {2023},
  month = feb,
  number = {arXiv:2302.13971},
  eprint = {2302.13971},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.13971},
  urldate = {2023-08-31},
  abstract = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/brede/Zotero/storage/7TM7A2R9/Touvron et al. - 2023 - LLaMA Open and Efficient Foundation Language Mode.pdf;/home/brede/Zotero/storage/HFLWLNK7/2302.html}
}

@misc{tunstallEfficientFewShotLearning2022,
  title = {Efficient {{Few-Shot Learning Without Prompts}}},
  author = {Tunstall, Lewis and Reimers, Nils and Jo, Unso Eun Seo and Bates, Luke and Korat, Daniel and Wasserblat, Moshe and Pereg, Oren},
  year = {2022},
  month = sep,
  number = {arXiv:2209.11055},
  eprint = {2209.11055},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-23},
  abstract = {Recent few-shot methods, such as parameter-efficient fine-tuning (PEFT) and pattern exploiting training (PET), have achieved impressive results in label-scarce settings. However, they are difficult to employ since they are subject to high variability from manually crafted prompts, and typically require billion-parameter language models to achieve high accuracy. To address these shortcomings, we propose SetFit (Sentence Transformer Fine-tuning), an efficient and prompt-free framework for few-shot fine-tuning of Sentence Transformers (ST). SetFit works by first fine-tuning a pretrained ST on a small number of text pairs, in a contrastive Siamese manner. The resulting model is then used to generate rich text embeddings, which are used to train a classification head. This simple framework requires no prompts or verbalizers, and achieves high accuracy with orders of magnitude less parameters than existing techniques. Our experiments show that SetFit obtains comparable results with PEFT and PET techniques, while being an order of magnitude faster to train. We also show that SetFit can be applied in multilingual settings by simply switching the ST body. Our code is available at https://github.com/huggingface/setfit and our datasets at https://huggingface.co/setfit .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/brede/Zotero/storage/LTAXJTH7/Tunstall et al. - 2022 - Efficient Few-Shot Learning Without Prompts.pdf;/home/brede/Zotero/storage/F4PBBEE5/2209.html}
}

@misc{umarjamilAttentionAllYou2023,
  title = {Attention Is All You Need ({{Transformer}}) - {{Model}} Explanation (Including Math), {{Inference}} and {{Training}}},
  author = {{Umar Jamil}},
  year = {2023},
  month = may,
  urldate = {2024-11-02},
  abstract = {A complete explanation of all the layers of a Transformer Model: Multi-Head Self-Attention, Positional Encoding, including all the matrix multiplications and a complete description of the training and inference process. Paper: Attention is all you need - https://arxiv.org/abs/1706.03762 Slides PDF: https://github.com/hkproj/transformer... Chapters 00:00 - Intro 01:10 - RNN and their problems 08:04 - Transformer Model 09:02 - Maths background and notations 12:20 - Encoder (overview) 12:31 - Input Embeddings 15:04 - Positional Encoding 20:08 - Single Head Self-Attention 28:30 - Multi-Head Attention 35:39 - Query, Key, Value 37:55 - Layer Normalization 40:13 - Decoder (overview) 42:24 - Masked Multi-Head Attention 44:59 - Training 52:09 - Inference}
}

@misc{UnslothPhi3mini4kinstructHugging,
  title = {Unsloth/{{Phi-3-mini-4k-instruct}} {$\cdot$} {{Hugging Face}}},
  urldate = {2024-06-04},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  howpublished = {https://huggingface.co/unsloth/Phi-3-mini-4k-instruct},
  file = {/home/brede/Zotero/storage/BK25ZE4L/Phi-3-mini-4k-instruct.html}
}

@misc{vaswaniAttentionAllYou2023a,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2023},
  month = aug,
  number = {arXiv:1706.03762},
  eprint = {1706.03762},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1706.03762},
  urldate = {2023-08-24},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/brede/Zotero/storage/SIXSJ9UQ/Vaswani et al. - 2023 - Attention Is All You Need.pdf;/home/brede/Zotero/storage/E2SPPV4S/1706.html}
}

@inproceedings{villalobosPositionWillWe2024,
  title = {Position: {{Will}} We Run out of Data? {{Limits}} of {{LLM}} Scaling Based on Human-Generated Data},
  shorttitle = {Position},
  booktitle = {Forty-First {{International Conference}} on {{Machine Learning}}},
  author = {Villalobos, Pablo and Ho, Anson and Sevilla, Jaime and Besiroglu, Tamay and Heim, Lennart and Hobbhahn, Marius},
  year = {2024},
  month = jun,
  urldate = {2024-11-08},
  abstract = {We investigate the potential constraints on LLM scaling posed by the availability of public human-generated text data. We forecast the growing demand for training data based on current trends and estimate the total stock of public human text data. Our findings indicate that if current LLM development trends continue, models will be trained on datasets roughly equal in size to the available stock of public human text data between 2026 and 2032, or slightly earlier if models are overtrained. We explore how progress in language modeling can continue when human-generated text datasets cannot be scaled any further. We argue that synthetic data generation, transfer learning from data-rich domains, and data efficiency improvements might support further progress.},
  langid = {english},
  file = {/home/brede/Zotero/storage/W3X2ZPU7/Villalobos et al. - 2024 - Position Will we run out of data Limits of LLM scaling based on human-generated data.pdf}
}

@misc{wangImprovingTextEmbeddings2024,
  title = {Improving {{Text Embeddings}} with {{Large Language Models}}},
  author = {Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu},
  year = {2024},
  month = may,
  number = {arXiv:2401.00368},
  eprint = {2401.00368},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-06-04},
  abstract = {In this paper, we introduce a novel and simple method for obtaining high-quality text embeddings using only synthetic data and less than 1k training steps. Unlike existing methods that often depend on multi-stage intermediate pre-training with billions of weakly-supervised text pairs, followed by fine-tuning with a few labeled datasets, our method does not require building complex training pipelines or relying on manually collected datasets that are often constrained by task diversity and language coverage. We leverage proprietary LLMs to generate diverse synthetic data for hundreds of thousands of text embedding tasks across 93 languages. We then fine-tune open-source decoder-only LLMs on the synthetic data using standard contrastive loss. Experiments demonstrate that our method achieves strong performance on highly competitive text embedding benchmarks without using any labeled data. Furthermore, when fine-tuned with a mixture of synthetic and labeled data, our model sets new state-of-the-art results on the BEIR and MTEB benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {/home/brede/Zotero/storage/Q6EXVLQ8/Wang et al. - 2024 - Improving Text Embeddings with Large Language Mode.pdf;/home/brede/Zotero/storage/K7NIQ7K7/2401.html}
}

@misc{wangMultilingualE5Text2024a,
  title = {Multilingual {{E5 Text Embeddings}}: {{A Technical Report}}},
  shorttitle = {Multilingual {{E5 Text Embeddings}}},
  author = {Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu},
  year = {2024},
  month = feb,
  number = {arXiv:2402.05672},
  eprint = {2402.05672},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-06-17},
  abstract = {This technical report presents the training methodology and evaluation results of the opensource multilingual E5 text embedding models, released in mid-2023. Three embedding models of different sizes (small / base / large) are provided, offering a balance between the inference efficiency and embedding quality. The training procedure adheres to the English E5 model recipe, involving contrastive pre-training on 1 billion multilingual text pairs, followed by fine-tuning on a combination of labeled datasets. Additionally, we introduce a new instruction-tuned embedding model, whose performance is on par with state-of-the-art, English-only models of similar sizes. Information regarding the model release can be found at https://github.com/microsoft/unilm/ tree/master/e5.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {/home/brede/Zotero/storage/8ZMX5C8M/Wang et al. - 2024 - Multilingual E5 Text Embeddings A Technical Repor.pdf}
}

@misc{wangSearchingBestPractices2024,
  title = {Searching for {{Best Practices}} in {{Retrieval-Augmented Generation}}},
  author = {Wang, Xiaohua and Wang, Zhenghua and Gao, Xuan and Zhang, Feiran and Wu, Yixin and Xu, Zhibo and Shi, Tianyuan and Wang, Zhengyuan and Li, Shizheng and Qian, Qi and Yin, Ruicheng and Lv, Changze and Zheng, Xiaoqing and Huang, Xuanjing},
  year = {2024},
  month = jul,
  number = {arXiv:2407.01219},
  eprint = {2407.01219},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-12},
  abstract = {Retrieval-augmented generation (RAG) techniques have proven to be effective in integrating up-to-date information, mitigating hallucinations, and enhancing response quality, particularly in specialized domains. While many RAG approaches have been proposed to enhance large language models through query-dependent retrievals, these approaches still suffer from their complex implementation and prolonged response times. Typically, a RAG workflow involves multiple processing steps, each of which can be executed in various ways. Here, we investigate existing RAG approaches and their potential combinations to identify optimal RAG practices. Through extensive experiments, we suggest several strategies for deploying RAG that balance both performance and efficiency. Moreover, we demonstrate that multimodal retrieval techniques can significantly enhance question-answering capabilities about visual inputs and accelerate the generation of multimodal content using a ``retrieval as generation'' strategy. Resources are available at https://github.com/FudanDNN-NLP/RAG.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/brede/Zotero/storage/AQAWM3DG/Wang et al. - 2024 - Searching for Best Practices in Retrieval-Augmente.pdf}
}

@article{wangSurveyLargeLanguage2024,
  title = {A {{Survey}} on {{Large Language Model}} Based {{Autonomous Agents}}},
  author = {Wang, Lei and Ma, Chen and Feng, Xueyang and Zhang, Zeyu and Yang, Hao and Zhang, Jingsen and Chen, Zhiyuan and Tang, Jiakai and Chen, Xu and Lin, Yankai and Zhao, Wayne Xin and Wei, Zhewei and Wen, Ji-Rong},
  year = {2024},
  month = dec,
  journal = {Frontiers of Computer Science},
  volume = {18},
  number = {6},
  eprint = {2308.11432},
  primaryclass = {cs},
  pages = {186345},
  issn = {2095-2228, 2095-2236},
  doi = {10.1007/s11704-024-40231-1},
  urldate = {2024-11-16},
  abstract = {Abstract Autonomous agents have long been a research focus in academic and industry communities. Previous research often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from human learning processes, and makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have shown potential in human-level intelligence, leading to a surge in research on LLM-based autonomous agents. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of LLMbased autonomous agents from a holistic perspective. We first discuss the construction of LLM-based autonomous agents, proposing a unified framework that encompasses much of previous work. Then, we present a overview of the diverse applications of LLM-based autonomous agents in social science, natural science, and engineering. Finally, we delve into the evaluation strategies commonly used for LLM-based autonomous agents. Based on the previous studies, we also present several challenges and future directions in this field.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/brede/Zotero/storage/Q2IUISKX/Wang et al. - 2024 - A Survey on Large Language Model based Autonomous Agents.pdf}
}

@misc{weiChainThoughtPromptingElicits2023b,
  title = {Chain-of-{{Thought Prompting Elicits Reasoning}} in {{Large Language Models}}},
  author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  year = {2023},
  month = jan,
  number = {arXiv:2201.11903},
  eprint = {2201.11903},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2201.11903},
  urldate = {2024-11-18},
  abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/brede/Zotero/storage/R4X5XNKS/Wei et al. - 2023 - Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.pdf;/home/brede/Zotero/storage/WAZQE6VI/2201.html}
}

@misc{WhatAIAgent2024,
  title = {What Is an {{AI}} Agent?},
  year = {2024},
  month = jun,
  journal = {LangChain Blog},
  urldate = {2024-11-17},
  abstract = {Introducing a new series of musings on AI agents, called "In the Loop".},
  howpublished = {https://blog.langchain.dev/what-is-an-agent/},
  langid = {english},
  file = {/home/brede/Zotero/storage/QWWBGZAA/what-is-an-agent.html}
}

@misc{yadkoriBelieveNotBelieve2024,
  title = {To {{Believe}} or {{Not}} to {{Believe Your LLM}}},
  author = {Yadkori, Yasin Abbasi and Kuzborskij, Ilja and Gy{\"o}rgy, Andr{\'a}s and Szepesv{\'a}ri, Csaba},
  year = {2024},
  month = jun,
  number = {arXiv:2406.02543},
  eprint = {2406.02543},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.02543},
  urldate = {2024-06-10},
  abstract = {We explore uncertainty quantification in large language models (LLMs), with the goal to identify when uncertainty in responses given a query is large. We simultaneously consider both epistemic and aleatoric uncertainties, where the former comes from the lack of knowledge about the ground truth (such as about facts or the language), and the latter comes from irreducible randomness (such as multiple possible answers). In particular, we derive an information-theoretic metric that allows to reliably detect when only epistemic uncertainty is large, in which case the output of the model is unreliable. This condition can be computed based solely on the output of the model obtained simply by some special iterative prompting based on the previous responses. Such quantification, for instance, allows to detect hallucinations (cases when epistemic uncertainty is high) in both single- and multi-answer responses. This is in contrast to many standard uncertainty quantification strategies (such as thresholding the log-likelihood of a response) where hallucinations in the multi-answer case cannot be detected. We conduct a series of experiments which demonstrate the advantage of our formulation. Further, our investigations shed some light on how the probabilities assigned to a given output by an LLM can be amplified by iterative prompting, which might be of independent interest.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/brede/Zotero/storage/ZE6Q2E5H/Yadkori et al. - 2024 - To Believe or Not to Believe Your LLM.pdf;/home/brede/Zotero/storage/CA2UBGSW/2406.html}
}

@misc{yaoReActSynergizingReasoning2023,
  title = {{{ReAct}}: {{Synergizing Reasoning}} and {{Acting}} in {{Language Models}}},
  shorttitle = {{{ReAct}}},
  author = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  year = {2023},
  month = mar,
  number = {arXiv:2210.03629},
  eprint = {2210.03629},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2210.03629},
  urldate = {2024-11-17},
  abstract = {While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34\% and 10\% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/brede/Zotero/storage/KFNFBS4T/Yao et al. - 2023 - ReAct Synergizing Reasoning and Acting in Language Models.pdf;/home/brede/Zotero/storage/NEXT99VA/2210.html}
}

@misc{yaoTreeThoughtsDeliberate2023,
  title = {Tree of {{Thoughts}}: {{Deliberate Problem Solving}} with {{Large Language Models}}},
  shorttitle = {Tree of {{Thoughts}}},
  author = {Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Thomas L. and Cao, Yuan and Narasimhan, Karthik},
  year = {2023},
  month = dec,
  number = {arXiv:2305.10601},
  eprint = {2305.10601},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-17},
  abstract = {Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, ``Tree of Thoughts'' (ToT), which generalizes over the popular ``Chain of Thought'' approach to prompting language models, and enables exploration over coherent units of text (``thoughts'') that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4\% of tasks, our method achieved a success rate of 74\%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/brede/Zotero/storage/VYBASVGF/Yao et al. - 2023 - Tree of Thoughts Deliberate Problem Solving with Large Language Models.pdf}
}

@misc{zhongSyntheT2CGeneratingSynthetic2024,
  title = {{{SyntheT2C}}: {{Generating Synthetic Data}} for {{Fine-Tuning Large Language Models}} on the {{Text2Cypher Task}}},
  shorttitle = {{{SyntheT2C}}},
  author = {Zhong, Ziije and Zhong, Linqing and Sun, Zhaoze and Jin, Qingyun and Qin, Zengchang and Zhang, Xiaofan},
  year = {2024},
  month = jun,
  number = {arXiv:2406.10710},
  eprint = {2406.10710},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.10710},
  urldate = {2024-11-10},
  abstract = {Integrating Large Language Models (LLMs) with existing Knowledge Graph (KG) databases presents a promising avenue for enhancing LLMs' efficacy and mitigating their "hallucinations". Given that most KGs reside in graph databases accessible solely through specialized query languages (e.g., Cypher), there exists a critical need to bridge the divide between LLMs and KG databases by automating the translation of natural language into Cypher queries (commonly termed the "Text2Cypher" task). Prior efforts tried to bolster LLMs' proficiency in Cypher generation through Supervised Fine-Tuning. However, these explorations are hindered by the lack of annotated datasets of Query-Cypher pairs, resulting from the labor-intensive and domain-specific nature of annotating such datasets. In this study, we propose SyntheT2C, a methodology for constructing a synthetic Query-Cypher pair dataset, comprising two distinct pipelines: (1) LLM-based prompting and (2) template-filling. SyntheT2C facilitates the generation of extensive Query-Cypher pairs with values sampled from an underlying Neo4j graph database. Subsequently, SyntheT2C is applied to two medical databases, culminating in the creation of a synthetic dataset, MedT2C. Comprehensive experiments demonstrate that the MedT2C dataset effectively enhances the performance of backbone LLMs on the Text2Cypher task. Both the SyntheT2C codebase and the MedT2C dataset will be released soon.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}
