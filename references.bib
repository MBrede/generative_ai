@misc{24Sparse2024,
  title = {2:4 {{Sparse Llama}}: {{Smaller Models}} for {{Efficient GPU Inference}}},
  shorttitle = {2},
  year = {2024},
  month = nov,
  urldate = {2024-11-26},
  abstract = {Discover Sparse Llama: A 50\% pruned, GPU-optimized Llama 3.1 model with 2:4 sparsity, enabling faster, cost-effective inference without sacrificing accuracy.},
  howpublished = {https://neuralmagic.com/blog/24-sparse-llama-smaller-models-for-efficient-gpu-inference/},
  langid = {american}
}

@misc{424127,
  title = {What Exactly Are Keys, Queries, and Values in Attention Mechanisms?},
  author = {(https://stats.stackexchange.com/users/95569/dontloo), dontloo},
  eprint = {https://stats.stackexchange.com/q/424127},
  howpublished = {Cross Validated}
}

@misc{AIAAICChatGPTTraining,
  title = {{AIAAIC - ChatGPT training emits 502 metric tons of carbon}},
  urldate = {2024-11-09},
  abstract = {ChatGPT training estimated to emit 502 metric tonnes of carbon},
  howpublished = {https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/chatgpt-training-emits-502-metric-tons-of-carbon},
  langid = {ngerman}
}

@misc{angelovTop2VecDistributedRepresentations2020a,
  title = {{{Top2Vec}}: {{Distributed Representations}} of {{Topics}}},
  shorttitle = {{{Top2Vec}}},
  author = {Angelov, Dimo},
  year = {2020},
  month = aug,
  number = {arXiv:2008.09470},
  eprint = {2008.09470},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2008.09470},
  urldate = {2023-02-08},
  abstract = {Topic modeling is used for discovering latent semantic structure, usually referred to as topics, in a large collection of documents. The most widely used methods are Latent Dirichlet Allocation and Probabilistic Latent Semantic Analysis. Despite their popularity they have several weaknesses. In order to achieve optimal results they often require the number of topics to be known, custom stop-word lists, stemming, and lemmatization. Additionally these methods rely on bag-of-words representation of documents which ignore the ordering and semantics of words. Distributed representations of documents and words have gained popularity due to their ability to capture semantics of words and documents. We present \${\textbackslash}texttt\{top2vec\}\$, which leverages joint document and word semantic embedding to find \${\textbackslash}textit\{topic vectors\}\$. This model does not require stop-word lists, stemming or lemmatization, and it automatically finds the number of topics. The resulting topic vectors are jointly embedded with the document and word vectors with distance between them representing semantic similarity. Our experiments demonstrate that \${\textbackslash}texttt\{top2vec\}\$ finds topics which are significantly more informative and representative of the corpus trained on than probabilistic generative models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{bahdanauNeuralMachineTranslation2014,
  title = {Neural Machine Translation by Jointly Learning to Align and Translate},
  author = {Bahdanau, Dzmitry},
  year = {2014},
  journal = {arXiv preprint arXiv:1409.0473},
  eprint = {1409.0473},
  urldate = {2024-11-09},
  archiveprefix = {arXiv}
}

@inproceedings{baoAllAreWorth2022,
  title = {All Are Worth Words: A Vit Backbone for Score-Based Diffusion Models},
  shorttitle = {All Are Worth Words},
  booktitle = {{{NeurIPS}} 2022 {{Workshop}} on {{Score-Based Methods}}},
  author = {Bao, Fan and Li, Chongxuan and Cao, Yue and Zhu, Jun},
  year = {2022},
  urldate = {2024-12-01}
}

@misc{baoOneTransformerFits2023,
  title = {One {{Transformer Fits All Distributions}} in {{Multi-Modal Diffusion}} at {{Scale}}},
  author = {Bao, Fan and Nie, Shen and Xue, Kaiwen and Li, Chongxuan and Pu, Shi and Wang, Yaole and Yue, Gang and Cao, Yue and Su, Hang and Zhu, Jun},
  year = {2023},
  month = may,
  number = {arXiv:2303.06555},
  eprint = {2303.06555},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.06555},
  urldate = {2024-12-01},
  abstract = {This paper proposes a unified diffusion framework (dubbed UniDiffuser) to fit all distributions relevant to a set of multi-modal data in one model. Our key insight is -- learning diffusion models for marginal, conditional, and joint distributions can be unified as predicting the noise in the perturbed data, where the perturbation levels (i.e. timesteps) can be different for different modalities. Inspired by the unified view, UniDiffuser learns all distributions simultaneously with a minimal modification to the original diffusion model -- perturbs data in all modalities instead of a single modality, inputs individual timesteps in different modalities, and predicts the noise of all modalities instead of a single modality. UniDiffuser is parameterized by a transformer for diffusion models to handle input types of different modalities. Implemented on large-scale paired image-text data, UniDiffuser is able to perform image, text, text-to-image, image-to-text, and image-text pair generation by setting proper timesteps without additional overhead. In particular, UniDiffuser is able to produce perceptually realistic samples in all tasks and its quantitative results (e.g., the FID and CLIP score) are not only superior to existing general-purpose models but also comparable to the bespoken models (e.g., Stable Diffusion and DALL-E 2) in representative tasks (e.g., text-to-image generation).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@misc{BlogArticlesMulti_agentsmd,
  title = {Blog/Articles/Multi\_agents.Md at Main {$\cdot$} {{YukoOshima}}/{{Blog}}},
  journal = {GitHub},
  urldate = {2024-11-26},
  abstract = {Contribute to YukoOshima/Blog development by creating an account on GitHub.},
  howpublished = {https://github.com/YukoOshima/Blog/blob/main/articles/multi\_agents.md},
  langid = {english}
}

@misc{bojanowskiEnrichingWordVectors2017,
  title = {Enriching {{Word Vectors}} with {{Subword Information}}},
  author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  year = {2017},
  month = jun,
  number = {arXiv:1607.04606},
  eprint = {1607.04606},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1607.04606},
  urldate = {2024-11-08},
  abstract = {Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character \$n\$-grams. A vector representation is associated to each character \$n\$-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@inproceedings{bowmanLargeAnnotatedCorpus2015,
  title = {A Large Annotated Corpus for Learning Natural Language Inference},
  booktitle = {Proceedings of the 2015 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Bowman, Samuel R. and Angeli, Gabor and Potts, Christopher and Manning, Christopher D.},
  editor = {M{\`a}rquez, Llu{\'i}s and {Callison-Burch}, Chris and Su, Jian},
  year = {2015},
  month = sep,
  pages = {632--642},
  publisher = {Association for Computational Linguistics},
  address = {Lisbon, Portugal},
  doi = {10.18653/v1/D15-1075},
  urldate = {2024-11-16}
}

@misc{brownLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = {2020},
  month = jul,
  number = {arXiv:2005.14165},
  eprint = {2005.14165},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2005.14165},
  urldate = {2024-06-13},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{BuildingMultiAgentFramework2024,
  title = {Building a {{Multi-Agent Framework}} from {{Scratch}} with {{LlamaIndex}}},
  year = {2024},
  month = oct,
  journal = {DEV Community},
  urldate = {2024-11-26},
  abstract = {Learn how to build a sophisticated multi-agent system using LlamaIndex to generate high-quality Anki flashcards. This tutorial guides you through the process of creating a modular and robust framework, from a basic single-agent setup to a complex orchestrated system.},
  howpublished = {https://dev.to/yukooshima/building-a-multi-agent-framework-from-scratch-with-llamaindex-5ecn},
  langid = {english}
}

@misc{cerUniversalSentenceEncoder2018a,
  title = {Universal {{Sentence Encoder}}},
  author = {Cer, Daniel and Yang, Yinfei and Kong, Sheng-yi and Hua, Nan and Limtiaco, Nicole and John, Rhomni St and Constant, Noah and {Guajardo-Cespedes}, Mario and Yuan, Steve and Tar, Chris and Sung, Yun-Hsuan and Strope, Brian and Kurzweil, Ray},
  year = {2018},
  month = apr,
  number = {arXiv:1803.11175},
  eprint = {1803.11175},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1803.11175},
  urldate = {2023-08-31},
  abstract = {We present models for encoding sentences into embedding vectors that specifically target transfer learning to other NLP tasks. The models are efficient and result in accurate performance on diverse transfer tasks. Two variants of the encoding models allow for trade-offs between accuracy and compute resources. For both variants, we investigate and report the relationship between model complexity, resource consumption, the availability of transfer task training data, and task performance. Comparisons are made with baselines that use word level transfer learning via pretrained word embeddings as well as baselines do not use any transfer learning. We find that transfer learning using sentence embeddings tends to outperform word level transfer. With transfer learning via sentence embeddings, we observe surprisingly good performance with minimal amounts of supervised training data for a transfer task. We obtain encouraging results on Word Embedding Association Tests (WEAT) targeted at detecting model bias. Our pre-trained sentence encoding models are made freely available for download and on TF Hub.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{ConstitutionalAIHarmlessness,
  title = {Constitutional {{AI}}: {{Harmlessness}} from {{AI Feedback}}},
  shorttitle = {Constitutional {{AI}}},
  urldate = {2024-11-26},
  abstract = {Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.},
  howpublished = {https://www.anthropic.com/research/constitutional-ai-harmlessness-from-ai-feedback},
  langid = {english}
}

@misc{ConstitutionalAIOpen,
  title = {Constitutional {{AI}} with {{Open LLMs}}},
  urldate = {2024-11-26},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  howpublished = {https://huggingface.co/blog/constitutional\_ai}
}

@misc{dettmersQLoRAEfficientFinetuning2023,
  title = {{{QLoRA}}: {{Efficient Finetuning}} of {{Quantized LLMs}}},
  shorttitle = {{{QLoRA}}},
  author = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  year = {2023},
  month = may,
  number = {arXiv:2305.14314},
  eprint = {2305.14314},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.14314},
  urldate = {2024-06-13},
  abstract = {We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters{\textasciitilde}(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3\% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning}
}

@misc{devlinBERTPretrainingDeep2019a,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  number = {arXiv:1810.04805},
  eprint = {1810.04805},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1810.04805},
  urldate = {2023-08-31},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@inproceedings{dhariwalDiffusionModelsBeat2021,
  title = {Diffusion {{Models Beat GANs}} on {{Image Synthesis}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Dhariwal, Prafulla and Nichol, Alexander},
  year = {2021},
  volume = {34},
  pages = {8780--8794},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-11-30}
}

@misc{doerschTutorialVariationalAutoencoders2021,
  title = {Tutorial on {{Variational Autoencoders}}},
  author = {Doersch, Carl},
  year = {2021},
  month = jan,
  number = {arXiv:1606.05908},
  eprint = {1606.05908},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1606.05908},
  urldate = {2024-12-01},
  abstract = {In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, CIFAR images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{esserTamingTransformersHighResolution2021,
  title = {Taming {{Transformers}} for {{High-Resolution Image Synthesis}}},
  author = {Esser, Patrick and Rombach, Robin and Ommer, Bj{\"o}rn},
  year = {2021},
  month = jun,
  number = {arXiv:2012.09841},
  eprint = {2012.09841},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2012.09841},
  urldate = {2024-12-01},
  abstract = {Designed to learn long-range interactions on sequential data, transformers continue to show state-of-the-art results on a wide variety of tasks. In contrast to CNNs, they contain no inductive bias that prioritizes local interactions. This makes them expressive, but also computationally infeasible for long sequences, such as high-resolution images. We demonstrate how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution images. We show how to (i) use CNNs to learn a context-rich vocabulary of image constituents, and in turn (ii) utilize transformers to efficiently model their composition within high-resolution images. Our approach is readily applied to conditional synthesis tasks, where both non-spatial information, such as object classes, and spatial information, such as segmentations, can control the generated image. In particular, we present the first results on semantically-guided synthesis of megapixel images with transformers and obtain the state of the art among autoregressive models on class-conditional ImageNet. Code and pretrained models can be found at https://github.com/CompVis/taming-transformers .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{ganesanEmpiricalEvaluationPretrained2021,
  title = {Empirical {{Evaluation}} of {{Pre-trained Transformers}} for {{Human-Level NLP}}: {{The Role}} of {{Sample Size}} and {{Dimensionality}}},
  shorttitle = {Empirical {{Evaluation}} of {{Pre-trained Transformers}} for {{Human-Level NLP}}},
  author = {Ganesan, Adithya V and Matero, Matthew and Ravula, Aravind Reddy and Vu, Huy and Schwartz, H. Andrew},
  year = {2021},
  month = jun,
  journal = {Proceedings of the conference. Association for Computational Linguistics. North American Chapter. Meeting},
  volume = {2021},
  pages = {4515--4532},
  doi = {10.18653/v1/2021.naacl-main.357},
  urldate = {2024-05-23},
  abstract = {In human-level NLP tasks, such as predicting mental health, personality, or demographics, the number of observations is often smaller than the standard 768+ hidden state sizes of each layer within modern transformer-based language models, limiting the ability to effectively leverage transformers. Here, we provide a systematic study on the role of dimension reduction methods (principal components analysis, factorization techniques, or multi-layer auto-encoders) as well as the dimensionality of embedding vectors and sample sizes as a function of predictive performance. We first find that fine-tuning large models with a limited amount of data pose a significant difficulty which can be overcome with a pre-trained dimension reduction regime. RoBERTa consistently achieves top performance in human-level tasks, with PCA giving benefit over other reduction methods in better handling users that write longer texts. Finally, we observe that a majority of the tasks achieve results comparable to the best performance with just 112 of the embedding dimensions.},
  pmcid = {PMC8294338},
  pmid = {34296226}
}

@misc{goodfellowGenerativeAdversarialNetworks2014,
  title = {Generative {{Adversarial Networks}}},
  author = {Goodfellow, Ian J. and {Pouget-Abadie}, Jean and Mirza, Mehdi and Xu, Bing and {Warde-Farley}, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  year = {2014},
  month = jun,
  number = {arXiv:1406.2661},
  eprint = {1406.2661},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1406.2661},
  urldate = {2024-12-01},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{GoogleGemma2bHugging2024,
  title = {Google/Gemma-2b {$\cdot$} {{Hugging Face}}},
  year = {2024},
  month = may,
  urldate = {2024-06-04},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  howpublished = {https://huggingface.co/google/gemma-2b}
}

@inproceedings{goyalComparativeAnalysisDifferent2022,
  title = {Comparative {{Analysis}} of {{Different Vectorizing Techniques}} for {{Document Similarity}} Using {{Cosine Similarity}}},
  booktitle = {2022 {{Second International Conference}} on {{Advanced Technologies}} in {{Intelligent Control}}, {{Environment}}, {{Computing}} \& {{Communication Engineering}} ({{ICATIECE}})},
  author = {Goyal, Kanav and Sharma, Megha},
  year = {2022},
  month = dec,
  pages = {1--5},
  doi = {10.1109/ICATIECE56365.2022.10046766},
  urldate = {2024-11-17},
  abstract = {In this paper, multiple methods to vectorize documents were compared, and cosine similarities were calculated for the corresponding documents. Some of the vectorizing methods also consider the text's semantic meaning. The methods involve cosine similarity with algorithms like Bag of Words, Binary Bag of Words, Tf-Idf, Bidirectional Encoder Representations from Transformers, and Universal Sentence Encoder. Two important libraries to preprocess the text were used; these are NLTK and Genism. The Binary bag of words with Genism gave the best results of all the methods used. The dataset used involved around 2000 short news articles; these belonged to 5 categories.},
  keywords = {Bit error rate,Cosine Similarity,F1 Score,Libraries,Precision,Recall,Semantics,Transformers,Vectorizing Methods}
}

@inproceedings{guhrTrainingBroadCoverageGerman2020a,
  title = {Training a {{Broad-Coverage German Sentiment Classification Model}} for {{Dialog Systems}}},
  booktitle = {Proceedings of the {{Twelfth Language Resources}} and {{Evaluation Conference}}},
  author = {Guhr, Oliver and Schumann, Anne-Kathrin and Bahrmann, Frank and B{\"o}hme, Hans Joachim},
  year = {2020},
  month = may,
  pages = {1627--1632},
  publisher = {European Language Resources Association},
  address = {Marseille, France},
  urldate = {2023-02-08},
  abstract = {This paper describes the training of a general-purpose German sentiment classification model. Sentiment classification is an important aspect of general text analytics. Furthermore, it plays a vital role in dialogue systems and voice interfaces that depend on the ability of the system to pick up and understand emotional signals from user utterances. The presented study outlines how we have collected a new German sentiment corpus and then combined this corpus with existing resources to train a broad-coverage German sentiment model. The resulting data set contains 5.4 million labelled samples. We have used the data to train both, a simple convolutional and a transformer-based classification model and compared the results achieved on various training configurations. The model and the data set will be published along with this paper.},
  isbn = {979-10-95546-34-4},
  langid = {english}
}

@misc{heidloffFinetuningSmallLLMs2023,
  title = {Fine-Tuning Small {{LLMs}} with {{Output}} from Large {{LLMs}}},
  author = {Heidloff, Niklas},
  year = {2023},
  month = aug,
  journal = {Niklas Heidloff},
  urldate = {2024-11-10},
  abstract = {The innovation in the AI space is continuing in an incredible speed. This post describes a new technique that has evolved recently which allows smaller fine-tuned models to almost reach the performance of much larger models.},
  howpublished = {https://heidloff.net/article/fine-tune-small-llm-with-big-llm/},
  langid = {english}
}

@misc{heidloffFoundationModelsTransformers2023,
  title = {Foundation {{Models}}, {{Transformers}}, {{BERT}} and {{GPT}}},
  author = {Heidloff, Niklas},
  year = {2023},
  month = feb,
  journal = {Niklas Heidloff},
  urldate = {2024-11-09},
  abstract = {Since I'm excited by the incredible capabilities which technologies like ChatGPT and Bard provide, I'm trying to understand better how they work. This post summarizes my current understanding about foundation models, transformers, BERT and GPT.},
  howpublished = {https://heidloff.net/article/foundation-models-transformers-bert-and-gpt/},
  langid = {english}
}

@misc{hoClassifierFreeDiffusionGuidance2022,
  title = {Classifier-{{Free Diffusion Guidance}}},
  author = {Ho, Jonathan and Salimans, Tim},
  year = {2022},
  month = jul,
  number = {arXiv:2207.12598},
  eprint = {2207.12598},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2207.12598},
  urldate = {2024-12-02},
  abstract = {Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@misc{hoffmannTrainingComputeOptimalLarge2022a,
  title = {Training {{Compute-Optimal Large Language Models}}},
  author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and van den Driessche, George and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Rae, Jack W. and Vinyals, Oriol and Sifre, Laurent},
  year = {2022},
  month = mar,
  number = {arXiv:2203.15556},
  eprint = {2203.15556},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.15556},
  urldate = {2023-08-31},
  abstract = {We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4\${\textbackslash}times\$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5\% on the MMLU benchmark, greater than a 7\% improvement over Gopher.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{hsiehDistillingStepStepOutperforming2023,
  title = {Distilling {{Step-by-Step}}! {{Outperforming Larger Language Models}} with {{Less Training Data}} and {{Smaller Model Sizes}}},
  author = {Hsieh, Cheng-Yu and Li, Chun-Liang and Yeh, Chih-Kuan and Nakhost, Hootan and Fujii, Yasuhisa and Ratner, Alexander and Krishna, Ranjay and Lee, Chen-Yu and Pfister, Tomas},
  year = {2023},
  month = jul,
  number = {arXiv:2305.02301},
  eprint = {2305.02301},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.02301},
  urldate = {2024-11-09},
  abstract = {Deploying large language models (LLMs) is challenging because they are memory inefficient and compute-intensive for practical applications. In reaction, researchers train smaller task-specific models by either finetuning with human labels or distilling using LLM-generated labels. However, finetuning and distillation require large amounts of training data to achieve comparable performance to LLMs. We introduce Distilling step-by-step, a new mechanism that (a) trains smaller models that outperform LLMs, and (b) achieves so by leveraging less training data needed by finetuning or distillation. Our method extracts LLM rationales as additional supervision for training small models within a multi-task framework. We present three findings across 4 NLP benchmarks: First, compared to both finetuning and distillation, our mechanism achieves better performance with much fewer labeled/unlabeled training examples. Second, compared to few-shot prompted LLMs, we achieve better performance using substantially smaller model sizes. Third, we reduce both the model size and the amount of data required to outperform LLMs; our finetuned 770M T5 model outperforms the few-shot prompted 540B PaLM model using only 80\% of available data on a benchmark, whereas standard finetuning the same T5 model struggles to match even by using 100\% of the dataset. We release the code at: https://github.com/google-research/distilling-step-by-step .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{HttpsArxivorgPdf,
  title = {{{https://arxiv.org/pdf/2007.11301}}},
  urldate = {2024-11-20},
  howpublished = {https://arxiv.org/pdf/2007.11301}
}

@misc{hulbertUsingTreeThoughtPrompting2023,
  title = {Using {{Tree-of-Thought Prompting}} to Boost {{ChatGPT}}'s Reasoning},
  author = {Hulbert, Dave},
  year = {2023},
  month = may,
  urldate = {2024-11-19},
  abstract = {Using Tree-of-Thought Prompting to boost ChatGPT's reasoning},
  copyright = {MIT},
  howpublished = {https://github.com/dave1010/tree-of-thought-prompting}
}

@misc{huLoRALowRankAdaptation2021,
  title = {{{LoRA}}: {{Low-Rank Adaptation}} of {{Large Language Models}}},
  shorttitle = {{{LoRA}}},
  author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and {Allen-Zhu}, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  year = {2021},
  month = oct,
  number = {arXiv:2106.09685},
  eprint = {2106.09685},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2106.09685},
  urldate = {2024-06-13},
  abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@article{hussainTutorialOpensourceLarge2024,
  title = {A Tutorial on Open-Source Large Language Models for Behavioral Science},
  author = {Hussain, Zak and Binz, Marcel and Mata, Rui and Wulff, Dirk U.},
  year = {2024},
  month = aug,
  journal = {Behavior Research Methods},
  volume = {56},
  number = {8},
  pages = {8214--8237},
  issn = {1554-3528},
  doi = {10.3758/s13428-024-02455-8},
  urldate = {2024-11-09},
  abstract = {Abstract                            Large language models (LLMs) have the potential to revolutionize behavioral science by accelerating and improving the research cycle, from conceptualization to data analysis. Unlike closed-source solutions, open-source frameworks for LLMs can enable transparency, reproducibility, and adherence to data protection standards, which gives them a crucial advantage for use in behavioral science. To help researchers harness the promise of LLMs, this tutorial offers a primer on the open-source Hugging Face ecosystem and demonstrates several applications that advance conceptual and empirical work in behavioral science, including feature extraction, fine-tuning of models for prediction, and generation of behavioral responses. Executable code is made available at               github.com/Zak-Hussain/LLM4BeSci.git               . Finally, the tutorial discusses challenges faced by research with (open-source) LLMs related to interpretability and safety and offers a perspective on future research at the intersection of language modeling and behavioral science.},
  langid = {english}
}

@misc{IntroductionLLMAgents2023,
  title = {Introduction to {{LLM Agents}}},
  year = {2023},
  month = nov,
  journal = {NVIDIA Technical Blog},
  urldate = {2024-11-17},
  abstract = {Consider a large language model (LLM) application that is designed to help financial analysts answer questions about the performance of a company. With a well-designed retrieval augmented generation{\dots}},
  howpublished = {https://developer.nvidia.com/blog/introduction-to-llm-agents/},
  langid = {american}
}

@misc{jiangMoRAHighRankUpdating2024,
  title = {{{MoRA}}: {{High-Rank Updating}} for {{Parameter-Efficient Fine-Tuning}}},
  shorttitle = {{{MoRA}}},
  author = {Jiang, Ting and Huang, Shaohan and Luo, Shengyue and Zhang, Zihan and Huang, Haizhen and Wei, Furu and Deng, Weiwei and Sun, Feng and Zhang, Qi and Wang, Deqing and Zhuang, Fuzhen},
  year = {2024},
  month = may,
  number = {arXiv:2405.12130},
  eprint = {2405.12130},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.12130},
  urldate = {2024-06-13},
  abstract = {Low-rank adaptation is a popular parameter-efficient fine-tuning method for large language models. In this paper, we analyze the impact of low-rank updating, as implemented in LoRA. Our findings suggest that the low-rank updating mechanism may limit the ability of LLMs to effectively learn and memorize new knowledge. Inspired by this observation, we propose a new method called MoRA, which employs a square matrix to achieve high-rank updating while maintaining the same number of trainable parameters. To achieve it, we introduce the corresponding non-parameter operators to reduce the input dimension and increase the output dimension for the square matrix. Furthermore, these operators ensure that the weight can be merged back into LLMs, which makes our method can be deployed like LoRA. We perform a comprehensive evaluation of our method across five tasks: instruction tuning, mathematical reasoning, continual pretraining, memory and pretraining. Our method outperforms LoRA on memory-intensive tasks and achieves comparable performance on other tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{jiangScalingSentenceEmbeddings2023a,
  title = {Scaling {{Sentence Embeddings}} with {{Large Language Models}}},
  author = {Jiang, Ting and Huang, Shaohan and Luan, Zhongzhi and Wang, Deqing and Zhuang, Fuzhen},
  year = {2023},
  month = jul,
  number = {arXiv:2307.16645},
  eprint = {2307.16645},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.16645},
  urldate = {2024-02-12},
  abstract = {Large language models (LLMs) have recently garnered significant interest. With in-context learning, LLMs achieve impressive results in various natural language tasks. However, the application of LLMs to sentence embeddings remains an area of ongoing research. In this work, we propose an in-context learning-based method aimed at improving sentence embeddings performance. Our approach involves adapting the previous prompt-based representation method for autoregressive models, constructing a demonstration set that enables LLMs to perform in-context learning, and scaling up the LLMs to different model sizes. Through extensive experiments, in-context learning enables LLMs to generate high-quality sentence embeddings without any fine-tuning. It helps LLMs achieve performance comparable to current contrastive learning methods. By scaling model size, we find scaling to more than tens of billion parameters harms the performance on semantic textual similarity (STS) tasks. However, the largest model outperforms other counterparts and achieves the new state-of-the-art result on transfer tasks. We also fine-tune LLMs with current contrastive learning approach, and the 2.7B OPT model, incorporating our prompt-based method, surpasses the performance of 4.8B ST5, achieving the new state-of-the-art results on STS tasks. Our code is available at https://github.com/kongds/scaling\_sentemb.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{JphmeEm_german_13b_v01Hugging2023a,
  title = {Jphme/Em\_german\_13b\_v01 {$\cdot$} {{Hugging Face}}},
  year = {2023},
  month = oct,
  urldate = {2023-12-12},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  howpublished = {https://huggingface.co/jphme/em\_german\_13b\_v01}
}

@misc{kaplanScalingLawsNeural2020a,
  title = {Scaling {{Laws}} for {{Neural Language Models}}},
  author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  year = {2020},
  month = jan,
  number = {arXiv:2001.08361},
  eprint = {2001.08361},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2001.08361},
  urldate = {2023-08-31},
  abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{kentonBertPretrainingDeep2019,
  title = {Bert: {{Pre-training}} of Deep Bidirectional Transformers for Language Understanding},
  shorttitle = {Bert},
  booktitle = {Proceedings of {{naacL-HLT}}},
  author = {Kenton, Jacob Devlin Ming-Wei Chang and Toutanova, Lee Kristina},
  year = {2019},
  volume = {1},
  pages = {2},
  publisher = {Minneapolis, Minnesota},
  urldate = {2024-11-02}
}

@article{kingmaAutoencodingVariationalBayes2013,
  title = {Auto-Encoding Variational Bayes},
  author = {Kingma, Diederik P.},
  year = {2013},
  journal = {arXiv preprint arXiv:1312.6114},
  eprint = {1312.6114},
  urldate = {2024-12-01},
  archiveprefix = {arXiv}
}

@article{klingerAutomaticEmotionDetection2016a,
  title = {Automatic {{Emotion Detection}} for {{Quantitative Literary Studies}}},
  author = {Klinger, Roman and Suliya, Surayya Samat and Reiter, Nils},
  year = {2016}
}

@misc{kojimaLargeLanguageModels2023a,
  title = {Large {{Language Models}} Are {{Zero-Shot Reasoners}}},
  author = {Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  year = {2023},
  month = jan,
  number = {arXiv:2205.11916},
  eprint = {2205.11916},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.11916},
  urldate = {2024-11-24},
  abstract = {Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding "Let's think step by step" before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7\% to 78.7\% and GSM8K from 10.4\% to 40.7\% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{kumarSelectiveFinetuningLLMlabeled2024,
  title = {Selective {{Fine-tuning}} on {{LLM-labeled Data May Reduce Reliance}} on {{Human Annotation}}: {{A Case Study Using Schedule-of-Event Table Detection}}},
  shorttitle = {Selective {{Fine-tuning}} on {{LLM-labeled Data May Reduce Reliance}} on {{Human Annotation}}},
  author = {Kumar, Bhawesh and Amar, Jonathan and Yang, Eric and Li, Nan and Jia, Yugang},
  year = {2024},
  month = aug,
  number = {arXiv:2405.06093},
  eprint = {2405.06093},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.06093},
  urldate = {2024-11-10},
  abstract = {Large Language Models (LLMs) have demonstrated their efficacy across a broad spectrum of tasks in healthcare applications. However, often LLMs need to be fine-tuned on task-specific expert annotated data to achieve optimal performance, which can be expensive and time consuming. In this study, we fine-tune PaLM-2 with parameter efficient fine-tuning (PEFT) using noisy labels obtained from gemini-pro 1.0 for the detection of Schedule-of-Event (SoE) tables, which specify care plan in clinical trial protocols. We introduce a filtering mechanism to select high-confidence labels for this table classification task, thereby reducing the noise in the auto-generated labels. We show that fine-tuned PaLM-2 with those labels achieves performance that exceeds the gemini-pro 1.0 and other LLMs. Furthermore, its performance is close to a PaLM-2 fine-tuned on labels obtained from non-expert annotators. Our results show that leveraging LLM-generated labels through powerful models like gemini-pro can potentially serve as a viable strategy for improving LLM performance through fine-tuning in specialized tasks, particularly in domains where expert annotations are scarce, expensive, or time-consuming to obtain.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{kuratovBABILongTestingLimits2024,
  title = {{{BABILong}}: {{Testing}} the {{Limits}} of {{LLMs}} with {{Long Context Reasoning-in-a-Haystack}}},
  shorttitle = {{{BABILong}}},
  author = {Kuratov, Yuri and Bulatov, Aydar and Anokhin, Petr and Rodkin, Ivan and Sorokin, Dmitry and Sorokin, Artyom and Burtsev, Mikhail},
  year = {2024},
  month = nov,
  number = {arXiv:2406.10149},
  eprint = {2406.10149},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.10149},
  urldate = {2024-11-12},
  abstract = {In recent years, the input context sizes of large language models (LLMs) have increased dramatically. However, existing evaluation methods have not kept pace, failing to comprehensively assess the efficiency of models in handling long contexts. To bridge this gap, we introduce the BABILong benchmark, designed to test language models' ability to reason across facts distributed in extremely long documents. BABILong includes a diverse set of 20 reasoning tasks, including fact chaining, simple induction, deduction, counting, and handling lists/sets. These tasks are challenging on their own, and even more demanding when the required facts are scattered across long natural text. Our evaluations show that popular LLMs effectively utilize only 10-20{\textbackslash}\% of the context and their performance declines sharply with increased reasoning complexity. Among alternatives to in-context reasoning, Retrieval-Augmented Generation methods achieve a modest 60{\textbackslash}\% accuracy on single-fact question answering, independent of context length. Among context extension methods, the highest performance is demonstrated by recurrent memory transformers after fine-tuning, enabling the processing of lengths up to 50 million tokens. The BABILong benchmark is extendable to any length to support the evaluation of new upcoming models with increased capabilities, and we provide splits up to 10 million token lengths.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@misc{LateChunkingLongContext2024,
  title = {Late {{Chunking}} in {{Long-Context Embedding Models}}},
  year = {2024},
  month = aug,
  urldate = {2024-11-17},
  abstract = {Chunking long documents while preserving contextual information is challenging. We introduce the "Late Chunking" that leverages long-context embedding models to generate contextual chunk embeddings for better retrieval applications.},
  howpublished = {https://jina.ai/news/late-chunking-in-long-context-embedding-models},
  langid = {american}
}

@inproceedings{leDistributedRepresentationsSentences2014a,
  title = {Distributed {{Representations}} of {{Sentences}} and {{Documents}}},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Machine Learning}}},
  author = {Le, Quoc and Mikolov, Tomas},
  year = {2014},
  month = jun,
  pages = {1188--1196},
  publisher = {PMLR},
  issn = {1938-7228},
  urldate = {2023-02-18},
  abstract = {Many machine learning algorithms require the  input to be represented as a fixed length feature  vector. When it comes to texts, one of the most  common representations is bag-of-words. Despite their popularity, bag-of-words models have  two major weaknesses: they lose the ordering  of the words and they also ignore semantics of  the words. For example, "powerful," "strong"  and "Paris" are equally distant. In this paper,  we propose an unsupervised algorithm that learns  vector representations of sentences and text documents. This algorithm represents each document by a dense vector which is trained to predict  words in the document. Its construction gives our  algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that our technique outperforms bag-of-words models as well as other techniques for  text representations. Finally, we achieve new  state-of-the-art results on several text classification and sentiment analysis tasks.},
  langid = {english}
}

@misc{liuRoBERTaRobustlyOptimized2019a,
  title = {{{RoBERTa}}: {{A Robustly Optimized BERT Pretraining Approach}}},
  shorttitle = {{{RoBERTa}}},
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  year = {2019},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1907.11692},
  urldate = {2024-06-04},
  abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences}
}

@misc{Llama32Revolutionizing,
  title = {Llama 3.2: {{Revolutionizing}} Edge {{AI}} and Vision with Open, Customizable Models},
  shorttitle = {Llama 3.2},
  journal = {Meta AI},
  urldate = {2024-12-01},
  abstract = {Today, we're releasing Llama 3.2, which includes small and medium-sized vision LLMs, and lightweight, text-only models that fit onto edge and mobile devices.},
  howpublished = {https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/},
  langid = {english}
}

@misc{LLMAgentsNextra2024,
  title = {{{LLM Agents}} -- {{Nextra}}},
  year = {2024},
  month = oct,
  urldate = {2024-11-17},
  abstract = {A Comprehensive Overview of Prompt Engineering},
  howpublished = {https://www.promptingguide.ai/research/llm-agents},
  langid = {english}
}

@misc{longLargeLanguageModel2023,
  title = {Large {{Language Model Guided Tree-of-Thought}}},
  author = {Long, Jieyi},
  year = {2023},
  month = may,
  number = {arXiv:2305.08291},
  eprint = {2305.08291},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-17},
  abstract = {In this paper, we introduce the Tree-of-Thought (ToT) framework, a novel approach aimed at improving the problem-solving capabilities of auto-regressive large language models (LLMs). The ToT technique is inspired by the human mind's approach for solving complex reasoning tasks through trial and error. In this process, the human mind explores the solution space through a tree-like thought process, allowing for backtracking when necessary. To implement ToT as a software system, we augment an LLM with additional modules including a prompter agent, a checker module, a memory module, and a ToT controller. In order to solve a given problem, these modules engage in a multi-round conversation with the LLM. The memory module records the conversation and state history of the problem solving process, which allows the system to backtrack to the previous steps of the thought-process and explore other directions from there. To verify the effectiveness of the proposed technique, we implemented a ToT-based solver for the Sudoku Puzzle. Experimental results show that the ToT framework can significantly increase the success rate of Sudoku puzzle solving. Our implementation of the ToT-based Sudoku solver is available on GitHub: https://github.com/jieyilong/tree-of-thought-puzzle-solver.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
}

@misc{mikolovEfficientEstimationWord2013,
  title = {Efficient {{Estimation}} of {{Word Representations}} in {{Vector Space}}},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  year = {2013},
  month = sep,
  number = {arXiv:1301.3781},
  eprint = {1301.3781},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1301.3781},
  urldate = {2024-11-08},
  abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{MistralaiMixtral8x7BInstructv01Hugginga,
  title = {Mistralai/{{Mixtral-8x7B-Instruct-v0}}.1 {$\cdot$} {{Hugging Face}}},
  urldate = {2024-06-04},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  howpublished = {https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1}
}

@misc{petersDeepContextualizedWord2018,
  title = {Deep Contextualized Word Representations},
  author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  year = {2018},
  month = mar,
  number = {arXiv:1802.05365},
  eprint = {1802.05365},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1802.05365},
  urldate = {2024-11-08},
  abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@article{radfordImprovingLanguageUnderstanding2018,
  title = {Improving Language Understanding with Unsupervised Learning},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year = {2018},
  publisher = {Technical report, OpenAI}
}

@article{radfordLanguageModelsAre2019,
  title = {Language Models Are Unsupervised Multitask Learners},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year = {2019},
  journal = {OpenAI blog},
  volume = {1},
  number = {8},
  pages = {9},
  urldate = {2024-06-13}
}

@inproceedings{radfordLearningTransferableVisual2021,
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  year = {2021},
  month = jul,
  pages = {8748--8763},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-11-30},
  abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.},
  langid = {english}
}

@misc{rameshZeroShotTexttoImageGeneration2021,
  title = {Zero-{{Shot Text-to-Image Generation}}},
  author = {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  year = {2021},
  month = feb,
  number = {arXiv:2102.12092},
  eprint = {2102.12092},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2102.12092},
  urldate = {2024-11-30},
  abstract = {Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@misc{reimersMakingMonolingualSentence2020a,
  title = {Making {{Monolingual Sentence Embeddings Multilingual}} Using {{Knowledge Distillation}}},
  author = {Reimers, Nils and Gurevych, Iryna},
  year = {2020},
  month = oct,
  number = {arXiv:2004.09813},
  eprint = {2004.09813},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2004.09813},
  urldate = {2023-02-08},
  abstract = {We present an easy and efficient method to extend existing sentence embedding models to new languages. This allows to create multilingual versions from previously monolingual models. The training is based on the idea that a translated sentence should be mapped to the same location in the vector space as the original sentence. We use the original (monolingual) model to generate sentence embeddings for the source language and then train a new system on translated sentences to mimic the original model. Compared to other methods for training multilingual sentence embeddings, this approach has several advantages: It is easy to extend existing models with relatively few samples to new languages, it is easier to ensure desired properties for the vector space, and the hardware requirements for training is lower. We demonstrate the effectiveness of our approach for 50+ languages from various language families. Code to extend sentence embeddings models to more than 400 languages is publicly available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{reimersSentenceBERTSentenceEmbeddings2019a,
  title = {Sentence-{{BERT}}: {{Sentence Embeddings}} Using {{Siamese BERT-Networks}}},
  shorttitle = {Sentence-{{BERT}}},
  author = {Reimers, Nils and Gurevych, Iryna},
  year = {2019},
  month = aug,
  number = {arXiv:1908.10084},
  eprint = {1908.10084},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1908.10084},
  urldate = {2023-08-31},
  abstract = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textasciitilde}65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@inproceedings{remusSentiWSPubliclyAvailableGermanlanguage2010,
  title = {{{SentiWS-A Publicly Available German-language Resource}} for {{Sentiment Analysis}}},
  booktitle = {Proceedings of the {{Seventh International Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}}'10)},
  author = {Remus, Robert and Quasthoff, Uwe and Heyer, Gerhard},
  year = {2010}
}

@misc{rombachHighResolutionImageSynthesis2022,
  title = {High-{{Resolution Image Synthesis}} with {{Latent Diffusion Models}}},
  author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  year = {2022},
  month = apr,
  number = {arXiv:2112.10752},
  eprint = {2112.10752},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2112.10752},
  urldate = {2024-12-01},
  abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@misc{sardanaChinchillaOptimalAccountingInference2024,
  title = {Beyond {{Chinchilla-Optimal}}: {{Accounting}} for {{Inference}} in {{Language Model Scaling Laws}}},
  shorttitle = {Beyond {{Chinchilla-Optimal}}},
  author = {Sardana, Nikhil and Portes, Jacob and Doubov, Sasha and Frankle, Jonathan},
  year = {2024},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2401.00448},
  urldate = {2024-11-08},
  abstract = {Large language model (LLM) scaling laws are empirical formulas that estimate changes in model quality as a result of increasing parameter count and training data. However, these formulas, including the popular Deepmind Chinchilla scaling laws, neglect to include the cost of inference. We modify the Chinchilla scaling laws to calculate the optimal LLM parameter count and pre-training data size to train and deploy a model of a given quality and inference demand. We conduct our analysis both in terms of a compute budget and real-world costs and find that LLM researchers expecting reasonably large inference demand ({\textasciitilde}1B requests) should train models smaller and longer than Chinchilla-optimal. Furthermore, we train 47 models of varying sizes and parameter counts to validate our formula and find that model quality continues to improve as we scale tokens per parameter to extreme ranges (up to 10,000). Finally, we ablate the procedure used to fit the Chinchilla scaling law coefficients and find that developing scaling laws only from data collected at typical token/parameter ratios overestimates the impact of additional tokens at these extreme ranges.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences,Machine Learning (cs.LG)}
}

@misc{SayWhatYou,
  title = {Say {{What You Mean}}: {{A Response}} to '{{Let Me Speak Freely}}'},
  urldate = {2024-11-25},
  howpublished = {https://blog.dottxt.co/say-what-you-mean.html}
}

@misc{SemanticSimilarityText,
  title = {Semantic {{Similarity}} for {{Text Comparison}} between {{Textual Documents}} or {{Sentences}} {\textbar} {{IEEE Conference Publication}} {\textbar} {{IEEE Xplore}}},
  urldate = {2024-11-17},
  howpublished = {https://ieeexplore.ieee.org/document/10465440}
}

@misc{shinnReflexionLanguageAgents2023,
  title = {Reflexion: {{Language Agents}} with {{Verbal Reinforcement Learning}}},
  shorttitle = {Reflexion},
  author = {Shinn, Noah and Cassano, Federico and Berman, Edward and Gopinath, Ashwin and Narasimhan, Karthik and Yao, Shunyu},
  year = {2023},
  month = oct,
  number = {arXiv:2303.11366},
  eprint = {2303.11366},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.11366},
  urldate = {2024-11-17},
  abstract = {Large language models (LLMs) have been increasingly used to interact with external environments (e.g., games, compilers, APIs) as goal-driven agents. However, it remains challenging for these language agents to quickly and efficiently learn from trial-and-error as traditional reinforcement learning methods require extensive training samples and expensive model fine-tuning. We propose Reflexion, a novel framework to reinforce language agents not by updating weights, but instead through linguistic feedback. Concretely, Reflexion agents verbally reflect on task feedback signals, then maintain their own reflective text in an episodic memory buffer to induce better decision-making in subsequent trials. Reflexion is flexible enough to incorporate various types (scalar values or free-form language) and sources (external or internally simulated) of feedback signals, and obtains significant improvements over a baseline agent across diverse tasks (sequential decision-making, coding, language reasoning). For example, Reflexion achieves a 91\% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous state-of-the-art GPT-4 that achieves 80\%. We also conduct ablation and analysis studies using different feedback signals, feedback incorporation methods, and agent types, and provide insights into how they affect performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@article{silvaImprovingDenseRetrieval2024,
  title = {Improving Dense Retrieval Models with {{LLM}} Augmented Data for Dataset Search},
  author = {Silva, Levy and Barbosa, Luciano},
  year = {2024},
  month = jun,
  journal = {Knowledge-Based Systems},
  volume = {294},
  pages = {111740},
  issn = {0950-7051},
  doi = {10.1016/j.knosys.2024.111740},
  urldate = {2024-11-10},
  abstract = {Data augmentation for training supervised models has achieved great results in different areas. With the popularity of Large Language Models (LLMs), a research area has emerged focused on applying LLMs for text data augmentation. This approach is particularly beneficial for low-resource tasks, whereby the availability of labeled data is very scarce. Dataset search is an information retrieval task that aims to retrieve relevant datasets based on user queries. However, due to the lack of labeled data tailored explicitly for this task, developing accurate retrieval models becomes challenging. In this paper, we target LLMs to create training examples for retrieval models in the dataset search task. Specifically, we propose a new pipeline that generates synthetic queries from dataset descriptions using LLMs. The query-description pairs are utilized to fine-tune dense retrieval approaches for re-ranking, which we assume as soft matches to our task. We evaluated our pipeline using fine-tuned embedding models for semantic search over dataset search benchmarks (NTCIR and ACORDAR). We tuned these models in the dataset search task using the synthetic data generated by our solution and compared their performance with the original models. The results show the models tuned on the synthetic data statistically outperform the baselines at different normalized discounted cumulative gain levels.},
  keywords = {Data augmentation,Dataset retrieval,Dataset search,Large language models,Query generation}
}

@misc{singhEmpiricalStudyValidating2024,
  title = {An {{Empirical Study}} of {{Validating Synthetic Data}} for {{Formula Generation}}},
  author = {Singh, Usneek and Cambronero, Jos{\'e} and Gulwani, Sumit and Kanade, Aditya and Khatry, Anirudh and Le, Vu and Singh, Mukul and Verbruggen, Gust},
  year = {2024},
  month = nov,
  number = {arXiv:2407.10657},
  eprint = {2407.10657},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.10657},
  urldate = {2024-11-10},
  abstract = {Large language models (LLMs) can be leveraged to help with writing formulas in spreadsheets, but resources on these formulas are scarce, impacting both the base performance of pre-trained models and limiting the ability to fine-tune them. Given a corpus of formulas, we can use a(nother) model to generate synthetic natural language utterances for fine-tuning. However, it is important to validate whether the NL generated by the LLM is indeed accurate to be beneficial for fine-tuning. In this paper, we provide empirical results on the impact of validating these synthetic training examples with surrogate objectives that evaluate the accuracy of the synthetic annotations. We demonstrate that validation improves performance over raw data across four models (2 open and 2 closed weight). Interestingly, we show that although validation tends to prune more challenging examples, it increases the complexity of problems that models can solve after being fine-tuned on validated data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@inproceedings{sohl-dicksteinDeepUnsupervisedLearning2015,
  title = {Deep Unsupervised Learning Using Nonequilibrium Thermodynamics},
  booktitle = {International Conference on Machine Learning},
  author = {{Sohl-Dickstein}, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
  year = {2015},
  pages = {2256--2265},
  publisher = {PMLR},
  urldate = {2024-11-30}
}

@article{songScoreBasedGenerativeModeling2020,
  title = {Score-{{Based Generative Modeling}} through {{Stochastic Differential Equations}}},
  author = {Song, Yang and {Sohl-Dickstein}, Jascha Narain and Kingma, Diederik P. and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  year = {2020},
  month = nov,
  journal = {ArXiv},
  urldate = {2024-11-30},
  abstract = {Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field ({\textbackslash}aka, score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.}
}

@article{stadlerCognitiveEaseCost2024,
  title = {Cognitive Ease at a Cost: {{LLMs}} Reduce Mental Effort but Compromise Depth in Student Scientific Inquiry},
  shorttitle = {Cognitive Ease at a Cost},
  author = {Stadler, Matthias and Bannert, Maria and Sailer, Michael},
  year = {2024},
  month = nov,
  journal = {Computers in Human Behavior},
  volume = {160},
  pages = {108386},
  issn = {0747-5632},
  doi = {10.1016/j.chb.2024.108386},
  urldate = {2024-11-25},
  abstract = {This study explores the cognitive load and learning outcomes associated with using large language models (LLMs) versus traditional search engines for information gathering during learning. A total of 91 university students were randomly assigned to either use ChatGPT3.5 or Google to research the socio-scientific issue of nanoparticles in sunscreen to derive valid recommendations and justifications. The study aimed to investigate potential differences in cognitive load, as well as the quality and homogeneity of the students' recommendations and justifications. Results indicated that students using LLMs experienced significantly lower cognitive load. However, despite this reduction, these students demonstrated lower-quality reasoning and argumentation in their final recommendations compared to those who used traditional search engines. Further, the homogeneity of the recommendations and justifications did not differ significantly between the two groups, suggesting that LLMs did not restrict the diversity of students' perspectives. These findings highlight the nuanced implications of digital tools on learning, suggesting that while LLMs can decrease the cognitive burden associated with information gathering during a learning task, they may not promote deeper engagement with content necessary for high-quality learning per se.}
}

@inproceedings{steckCosineSimilarityEmbeddingsReally2024,
  title = {Is {{Cosine-Similarity}} of {{Embeddings Really About Similarity}}?},
  booktitle = {Companion {{Proceedings}} of the {{ACM Web Conference}} 2024},
  author = {Steck, Harald and Ekanadham, Chaitanya and Kallus, Nathan},
  year = {2024},
  month = may,
  series = {{{WWW}} '24},
  pages = {887--890},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3589335.3651526},
  urldate = {2024-11-17},
  abstract = {Cosine-similarity is the cosine of the angle between two vectors, or equivalently the dot product between their normalizations. A popular application is to quantify semantic similarity between high-dimensional objects by applying cosine-similarity to a learned low-dimensional feature embedding. This can work better but sometimes also worse than the unnormalized dot-product between embedded vectors in practice. To gain insight into this empirical observation, we study embeddings derived from regularized linear models, where closed-form solutions facilitate analytical insights. We derive analytically how cosine-similarity can yield arbitrary and therefore meaningless 'similarities.' For some linear models the similarities are not even unique, while for others they are implicitly controlled by the regularization. We discuss implications beyond linear models: a combination of different regularizations are employed when learning deep models; these have implicit and unintended effects when taking cosine-similarities of the resulting embeddings, rendering results opaque and possibly arbitrary. Based on these insights, we caution against blindly using cosine-similarity and outline alternatives.},
  isbn = {979-8-4007-0172-6}
}

@misc{touvronLLaMAOpenEfficient2023a,
  title = {{{LLaMA}}: {{Open}} and {{Efficient Foundation Language Models}}},
  shorttitle = {{{LLaMA}}},
  author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
  year = {2023},
  month = feb,
  number = {arXiv:2302.13971},
  eprint = {2302.13971},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.13971},
  urldate = {2023-08-31},
  abstract = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{tunstallEfficientFewShotLearning2022,
  title = {Efficient {{Few-Shot Learning Without Prompts}}},
  author = {Tunstall, Lewis and Reimers, Nils and Jo, Unso Eun Seo and Bates, Luke and Korat, Daniel and Wasserblat, Moshe and Pereg, Oren},
  year = {2022},
  month = sep,
  number = {arXiv:2209.11055},
  eprint = {2209.11055},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-23},
  abstract = {Recent few-shot methods, such as parameter-efficient fine-tuning (PEFT) and pattern exploiting training (PET), have achieved impressive results in label-scarce settings. However, they are difficult to employ since they are subject to high variability from manually crafted prompts, and typically require billion-parameter language models to achieve high accuracy. To address these shortcomings, we propose SetFit (Sentence Transformer Fine-tuning), an efficient and prompt-free framework for few-shot fine-tuning of Sentence Transformers (ST). SetFit works by first fine-tuning a pretrained ST on a small number of text pairs, in a contrastive Siamese manner. The resulting model is then used to generate rich text embeddings, which are used to train a classification head. This simple framework requires no prompts or verbalizers, and achieves high accuracy with orders of magnitude less parameters than existing techniques. Our experiments show that SetFit obtains comparable results with PEFT and PET techniques, while being an order of magnitude faster to train. We also show that SetFit can be applied in multilingual settings by simply switching the ST body. Our code is available at https://github.com/huggingface/setfit and our datasets at https://huggingface.co/setfit .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{umarjamilAttentionAllYou2023,
  title = {Attention Is All You Need ({{Transformer}}) - {{Model}} Explanation (Including Math), {{Inference}} and {{Training}}},
  author = {{Umar Jamil}},
  year = {2023},
  month = may,
  urldate = {2024-11-02},
  abstract = {A complete explanation of all the layers of a Transformer Model: Multi-Head Self-Attention, Positional Encoding, including all the matrix multiplications and a complete description of the training and inference process. Paper: Attention is all you need - https://arxiv.org/abs/1706.03762 Slides PDF: https://github.com/hkproj/transformer... Chapters 00:00 - Intro 01:10 - RNN and their problems 08:04 - Transformer Model 09:02 - Maths background and notations 12:20 - Encoder (overview) 12:31 - Input Embeddings 15:04 - Positional Encoding 20:08 - Single Head Self-Attention 28:30 - Multi-Head Attention 35:39 - Query, Key, Value 37:55 - Layer Normalization 40:13 - Decoder (overview) 42:24 - Masked Multi-Head Attention 44:59 - Training 52:09 - Inference}
}

@misc{UnslothPhi3mini4kinstructHugging,
  title = {Unsloth/{{Phi-3-mini-4k-instruct}} {$\cdot$} {{Hugging Face}}},
  urldate = {2024-06-04},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  howpublished = {https://huggingface.co/unsloth/Phi-3-mini-4k-instruct}
}

@misc{vaswaniAttentionAllYou2023a,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2023},
  month = aug,
  number = {arXiv:1706.03762},
  eprint = {1706.03762},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1706.03762},
  urldate = {2023-08-24},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@inproceedings{villalobosPositionWillWe2024,
  title = {Position: {{Will}} We Run out of Data? {{Limits}} of {{LLM}} Scaling Based on Human-Generated Data},
  shorttitle = {Position},
  booktitle = {Forty-First {{International Conference}} on {{Machine Learning}}},
  author = {Villalobos, Pablo and Ho, Anson and Sevilla, Jaime and Besiroglu, Tamay and Heim, Lennart and Hobbhahn, Marius},
  year = {2024},
  month = jun,
  urldate = {2024-11-08},
  abstract = {We investigate the potential constraints on LLM scaling posed by the availability of public human-generated text data. We forecast the growing demand for training data based on current trends and estimate the total stock of public human text data. Our findings indicate that if current LLM development trends continue, models will be trained on datasets roughly equal in size to the available stock of public human text data between 2026 and 2032, or slightly earlier if models are overtrained. We explore how progress in language modeling can continue when human-generated text datasets cannot be scaled any further. We argue that synthetic data generation, transfer learning from data-rich domains, and data efficiency improvements might support further progress.},
  langid = {english}
}

@misc{wangComprehensiveSurveySmall2024,
  title = {A {{Comprehensive Survey}} of {{Small Language Models}} in the {{Era}} of {{Large Language Models}}: {{Techniques}}, {{Enhancements}}, {{Applications}}, {{Collaboration}} with {{LLMs}}, and {{Trustworthiness}}},
  shorttitle = {A {{Comprehensive Survey}} of {{Small Language Models}} in the {{Era}} of {{Large Language Models}}},
  author = {Wang, Fali and Zhang, Zhiwei and Zhang, Xianren and Wu, Zongyu and Mo, Tzuhao and Lu, Qiuhao and Wang, Wanjing and Li, Rui and Xu, Junjie and Tang, Xianfeng and He, Qi and Ma, Yao and Huang, Ming and Wang, Suhang},
  year = {2024},
  month = nov,
  number = {arXiv:2411.03350},
  eprint = {2411.03350},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.03350},
  urldate = {2024-11-21},
  abstract = {Large language models (LLM) have demonstrated emergent abilities in text generation, question answering, and reasoning, facilitating various tasks and domains. Despite their proficiency in various tasks, LLMs like LaPM 540B and Llama-3.1 405B face limitations due to large parameter sizes and computational demands, often requiring cloud API use which raises privacy concerns, limits real-time applications on edge devices, and increases fine-tuning costs. Additionally, LLMs often underperform in specialized domains such as healthcare and law due to insufficient domain-specific knowledge, necessitating specialized models. Therefore, Small Language Models (SLMs) are increasingly favored for their low inference latency, cost-effectiveness, efficient development, and easy customization and adaptability. These models are particularly well-suited for resource-limited environments and domain knowledge acquisition, addressing LLMs' challenges and proving ideal for applications that require localized data handling for privacy, minimal inference latency for efficiency, and domain knowledge acquisition through lightweight fine-tuning. The rising demand for SLMs has spurred extensive research and development. However, a comprehensive survey investigating issues related to the definition, acquisition, application, enhancement, and reliability of SLM remains lacking, prompting us to conduct a detailed survey on these topics. The definition of SLMs varies widely, thus to standardize, we propose defining SLMs by their capability to perform specialized tasks and suitability for resource-constrained settings, setting boundaries based on the minimal size for emergent abilities and the maximum size sustainable under resource constraints. For other aspects, we provide a taxonomy of relevant models/methods and develop general frameworks for each category to enhance and utilize SLMs effectively.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{wangImprovingTextEmbeddings2024,
  title = {Improving {{Text Embeddings}} with {{Large Language Models}}},
  author = {Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu},
  year = {2024},
  month = may,
  number = {arXiv:2401.00368},
  eprint = {2401.00368},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-06-04},
  abstract = {In this paper, we introduce a novel and simple method for obtaining high-quality text embeddings using only synthetic data and less than 1k training steps. Unlike existing methods that often depend on multi-stage intermediate pre-training with billions of weakly-supervised text pairs, followed by fine-tuning with a few labeled datasets, our method does not require building complex training pipelines or relying on manually collected datasets that are often constrained by task diversity and language coverage. We leverage proprietary LLMs to generate diverse synthetic data for hundreds of thousands of text embedding tasks across 93 languages. We then fine-tune open-source decoder-only LLMs on the synthetic data using standard contrastive loss. Experiments demonstrate that our method achieves strong performance on highly competitive text embedding benchmarks without using any labeled data. Furthermore, when fine-tuned with a mixture of synthetic and labeled data, our model sets new state-of-the-art results on the BEIR and MTEB benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval}
}

@misc{wangMultilingualE5Text2024a,
  title = {Multilingual {{E5 Text Embeddings}}: {{A Technical Report}}},
  shorttitle = {Multilingual {{E5 Text Embeddings}}},
  author = {Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu},
  year = {2024},
  month = feb,
  number = {arXiv:2402.05672},
  eprint = {2402.05672},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-06-17},
  abstract = {This technical report presents the training methodology and evaluation results of the opensource multilingual E5 text embedding models, released in mid-2023. Three embedding models of different sizes (small / base / large) are provided, offering a balance between the inference efficiency and embedding quality. The training procedure adheres to the English E5 model recipe, involving contrastive pre-training on 1 billion multilingual text pairs, followed by fine-tuning on a combination of labeled datasets. Additionally, we introduce a new instruction-tuned embedding model, whose performance is on par with state-of-the-art, English-only models of similar sizes. Information regarding the model release can be found at https://github.com/microsoft/unilm/ tree/master/e5.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval}
}

@misc{wangSearchingBestPractices2024,
  title = {Searching for {{Best Practices}} in {{Retrieval-Augmented Generation}}},
  author = {Wang, Xiaohua and Wang, Zhenghua and Gao, Xuan and Zhang, Feiran and Wu, Yixin and Xu, Zhibo and Shi, Tianyuan and Wang, Zhengyuan and Li, Shizheng and Qian, Qi and Yin, Ruicheng and Lv, Changze and Zheng, Xiaoqing and Huang, Xuanjing},
  year = {2024},
  month = jul,
  number = {arXiv:2407.01219},
  eprint = {2407.01219},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-12},
  abstract = {Retrieval-augmented generation (RAG) techniques have proven to be effective in integrating up-to-date information, mitigating hallucinations, and enhancing response quality, particularly in specialized domains. While many RAG approaches have been proposed to enhance large language models through query-dependent retrievals, these approaches still suffer from their complex implementation and prolonged response times. Typically, a RAG workflow involves multiple processing steps, each of which can be executed in various ways. Here, we investigate existing RAG approaches and their potential combinations to identify optimal RAG practices. Through extensive experiments, we suggest several strategies for deploying RAG that balance both performance and efficiency. Moreover, we demonstrate that multimodal retrieval techniques can significantly enhance question-answering capabilities about visual inputs and accelerate the generation of multimodal content using a ``retrieval as generation'' strategy. Resources are available at https://github.com/FudanDNN-NLP/RAG.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language}
}

@article{wangSurveyLargeLanguage2024,
  title = {A {{Survey}} on {{Large Language Model}} Based {{Autonomous Agents}}},
  author = {Wang, Lei and Ma, Chen and Feng, Xueyang and Zhang, Zeyu and Yang, Hao and Zhang, Jingsen and Chen, Zhiyuan and Tang, Jiakai and Chen, Xu and Lin, Yankai and Zhao, Wayne Xin and Wei, Zhewei and Wen, Ji-Rong},
  year = {2024},
  month = dec,
  journal = {Frontiers of Computer Science},
  volume = {18},
  number = {6},
  eprint = {2308.11432},
  primaryclass = {cs},
  pages = {186345},
  issn = {2095-2228, 2095-2236},
  doi = {10.1007/s11704-024-40231-1},
  urldate = {2024-11-16},
  abstract = {Abstract Autonomous agents have long been a research focus in academic and industry communities. Previous research often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from human learning processes, and makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have shown potential in human-level intelligence, leading to a surge in research on LLM-based autonomous agents. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of LLMbased autonomous agents from a holistic perspective. We first discuss the construction of LLM-based autonomous agents, proposing a unified framework that encompasses much of previous work. Then, we present a overview of the diverse applications of LLM-based autonomous agents in social science, natural science, and engineering. Finally, we delve into the evaluation strategies commonly used for LLM-based autonomous agents. Based on the previous studies, we also present several challenges and future directions in this field.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@misc{weiChainofThoughtPromptingElicits2023,
  title = {Chain-of-{{Thought Prompting Elicits Reasoning}} in {{Large Language Models}}},
  author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  year = {2023},
  month = jan,
  number = {arXiv:2201.11903},
  eprint = {2201.11903},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2201.11903},
  urldate = {2024-11-24},
  abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@misc{WhatAIAgent2024,
  title = {What Is an {{AI}} Agent?},
  year = {2024},
  month = jun,
  journal = {LangChain Blog},
  urldate = {2024-11-17},
  abstract = {Introducing a new series of musings on AI agents, called "In the Loop".},
  howpublished = {https://blog.langchain.dev/what-is-an-agent/},
  langid = {english}
}

@misc{yadkoriBelieveNotBelieve2024,
  title = {To {{Believe}} or {{Not}} to {{Believe Your LLM}}},
  author = {Yadkori, Yasin Abbasi and Kuzborskij, Ilja and Gy{\"o}rgy, Andr{\'a}s and Szepesv{\'a}ri, Csaba},
  year = {2024},
  month = jun,
  number = {arXiv:2406.02543},
  eprint = {2406.02543},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.02543},
  urldate = {2024-06-10},
  abstract = {We explore uncertainty quantification in large language models (LLMs), with the goal to identify when uncertainty in responses given a query is large. We simultaneously consider both epistemic and aleatoric uncertainties, where the former comes from the lack of knowledge about the ground truth (such as about facts or the language), and the latter comes from irreducible randomness (such as multiple possible answers). In particular, we derive an information-theoretic metric that allows to reliably detect when only epistemic uncertainty is large, in which case the output of the model is unreliable. This condition can be computed based solely on the output of the model obtained simply by some special iterative prompting based on the previous responses. Such quantification, for instance, allows to detect hallucinations (cases when epistemic uncertainty is high) in both single- and multi-answer responses. This is in contrast to many standard uncertainty quantification strategies (such as thresholding the log-likelihood of a response) where hallucinations in the multi-answer case cannot be detected. We conduct a series of experiments which demonstrate the advantage of our formulation. Further, our investigations shed some light on how the probabilities assigned to a given output by an LLM can be amplified by iterative prompting, which might be of independent interest.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{yaoReActSynergizingReasoning2023,
  title = {{{ReAct}}: {{Synergizing Reasoning}} and {{Acting}} in {{Language Models}}},
  shorttitle = {{{ReAct}}},
  author = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  year = {2023},
  month = mar,
  number = {arXiv:2210.03629},
  eprint = {2210.03629},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2210.03629},
  urldate = {2024-11-17},
  abstract = {While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34\% and 10\% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{yaoTreeThoughtsDeliberate2023,
  title = {Tree of {{Thoughts}}: {{Deliberate Problem Solving}} with {{Large Language Models}}},
  shorttitle = {Tree of {{Thoughts}}},
  author = {Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Thomas L. and Cao, Yuan and Narasimhan, Karthik},
  year = {2023},
  month = dec,
  number = {arXiv:2305.10601},
  eprint = {2305.10601},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-17},
  abstract = {Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, ``Tree of Thoughts'' (ToT), which generalizes over the popular ``Chain of Thought'' approach to prompting language models, and enables exploration over coherent units of text (``thoughts'') that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4\% of tasks, our method achieved a success rate of 74\%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{zhongSyntheT2CGeneratingSynthetic2024,
  title = {{{SyntheT2C}}: {{Generating Synthetic Data}} for {{Fine-Tuning Large Language Models}} on the {{Text2Cypher Task}}},
  shorttitle = {{{SyntheT2C}}},
  author = {Zhong, Ziije and Zhong, Linqing and Sun, Zhaoze and Jin, Qingyun and Qin, Zengchang and Zhang, Xiaofan},
  year = {2024},
  month = jun,
  number = {arXiv:2406.10710},
  eprint = {2406.10710},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.10710},
  urldate = {2024-11-10},
  abstract = {Integrating Large Language Models (LLMs) with existing Knowledge Graph (KG) databases presents a promising avenue for enhancing LLMs' efficacy and mitigating their "hallucinations". Given that most KGs reside in graph databases accessible solely through specialized query languages (e.g., Cypher), there exists a critical need to bridge the divide between LLMs and KG databases by automating the translation of natural language into Cypher queries (commonly termed the "Text2Cypher" task). Prior efforts tried to bolster LLMs' proficiency in Cypher generation through Supervised Fine-Tuning. However, these explorations are hindered by the lack of annotated datasets of Query-Cypher pairs, resulting from the labor-intensive and domain-specific nature of annotating such datasets. In this study, we propose SyntheT2C, a methodology for constructing a synthetic Query-Cypher pair dataset, comprising two distinct pipelines: (1) LLM-based prompting and (2) template-filling. SyntheT2C facilitates the generation of extensive Query-Cypher pairs with values sampled from an underlying Neo4j graph database. Subsequently, SyntheT2C is applied to two medical databases, culminating in the creation of a synthetic dataset, MedT2C. Comprehensive experiments demonstrate that the MedT2C dataset effectively enhances the performance of backbone LLMs on the Text2Cypher task. Both the SyntheT2C codebase and the MedT2C dataset will be released soon.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}
